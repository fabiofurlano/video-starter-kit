This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.husky/
  pre-commit
docs/
  context-selected-file.txt
  deepreserch_18_fal_ai_sdk.md
  fal-integration-guide.md
  new_camera_components_guide.txt
  video_images_music_models..md
public/
  key-checker.js
  key-copy.js
  key-injector.html
  model-guide.md
  model-hints.json
src/
  app/
    api/
      download/
        route.ts
      fal/
        route.ts
      share/
        route.ts
      uploadthing/
        core.ts
        route.ts
    app/
      layout.tsx
      page.tsx
    docs/
      page.tsx
    share/
      [id]/
        page.tsx
    globals.css
    layout.tsx
    page.tsx
    session-manager.ts
  components/
    playht/
      voice-selector.tsx
    ui/
      accordion.tsx
      badge.tsx
      button.tsx
      collapsible.tsx
      command.tsx
      dialog.tsx
      download-button.tsx
      dropdown-menu.tsx
      form.tsx
      icons.tsx
      input.tsx
      label.tsx
      landing-laptop-mockup.tsx
      popover.tsx
      select.tsx
      separator.tsx
      sheet.tsx
      skeleton.tsx
      slider.tsx
      tabs.tsx
      textarea.tsx
      toast.tsx
      toaster.tsx
      toggle-group.tsx
      toggle.tsx
      tooltip.tsx
    video/
      timeline.tsx
      track.tsx
    aspect-ratio.tsx
    bottom-bar.tsx
    camera-control.tsx
    export-dialog.tsx
    footer.tsx
    header.tsx
    key-dialog.tsx
    landing-community.tsx
    landing-features.tsx
    landing-footer.tsx
    landing-header.tsx
    landing-hero.tsx
    left-panel.tsx
    logo.tsx
    main.tsx
    media-gallery.tsx
    media-panel.tsx
    model-helper.tsx
    novel-vision-logo.tsx
    project-dialog.tsx
    right-panel.tsx
    storyboard-panel.tsx
    theme-provider.tsx
    theme-toggle.tsx
    video-controls.tsx
    video-preview.tsx
  data/
    db.ts
    mutations.ts
    queries.ts
    schema.ts
    seed.ts
    store.ts
  docs/
    model-guide.md
  hooks/
    use-toast.ts
  lib/
    config.ts
    fal.ts
    ffmpeg.ts
    project.ts
    prompt.ts
    share.ts
    uploadthing.ts
    utils.ts
  styles/
    main-app-theme.css
    right-panel.css
.editorconfig
.env.example copy
.gitignore
.nvmrc
biome.json
CODE_OF_CONDUCT.md
components.json
CONTRIBUTING.md
LICENSE
next.config.mjs
nvmrc
package.json
postcss.config.mjs
README.md
tailwind.config.ts
test-fal.js
tsconfig.json

================================================================
Files
================================================================

================
File: .husky/pre-commit
================
npm run format

================
File: docs/context-selected-file.txt
================
This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: js/ai_visual_studio/index.js, js/ai_visual_studio/authReady.js, firebase_core/initFirebase.js, video-starter-kit/src/lib/fal.ts, video-starter-kit/src/app/session-manager.ts, video-starter-kit/src/app/page.tsx
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

## Additional Info

# Directory Structure
```

```

# Files

================
File: docs/deepreserch_18_fal_ai_sdk.md
================
Understood! I will now conduct a deep research to gather model-specific prompting techniques, best practices, advanced strategies, and fine-tuning tips from official Fal AI documentation and other verified sources. I will also include pricing details only from the Fal AI API and create a structured table with all key insights.

I'll update you once the research is complete.

# Prompting Techniques & Best Practices for Fal AI Models

## Image Generation Models

### FLUX.1 [dev] – Text-to-Image
**About & Key Features:** FLUX.1 [dev] is a 12-billion-parameter “flow” transformer model for image generation ([Generating Images from Text | fal.ai Docs](https://docs.fal.ai/guides/generating-images-from-text/#:~:text=%2A%20fal,5%20Large%20is)). It produces high-quality images from text and supports both personal and commercial use ([FLUX.1 [dev] | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/flux/dev#:~:text=fal)). As a next-gen model, it emphasizes detailed, coherent outputs but isn’t as speed-optimized as newer variants.

**Prompting Best Practices:** Craft descriptive prompts that specify the subject, style, and context. FLUX [dev] responds well to **long-form prompts** with rich detail (e.g. *“Extreme close-up of a tiger’s eye with detailed iris and the word ‘FLUX’ painted over it in white brush strokes”* ([FLUX.1 [dev] | Text to Image | API Documentation | fal.ai](https://fal.ai/models/fal-ai/flux/dev/api#:~:text=const%20result%20%3D%20await%20fal.subscribe%28%22fal,%7D%2C%20logs%3A%20true))). You can use **weighted prompt segments** separated by pipes (`|`) and double-colon weight indicators (`::`) to emphasize certain aspects ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Image%3A%20portrait%20,s%201000)) ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=portrait%20,s%201000)). For example, you might structure a prompt into segments for subject, style, background, etc., with numeric weights to guide the model’s focus. Although FLUX models do not use a traditional diffusion “negative prompt,” you can still mitigate unwanted elements by explicitly phrasing what you want (e.g. “no text on background” or “blank background”). FLUX [dev] allows **high resolution outputs** (e.g. 1024×1024) and may require ~20–40 inference steps for best quality ([FLUX.1 [dev] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/dev/examples#:~:text=num_inference_steps%3A%2028)) ([FLUX.1 [dev] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/dev/examples#:~:text=num_inference_steps%3A%2040)). Enabling more steps (the default is often 28) tends to improve detail. 

**Advanced Tips:** Leverage FLUX’s related tools and fine-tuning options. For example, Fal provides **Flux Realism LoRA** add-ons for style fine-tuning ([FLUX.1 [dev] | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/flux/dev#:~:text=improves%20image%20quality%2C%20typography%2C%20prompt,res%20realism%20%2013)), and a **Portrait Trainer** for faces (which fine-tunes FLUX on portrait data) ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=The%20FLUX%20model%20series%20has,art%20inpainting%2Foutpainting%20%28%2011%20FLUX.1)). You can chain FLUX [dev] with other models: e.g. generate a base image with FLUX [dev], then refine it via **flux redux (image-to-image)** or use it as input to a video model. This model is flexible and handles complex, lengthy prompts better than most – take advantage of that improved prompt understanding by providing clear scene descriptions and even camera or lighting directions.

**Cost & Performance:** FLUX [dev] is billed per image (by megapixels). The Fal API pricing is approximately **$0.025 per 1 megapixel image** (about 2.5¢ for a 1024×1024 image) ([ZimmWriter Image API Integration | www.rankingtactics.com](https://www.rankingtactics.com/zimmwriter-image-api-integration/#:~:text=www,The%20main)). It’s a balanced choice for quality vs. speed: not as fast as flux’s optimized versions, but still reasonably quick on GPU. There are no strict usage limits beyond typical Fal rate limits, but each request queues and runs on Fal’s inference engine. For faster results, ensure your prompt isn’t requesting an extremely large resolution (1080p is supported, but higher resolutions will incur proportionally higher cost and latency). 

### FLUX.1 [schnell] – Fast Text-to-Image 
**About & Key Features:** FLUX.1 [schnell] is an optimized variant of FLUX (also 12B parameters) designed for speed ([Fastest FLUX Endpoint | fal.ai Docs](https://docs.fal.ai/fast-flux/#:~:text=fal,flux)). “Schnell” (German for “fast”) can generate images in as few as **1 to 4 diffusion steps** while preserving quality ([README.md · frankjoshua/FLUX.1-schnell at aa3b18d89cc7c592cc8291df10c9b5045cf3a5db](https://huggingface.co/frankjoshua/FLUX.1-schnell/blame/aa3b18d89cc7c592cc8291df10c9b5045cf3a5db/README.md#:~:text=1.%20Cutting,model%20can%20be%20used%20for)). It delivers similar output fidelity to FLUX [dev] but much faster – ideal for rapid iteration or high-throughput needs. 

**Prompting Best Practices:** FLUX [schnell] supports the **same prompting syntax** as FLUX [dev]. You can use detailed or short prompts, but because it excels with fewer steps, concise prompts often yield good results quickly. For complex scenes, however, don’t shy away from detail – the model can handle it. Use **weighted cues and style tags** as with FLUX [dev] (the examples in Fal’s UI for [schnell] show prompts split into segments like “portrait | style | background | parameters” with weights like `::8` ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Image%3A%20portrait%20,s%201000))). This helps structure your prompt and instruct the model which aspects to prioritize. **Keep prompts focused:** since [schnell] converges in very few steps, extremely long prompts might not always utilize every detail; it’s often effective to emphasize the key elements with higher weights. 

**Advanced Tips:** FLUX [schnell] is great for **prompt chaining** – you can generate a quick image, then feed that image (using the `image_url` input of the FLUX *redux* endpoint or an image-to-image model) along with a refined prompt to improve it further. This two-step approach (fast draft then refined pass) leverages [schnell]’s speed and [dev] or [pro]’s quality. Also consider using **upscaling or enhancement** on [schnell] outputs if you need larger images – Fal provides upscaler endpoints (like RealESRGAN-based upscalers). In practice, [schnell] is often used to **test different prompt ideas quickly**, so you can iterate over styles or compositions in seconds. Take advantage of that by trying multiple small variations of your prompt (changing a weight or adjective) to see differences in output.

**Cost & Performance:** FLUX [schnell] is extremely cost-efficient – roughly **$0.003 per image** (for ~1MP images) ([ZimmWriter Image API Integration | www.rankingtactics.com](https://www.rankingtactics.com/zimmwriter-image-api-integration/#:~:text=www,The%20main)), an order of magnitude cheaper than [dev] due to its low step count. This low cost makes it feasible to generate many images quickly. Despite its speed, it maintains “cutting-edge output quality” comparable to larger diffusion models ([README.md · frankjoshua/FLUX.1-schnell at aa3b18d89cc7c592cc8291df10c9b5045cf3a5db](https://huggingface.co/frankjoshua/FLUX.1-schnell/blame/aa3b18d89cc7c592cc8291df10c9b5045cf3a5db/README.md#:~:text=%23%20Key%20Features%201.%20Cutting,personal%2C%20scientific%2C%20and%20commercial%20purposes)). It’s the **fastest FLUX endpoint** on Fal; as Fal’s docs claim, it’s likely the fastest in the world for this model ([Fastest FLUX Endpoint | fal.ai Docs](https://docs.fal.ai/fast-flux/#:~:text=We%20believe%20fal%20has%20the,beat%20it%20within%20one%20week)). There are no special usage limits, but note that by default it produces up to 4 inference steps – if you increase the steps (you can request up to, say, 4 or slightly above), generation time and cost increase slightly. The default image size is often 768×768 or 512×512 for speed; larger sizes will still be faster than other models, but will proportionally cost more (since pricing is per megapixel). 

### FLUX.1 [pro] v1.1 “Ultra” – High-Resolution Pro Model 
**About & Key Features:** FLUX.1 [pro] v1.1 Ultra is the latest professional-grade FLUX model ([FLUX.1 [dev] | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/flux/dev#:~:text=improves%20image%20quality%2C%20typography%2C%20prompt,res%20realism%20%2013)). It maintains the image quality of the earlier pro model but pushes resolution up to **2K output** with improved photorealism ([FLUX.1 [dev] | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/flux/dev#:~:text=improves%20image%20quality%2C%20typography%2C%20prompt,res%20realism%20%2013)). In other words, it’s tuned for high-resolution, highly realistic images. It’s suitable when you need the best quality Fal’s FLUX lineup can offer (e.g. marketing images, prints, etc.). 

**Prompting Best Practices:** Use **specific, vivid prompts** to take advantage of FLUX [pro]’s capabilities. Because it’s geared toward photorealism, prompting with photography terms can help (camera model, lens, lighting conditions, etc.). For example, adding details like *“Canon EOS R5, 50mm f/1.4, golden-hour lighting”* – as seen in Fal’s example prompts – can yield extremely realistic results ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Image%3A%20Close,and%20altruism%20through%20scene%20details)) ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Background%20hints%20at%20charitable%20setting,and%20altruism%20through%20scene%20details)). [Pro] also benefits from **structured prompts**: clearly delineate the subject, setting, mood, and any text you want embedded. It handles text in images better than most (though results may vary), so you can experiment with including short words/logos in the prompt. Keep in mind it’s a large model; a too-brief prompt might result in a generic image. Provide enough context for it to work with, or even a reference image (via the image-to-image “redux” variant) if you want to guide composition or style. Negative prompting is not explicitly documented (similar to other FLUX models), but you can still specify undesired elements in the prompt (e.g. “**, no crowds, no text**”). 

**Advanced Tips:** **Leverage high resolution:** you can request outputs up to 2048px with [pro] Ultra. However, very large outputs might benefit from providing an initiating image or using **dual-pass generation** (generate a smaller image, then use Fal’s upscaler or image-to-image on [pro] to recreate it at larger size). This model also likely allows **fine-tuning via Fal’s serverless model system** if you had a specialized need (Fal supports custom model deployment, though that’s beyond the scope of this answer). In terms of prompt strategy, treat FLUX [pro] similar to how you’d treat Stable Diffusion XL: be precise and consider using **artist or style references** if you want a particular look (the model should understand many artist names or style descriptors). You can also utilize **Fal’s LoRA tools** with [pro] – e.g. apply a LoRA style by referencing it in the Fal API if such are available for FLUX [pro]. This can imbue a specific aesthetic or character that the base model might not natively have.

**Cost & Performance:** FLUX [pro] v1.1 Ultra is the most expensive of the FLUX family – approximately **$0.05 per 1MP image** (roughly 5¢ for 1024×1024) ([Flux 1.1 Pro Ultra Mode Is Here | undefined - Flux Labs AI](https://www.fluxlabs.ai/blog/flux-11-pro-ultra-mode-is-here#:~:text=Flux%201%20Pro%3A%20fixed%20price,or%20Freepik%20if%20you)) ([ZimmWriter Image API Integration | www.rankingtactics.com](https://www.rankingtactics.com/zimmwriter-image-api-integration/#:~:text=www,The%20main)). The higher cost reflects the larger model size and higher resolution processing. It’s still billed per megapixel output, so generating a full 2K image will cost more (e.g. a 2048×2048 output is ~4 MP, roughly $0.20). In terms of speed, it’s slower than [dev] or [schnell] due to the increased computation for fidelity. Expect a slightly longer queue and generation time for Ultra outputs. Fal’s infrastructure uses powerful GPUs (often H100s) to handle these – as a developer, just be aware that *bigger images = more time/credits*. There may be rate limits to prevent abuse (large batch requests etc.), but under normal use you can generate images freely. For large-scale usage, Fal’s enterprise plans allow scaling out the inference if needed.

### Stable Diffusion 3.5 Large – Text-to-Image (MMDiT model)
**About & Key Features:** Stable Diffusion 3.5 Large is Stability AI’s latest diffusion model (a **Multimodal Diffusion Transformer**) ([Generating Images from Text | fal.ai Docs](https://docs.fal.ai/guides/generating-images-from-text/#:~:text=%2A%20fal,efficiency)). It introduces improved image quality, better understanding of complex prompts, and even decent handling of text (typography) in images ([Generating Images from Text | fal.ai Docs](https://docs.fal.ai/guides/generating-images-from-text/#:~:text=%2A%20fal,efficiency)). Notably, it’s more resource-efficient than its predecessors, meaning it can generate higher quality without drastically higher compute. This model is state-of-the-art for general-purpose image generation, benefitting from the vast training data Stability AI provides. It supports modalities like text+image input (ControlNet, etc.), making it “multimodal.”

**Prompting Best Practices:** **Rich, descriptive prompts** are recommended to fully utilize SD 3.5’s capabilities. Because it can understand long, complex prompts well, you can describe a scene in depth – for example Fal’s demo prompt: *“A dreamlike Japanese garden in perpetual twilight, bathed in bioluminescent cherry blossoms… The scene is captured through a cinematic lens… 8K resolution, masterful photography, hyperdetailed, magical realism.”* ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Prompt)) ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=creating%20an%20otherworldly%20atmosphere,masterful%20photography%2C%20hyperdetailed%2C%20magical%20realism)). This model will parse a paragraph-long prompt and attempt to satisfy as many details as possible. It’s helpful to **organize your prompt**: start with the main subject and setting, then add style and technical details (lens, lighting, art style). **Negative prompting is supported** – in Fal’s interface “Additional Settings” you can provide a negative prompt to steer the model away from certain content (e.g. “blurry, out of frame, text”). Use this to avoid common issues. Also, Stable Diffusion 3.5 responds well to **artist/style cues** (“in the style of Studio Ghibli” or “rendered in Unreal Engine”), and to **camera/photography terminology** for realism. If generating text in the image (like a sign or logo), it’s still challenging, but 3.5 is better than 1.x at legible text – keep the text short and include descriptors like *“clean lettering”*. 

**Advanced Tips:** Take advantage of the **multimodal features**: Fal’s UI for SD 3.5 exposes **ControlNet** inputs (you can provide a control image and choose a control type like depth, canny edges, etc.) and allows adding **LoRA models** ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Loras)) ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Image%20Encoder%20Path)). Advanced users can provide a sketch or pose via ControlNet to guide the composition while letting the model fill in details. Similarly, you can attach LoRA weights (for example, a LoRA that imparts a specific art style or a trained character) – Fal allows specifying these under “Loras” in the input. Another tip is to utilize the **“IP Adapter”** fields Fal shows ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Add%20item)) – this suggests image prompt adapters, which can align generated images to a reference image’s style. By uploading a reference under “Image Url” and toggling IP Adapter, you could influence the output toward that reference’s aesthetic. **Prompt chaining** is also useful: you might generate an initial image with SD3.5, then feed it back in with a refined prompt (image-to-image) to enhance certain features. For example, generate a base, then re-run with prompt “enhance details of [previous description]” at a higher guidance scale to sharpen. SD3.5 Large is quite powerful, so also consider using fewer inference steps than older models needed – it might reach good quality in 20–30 steps instead of 50. Adjust **CFG scale** (guidance scale) depending on output: if images are getting distorted or off-track, try a slightly lower scale (e.g. 7 instead of the default 8 or 9).

**Cost & Performance:** Stable Diffusion 3.5 is priced per image output on Fal. It is roughly on par with other diffusion models in cost; *for instance, Fal’s UI indicates about 15 images per $1 at default settings*, which works out to roughly **$0.067 per image** (for a ~1MP image). This is a bit higher cost than smaller Stable Diffusion models, reflecting the larger model size. However, its **“resource-efficiency” improvements** mean it doesn’t scale up cost drastically ([Generating Images from Text | fal.ai Docs](https://docs.fal.ai/guides/generating-images-from-text/#:~:text=%2A%20fal,efficiency)) – you’re getting better quality for a similar cost to older models. Performance-wise, it is heavier than SD1.5 or 2.1: generation may take a bit longer and uses more VRAM. Fal likely runs it on an A100 or better; you might see 5–10 seconds for a 512×512 image with default steps. The model supports up to 1024×1024 or higher, but larger sizes will use more credits (since cost is by pixel count). Fal’s API allows real-time streaming of Stable Diffusion results if you use their WebSocket (so you can see progressive results for long generations). There aren’t specific rate limits noted beyond standard Fal API limits. In summary, expect to pay a few cents per image and get top-tier results with moderate latency. For heavy use, consider Fal’s enterprise if you need to scale this model.

## Video Generation Models

### Minimax Video-01-Live – Text-to-Video (Hailuo AI)
**About & Key Features:** MiniMax’s Video-01-Live is a frontier text-to-video model, specialized in turning **static inputs into fluid animations** ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=static%20images%20to%20life%20and,of%20fal%E2%80%99s%20inference%20engine%2C%20this)). It excels in scenarios with high motion – e.g. lively scenes, camera movements – while maintaining **exceptional temporal consistency**, especially for faces ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=static%20images%20to%20life%20and,of%20fal%E2%80%99s%20inference%20engine%2C%20this)). It produces clips just a few seconds long (typically ~3–5 seconds) at moderate resolution. This model shines in both **stylized and realistic content**; it strikes a balance allowing cartoon-like results or real-life footage style. It’s considered a **“Live2D” model – ideal for bringing a single image to life** with motion.

**Prompting Best Practices:** You can use MiniMax Video in two ways on Fal: pure **text-to-video** (describe the scene and motion) or **image-to-video** (provide an initial image plus a prompt). For best results, use the **image-to-video mode with a reference image**. For example, supply an image of a character or scene, and prompt what action or camera motion should happen. The prompt can be relatively short: focus on the action and setting rather than excessive detail (since some details come from the image). If using text-only, describe the subject and the desired motion. MiniMax responds well to **action verbs and camera directions** – e.g. *“A painting of a ship at sea *starts to move*: waves crash, the ship rocks. Cinematic camera pan from left to right.”* Keep prompts in present tense as if describing a film scene. Because it handles faces well, you can use it for **talking avatar animations** or **dynamic portraits** – in such cases, prompt subtle motions (blink, head turn) to avoid unnatural movement. Note that **consistency of the subject** frame-to-frame is a strength, so you don’t need to repeatedly mention the subject in the prompt; set the scene once. Also, consider the **length** – Fal likely allows you to specify duration or it may be fixed ~4s. If duration is fixed, the model divides your described action into that time span. If it’s user-adjustable, be aware longer durations might reduce frame quality. 

**Advanced Tips:** Combine MiniMax with other tools for enhanced output. For instance, you can generate a still image with an image model (Flux or SD) and feed it to MiniMax to animate it – this **prompt chaining** ensures you get the exact character or art style you want, then adds motion. Use **camera instructions** liberally: MiniMax’s training emphasizes smooth camera moves ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=animations%2C%20excelling%20particularly%20in%20high,of%20fal%E2%80%99s%20inference%20engine%2C%20this)), so include terms like “camera zooms in,” “pan around,” “steadycam follows the subject,” etc., to exploit that cinematography understanding. If you have a specific motion in mind, you might also experiment with giving a **reference video** (if supported by the multi-conditioning “director” version of the model – Fal’s interface shows a “video-01-director” in their changelogs, which suggests an ability to use a reference video’s motion). When dealing with fast motion, ensure your prompt sets the scene clearly in the first frame (the model doesn’t get multiple prompts over time, just one overall description). For example, *“A middle-aged man stands in a desert outpost (start). He suddenly begins running as the wind kicks up sand (action). Camera shakes to emphasize speed (camera).”* The model will interpret this sequence of events within the short video. 

**Cost & Performance:** MiniMax Video-01-Live is **billed per output video** on Fal. Official Fal pricing indicates similar models (like Hunyuan) cost about $0.4 per video ([Pricing | fal.ai](https://fal.ai/pricing#:~:text=Pricing%20,megapixel%2C%20Billed%20by%20the)), so expect on the order of a few dimes per generated clip. It’s a **partner model** (Hailuo AI), implying it may not be open-source and thus carries a cost per run. Performance is quite good – it generates ~4-second 480p videos in roughly a minute or two. Fal’s infrastructure note mentions “under a minute” generation for top models ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=and%20realistic%20content,the%20fact%20that%20AI%20video)), and MiniMax is optimized, so your request might complete in ~30–60 seconds. There may be a queue for video models since they use a lot of GPU time (Fal will automatically queue and stream status). Also, **output frame rate** is decent (~24 FPS), giving smooth motion. However, video outputs are capped in length to keep inference feasible (likely 5 seconds max for this model by default). If you need longer videos, you’d have to generate in segments. Fal’s usage limits: typically, video models might have a concurrency limit (e.g. only a certain number of videos generating at once per user) – check Fal’s documentation if scaling up. Each generated video has a fixed cost; since MiniMax can also generate *with audio* (if combined with MMAudio), note that audio generation might be separate (MiniMax itself generates visuals only). 

### Hunyuan Video – Text-to-Video (Tencent Hunyuan)
**About & Key Features:** Hunyuan Video is a **large-scale (13B parameter) video foundation model** by Tencent ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=animations%2C%20excelling%20particularly%20in%20high,of%20fal%E2%80%99s%20inference%20engine%2C%20this)). It focuses on **cinematic video quality** with physically accurate motion and coherent scene transitions. In plain terms, Hunyuan produces short videos that look more like real movie footage: it handles camera movements, object interactions, and “real-world physics” (e.g. objects obey gravity, continuity of motion) exceptionally well ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=animations%2C%20excelling%20particularly%20in%20high,of%20fal%E2%80%99s%20inference%20engine%2C%20this)). It tends to generate a sequence with a clear beginning-to-end action (continuous action sequences) rather than disjointed frames. On Fal, it’s a flagship model for high-fidelity video and often requires powerful GPUs – the upside is Fal’s optimizations allow it to generate in **under a minute** for a clip ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=and%20realistic%20content,the%20fact%20that%20AI%20video)), which is quite fast for such complexity.

**Prompting Best Practices:** **Detailed scene descriptions** are important. Treat your prompt like a movie snippet script. Include the setting, characters, and the action, as well as cinematic cues. For example: *“A lone motorcycle speeds down a neon-lit city street at night, rain reflecting blue and pink lights on the asphalt. The camera follows from behind at a low angle, then swings to the side as the bike skids to a stop. Sparks fly as the tires scrape. (Cinematic, slow-motion as needed, dramatic lighting).”* In that prompt we described the environment, the action, and even the camera behavior. Hunyuan will attempt to honor those, producing a cohesive few-second scene. **Keep prompts logically consistent** – Hunyuan is good at continuity, so make sure your prompt doesn’t describe disjointed events that would be hard to resolve in a short clip. It’s better at realistic styles; you can prompt cartoon or fantasy, but its strength is making things look like real footage, so leveraging that (with terms like *“cinematic, 4K detail, film-like”*) will play to its strengths. **Specify motion clearly:** if you want a certain subject movement or camera cut, say so. Otherwise, the model might choose a default interpretation (like a simple pan). Because Hunyuan is trained for high quality, even short prompts can yield decent results, but providing context helps (who, where, when). Also, consider **temporal words** – use phrases like “slowly walks”, “suddenly jumps” to indicate timing of actions. If your scene has multiple actors, be mindful that the model might not perfectly maintain identity for more than one main subject (though it’s better than many models at it). It might be safer to focus on one primary subject in the prompt for clarity.

**Advanced Tips:** **Multimodal conditioning** could be very useful if supported: Hunyuan has an *image-to-video (I2V)* variant (Tencent released HunyuanVideo-I2V open-source for image input ([tencent/HunyuanVideo-I2V - Hugging Face](https://huggingface.co/tencent/HunyuanVideo-I2V#:~:text=tencent%2FHunyuanVideo,Video%20Model))). On Fal, if available, try providing a starting image via the Hunyuan Video LoRA endpoint or a special parameter. This can lock in the subject or background you want, and then Hunyuan will animate it with the described action. Another tip is to break complex sequences into smaller prompts and generate separate clips, then stitch them – because Hunyuan’s outputs are short, if you want a “story” longer than ~5 seconds, you might do multiple requests (e.g. scene1: man enters room, scene2: man picks up object). Use editing to concatenate them. **Leverage camera and lighting**: since this model excels at cinematic looks, you can specify camera lenses (e.g. “fish-eye view” vs “telephoto close-up”) and lighting conditions (“dusk with long shadows”, “strobe lighting in a club”). It will try to incorporate those and it makes the output feel professionally shot. **Motion physics** are a highlight – for example, if you describe “a ball bouncing three times then rolling to a stop,” Hunyuan is likely to get those beats correct. Don’t hesitate to describe nuanced motion (“limping run” vs “running” or “fluttering leaves in wind”). Finally, if available, use **Fal’s “Hunyuan Video LoRA”** endpoint ([Hunyuan Video | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/hunyuan-video#:~:text=fal,1)) or similar – this might allow fine-tuning or style adjustments on Hunyuan’s base model, giving you even more control (such as a specialized style or improved prompt adherence for certain content). 

**Cost & Performance:** Hunyuan Video is one of the pricier endpoints: **$0.40 per video output** (flat rate) on Fal ([Pricing | fal.ai](https://fal.ai/pricing#:~:text=Pricing%20,megapixel%2C%20Billed%20by%20the)). This means each clip (regardless of length up to the model’s limit) costs 40 cents. It’s billed per video rather than per second, likely assuming a standard clip length (e.g. ~4 seconds). In terms of performance, Fal’s integration is highly optimized – generation time is often ~30 seconds for a 4-second HD clip, which is remarkably fast given the model’s size. Fal’s team even boasted record-breaking generation times under a minute for cinematic content ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=and%20realistic%20content,the%20fact%20that%20AI%20video)). If you request the maximum resolution or length, times could edge up, but it’s still real-time relative to older methods. There might be **GPU time limits** per request (for example, Fal might limit to 8 seconds of 480p video to fit in memory/time). Keep outputs within recommended parameters to avoid errors. The **quality** is very high: expect 720p or higher frames with good coherence. For usage, Fal likely restricts usage to non-free tier given it’s partner model – you’ll need sufficient credits. Because each run is costly, test with lower settings or shorter durations if possible (to verify prompt effectiveness) before spending on full quality runs. 

### Kling 1.5 (Pro) – Image-to-Video / Text-to-Video 
**About & Key Features:** Kling 1.5 Pro is an advanced AI video generator known for its **1080p HD output** and enhanced physics simulation ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=model%20achieves%20record,also%20exponentially%20growing%20each%20month)). It’s a successor in the Kling series, which are models geared towards professional-quality video. Kling 1.5 can produce longer and more complex videos than many others, and particularly focuses on **realistic physical interactions** (for example, water flow, collisions, gravity) in the generated video. It’s often used for scenarios that demand a combination of high resolution and believable motion. The model was cutting-edge for 2024 and has since been updated (Kling 1.6), but 1.5 Pro remains relevant on Fal for slightly lower cost and as some users may prefer its style.

**Prompting Best Practices:** **Start with an image when possible.** Kling Pro models typically accept an initial image to guide the video (Fal’s interface for Kling 1.5 Pro is under “image-to-video” by default ([Kling 1.6 | Image to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/kling-video/v1.6/pro/image-to-video#:~:text=Kling%201)) ([Kling 1.6 | Image to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/kling-video/v1.6/pro/image-to-video#:~:text=Input))). Providing a keyframe or character image ensures the output video looks consistent with that. For example, supply a photo of a person and prompt them to perform an action. If you don’t have an image, you can still do text-to-video, but results may vary more in appearance. When writing the prompt, describe both the **scene and the movement**. Kling handles diverse situations, but it helps to emphasize what should happen physically. For instance: *“A red sports car drifts around a corner on a race track, kicking up smoke. The camera is stationary at the corner as the car enters frame, slides, then exits frame. Debris and smoke follow the tire motion.”* That prompt gives a clear physical scenario (drifting car with smoke, debris) which Kling can simulate (smoke and debris following physics). **Mention the resolution or detail** if needed – since Kling can do 1080p, you might say “in detailed HD” or similar to push it toward using full capacity. For style, if you want a particular look (cinematic vs. CCTV vs. animated), mention it. Kling is quite general; it doesn’t inherently lean cartoonish or realistic – your prompt can nudge it. Using **present tense and descriptive language** (like “walks,” “explodes,” “slowly pans”) will help it map timing. 

**Advanced Tips:** **Effects and multi-condition inputs:** Fal’s listing shows a “/effects” endpoint for Kling 1.5 Pro ([Luma Dream Machine | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/luma-dream-machine#:~:text=fal,5%20%28pro%29%20new)), suggesting you can input not just an image but possibly a **video or segmentation mask as additional conditioning**. If you have a reference video for motion or a mask to constrain the region of generation, use those advanced inputs via the API. For instance, you could input a rough animation (even stick figures) and have Kling render it in HD with your described scene – that’s speculative but indicated by “effects” features. Another advanced strategy is to exploit **the physics engine aspect**: test prompts that involve interactions (e.g. “a glass bowl shatters on the floor”) – Kling’s training includes physics, so it might handle these better than other models. Also, **lighting and camera**: since it’s pro, it likely supports complex lighting (shadows, reflections) and camera moves. You can do multi-sentence prompts where the second sentence is about camera work, like “Camera follows the glass pieces on the floor in slow motion.” Given Kling’s resolution, you can also extract high-quality frames (even use the output as images). If you need to extend a Kling video’s length, you can try a trick: generate sequential clips where the last frame of one is the first frame of the next (Fal’s API might allow you to feed the last frame as an initial image for the next segment). This can approximate a longer continuous video by pieces. It’s labor-intensive, but an advanced user could semi-automate it. Additionally, consider **post-processing**: using Fal’s **video upscaler** on Kling outputs or their **frame interpolator** if any, to possibly smooth or lengthen the video. Kling 1.5 outputs are already HD, but if you generate at a slightly lower res for speed (say 720p) you could then upscale after. 

**Cost & Performance:** Kling 1.5 Pro is **billed per second of video** on Fal’s platform. The pricing is around **$0.10 per second of generated video** ([Pricing | fal.ai](https://fal.ai/pricing#:~:text=Pricing%20,megapixel%2C%20Billed%20by%20the)). So a 5-second clip costs roughly $0.50. This usage-based pricing makes sense since users can often choose the duration. It’s a bit cheaper than Kling 1.6 (which is the same rate; 1.6 is $0.10/s as well) – essentially, Kling models cost ~$6 per minute of video on Fal’s API. Performance: it’s a heavyweight model, so generation might take on the order of **seconds per frame**. A 5-second (120-frame) 1080p video could take a few minutes to generate. Fal’s documentation suggests using a **high-end GPU backend** for these (and indeed Fal offers H100 instances). Expect, say, 2–3 minutes wait time for a clip on the regular queue. Because it’s high-res, the file sizes can be a few MBs for a short clip. Fal returns a URL to download the video when ready. On usage constraints: Fal might restrict maximum length (for example, possibly 10 seconds max to prevent excessive GPU use in one request). Also note that Kling models might be subject to content moderation (they are powerful – Fal likely blocks NSFW or harmful content prompts). All told, Kling 1.5 Pro gives excellent quality at a cost – use it when 1080p or complex physics are required. For simpler needs, you might use a cheaper model or lower resolution.

### Kling 1.0 (Standard) – Text-to-Video (basic version)
**About & Key Features:** Kling 1.0 Standard is an earlier generation model in the Kling video series. It produces somewhat lower resolution outputs (often 720p or less) and less detailed physics compared to the Pro version. However, it’s still capable of taking a text prompt (or an image + prompt) and generating a coherent video clip (a few seconds long). As a **standard tier**, it was designed to be more accessible (faster and lighter) at the cost of some fidelity. It’s suitable for quick prototyping or content where 1080p isn’t needed. This model is now a bit older, but Fal still provides it, likely because it runs faster or on less powerful hardware – offering a **cost-effective option** for video generation.

**Prompting Best Practices:** Use **simpler, concise prompts** for Kling 1.0. It doesn’t capture fine-grained details as well as the newer models, so focus on the primary action or scene. For example: *“A small puppy jumps into a pile of leaves.”* That might yield a cute, simple animation. If you heap too many details (lighting, exact camera moves, etc.), the model might struggle or produce muddier results. It’s best at **single focused actions** or scenes. If using image-to-video, provide a clear, front-and-center subject in the image (the model will then animate that subject doing something you describe). Kling 1.0 may not maintain identity perfectly through complex motion, so prompts with big changes (e.g. subject turning into something else) might not be stable – stick to one subject. **Leverage styles**: you can explicitly request a cartoon or an illustration style if you want – standard models sometimes adhere to style cues more easily (since they have less emphasis on realism). For instance, add *“in the style of a pencil sketch”* if you want an animated sketch look. Conversely, if you want realistic, mention *“realistic”* or camera terms to anchor it, though expect slightly less realism than Kling 1.5 or Hunyuan. Also, keep the prompt **short in chronology** – basically describe one scene, not multiple cuts. Kling 1.0 likely can’t do a complex multi-part sequence in ~3 seconds. 

**Advanced Tips:** One useful approach with Kling 1.0 is using it for **storyboarding**. Because it’s cheaper/faster, you can try out various scenes quickly, then upscale or refine with Kling Pro or another model. You can also experiment with its **“effects” mode or multi-conditioning** if available (Fal shows Kling 1.0 Standard under image-to-video and possibly “effects” similar to 1.5). This might let you input, say, a segmentation map or pose to guide the video. For example, you could draw a simple stick figure animation of the motion you want, feed that as a conditioning video, and have Kling render it with your prompt’s appearance. This is advanced and requires knowledge of Fal’s API (and whether they exposed that feature publicly). Another tip: because the output isn’t the highest resolution, you can more safely use Fal’s **video upscaler** on it to enhance resolution. The RealESRGAN upscaler for video can sharpen and upsize a Kling 1.0 clip fairly well since it doesn’t have to deal with super fine details (you’ll basically get a cleaned-up 1080p version of a 720p output). Just keep an eye on any artifacts – upscaling will exaggerate any weirdness from the original. If the model sometimes produces **jitter or flicker**, a trick is to generate a slightly longer video and then trim the ends (often the start or end frame might be less consistent). Or generate multiple and pick the one with the least flicker. Finally, consider **mixing audio** after generation: Kling 1.0 doesn’t generate sound, but pairing it with an audio model like MMAudio or adding sound effects manually can significantly improve the perceptual quality of the final video.

**Cost & Performance:** Kling 1.0 Standard is more economical. It’s typically **charged per second of video** like the Pro, but at a lower rate (Fal’s pricing might not list it explicitly, but one can infer it’s lower – possibly around **$0.05 per second**). In Fal’s UI, it might even show how many runs per $1; e.g., it might say something like “for $1 you can run ~20 times,” indicating a cheaper cost per clip. In any case, it’s roughly half the price of Kling Pro. Performance is faster – running on slightly less compute, maybe it generates frames a bit quicker. Users have reported around *4-5 seconds per video generation* with LTX or similar models on Fal ([Playing with the new LTX Video model, pretty insane results ... - Reddit](https://www.reddit.com/r/StableDiffusion/comments/1h1bb0f/playing_with_the_new_ltx_video_model_pretty/#:~:text=Playing%20with%20the%20new%20LTX,5%20seconds%20per%20video%20generation)), and Kling 1.0 standard would be in that ballpark when it launched. So expect perhaps ~10–30 seconds for a short clip to generate, depending on length and complexity. The output resolution might be around 576p or 720p by default, which speeds things up. There may be limits like max 3 seconds length (some early Kling versions had shorter max durations). But you can run multiple in parallel more easily since it’s lighter. Overall, it’s a good **budget option**: faster and cheaper, at the cost of some quality. Many developers use it during development, then switch to Kling Pro or Veo2 for final high-res results.

### Luma “Dream Machine” v1.5 – Text-to-Video 
**About & Key Features:** Luma Dream Machine 1.5 is a video generation model by Luma Labs that is notable for **immersive scene generation and consistent character interactions** ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=Luma%20Dream%20Machine%20is%20a,more%20ideas%20and%20dream%20bigger)). It’s built to handle both text and image inputs, creating **realistic videos from text or images** ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=Luma%20Dream%20Machine%20is%20a,more%20ideas%20and%20dream%20bigger)). Dream Machine places emphasis on maintaining accurate physics and interactions (similar to Hunyuan/Kling) and also showcases **cinematic camera movements** ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=Dream%20Machine%E2%80%99s%20advanced%20capabilities%20ensure,some%20limitations%2C%20the%20team%20is)). In practice, it’s used for creative storytelling – the model attempts to translate imaginative prompts into vivid video clips. It’s often praised for efficient generation and has been offered on Luma’s own platform as well; Fal’s integration makes it accessible via API. This model is a blend of artistic and realistic capabilities, hence the name “Dream Machine” – it’s meant to realize your imagined scenarios in video form.

**Prompting Best Practices:** **Imagination is key** – don’t be afraid to feed it fantastical or complex prompts. Luma DM is designed to accommodate “dreamlike” sequences. However, include concrete visual elements so the model has something to render. For example: *“A castle made of clouds floats in a sunset sky. Golden light beams through the towers as flying whales circle around it. Camera orbits slowly to reveal a distant rainbow horizon.”* This prompt is fanciful but provides clear visual cues (castle, clouds, sky, whales) and an action (camera orbits, whales circle). The model can take that and animate a short clip capturing the essence. **Include motion or changes** – even if subtle (clouds drifting, etc.), because video models need something to animate. Pure static scenes may result in only minimal motion. Luma DM, as the name suggests, handles **multiple subjects interacting** better than some; you can have, say, a person and an object and it should keep track of both. But try to keep it to one primary focal point to avoid confusion. Also, leverage its cinematic flair: asking for specific camera moves (orbit, zoom, POV, tilt) and atmosphere (lighting, mood) will enrich the output. For style, you can push it to either realistic or stylized: e.g., add *“in Pixar style”* for a cartoon vibe, or *“photorealistic 4K detail”* for realism. It’s quite versatile. **Keep prompt length moderate** – a brief paragraph is fine, but a super long prompt might not improve output further and could introduce incoherence. Finally, if using image-to-video mode, provide an image that aligns with your prompt (e.g., an image of a castle if your prompt is about a castle in the sky), so the model has a solid starting point.

**Advanced Tips:** **Multiconditioning and “Ray 2”**: The search results hinted at something called “Ray2 flash model” and Dream Machine updates ([Luma Dream Machine Just Got BETTER – Video to Audio & NEW ...](https://www.youtube.com/watch?v=Ms-UVcA64Uw#:~:text=Luma%20Dream%20Machine%20Just%20Got,them%20both%20to%20the)). This suggests Luma has multiple models or modes (maybe one for faster generation called Ray). On Fal, Dream Machine might have an update or a parameter for “Ray” (real-time mode) vs quality mode. If available, you could choose faster generation at slightly lower quality for drafts, then switch to high quality. Also, being a partner model, it might allow **longer durations or higher resolution** if you have the credits. Luma DM possibly can go beyond 5 seconds (some demos on Luma’s site show ~8 second clips). If Fal permits, try requesting a bit longer video if your scene needs it – the model is optimized for quality even at extended frames. Another tip: **control the beats of the video** by structuring your prompt with a sequence of events. For instance, use sentences that imply first and second halves of the video: “A wizard stands in a field of stars. He raises his staff (beginning). Suddenly, the stars swirl around him forming a galaxy (climax).” The model might interpret that as an evolving scene. Additionally, you can combine Dream Machine with Fal’s **video-to-video or interpolation tools**. For example, generate two different Dream Machine clips and then morph or interpolate between them using an interpolation model if one exists on Fal – that could create a longer continuous video from two segments. As for consistency, if you have a main character across clips, use the **same prompt wording** for that character in both clips to help the model keep it similar. Dream Machine is also known for **fast iteration** – the Luma team highlights speed ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=creation.%20It%20generates%20high,more%20ideas%20and%20dream%20bigger)), so you can quickly try different ideas. Embrace that by trying multiple variants of your prompt (different camera angle, different time of day) to see which you like best. 

**Cost & Performance:** Luma Dream Machine is priced around **$0.50 per video** generated ([Luma Dream Machine | Text to Video | AI Playground - Fal.ai](https://fal.ai/models/fal-ai/luma-dream-machine#:~:text=Generate%20video%20clips%20from%20your,can%20run%20this%20model)). Fal’s interface notes “Your request will cost $0.5 per video” for this model. So, each clip (likely ~4 seconds) is half a dollar. The performance is quite good: generation is usually on the order of seconds per second of video. Dream Machine has been optimized for speed (the Luma folks tout rapid iteration), so you might get results in ~20–30 seconds for a short clip. It may not be as heavy as something like Veo2, which is why the cost is a bit lower than Veo2’s per-second pricing. *For $1 you can run ~2 videos*, Fal indicates, which aligns with $0.5 each. If you want longer videos, you would pay proportionally (e.g., $1 for an 8-second video if allowed). Check Fal’s model card for any length restrictions – if not stated, assume ~5 seconds default. On usage, since it’s a partner model, you need adequate credits; there likely isn’t a free tier run for this. But it being available on Luma’s own platform means it’s battle-tested. Fal’s hosting likely uses powerful GPUs and possibly multi-GPU inference to accelerate it. **Reliability:** fewer “glitches” – users report that Dream Machine outputs are relatively stable with fewer artifacts ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=Dream%20Machine%E2%80%99s%20advanced%20capabilities%20ensure,impressive%20capabilities%20in%20the%20future)) (like less flicker or weird distortions). So you can generally trust the output quality. If anything, any inconsistencies might come from trying to do too much in a short clip. If you stay within a straightforward scene, it performs near state-of-the-art for its class.

### MMAudio V2 – Video-to-Audio (Sound Generation)
**About & Key Features:** MMAudio V2 is an audio generation model that takes **video (and optionally text) as input and produces a synchronized audio track** ([MMAudio V2 | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/mmaudio-v2#:~:text=fal)). In essence, it’s designed to add suitable audio to a silent video. This could be background music, sound effects, or ambient audio that matches the content of the video. MMAudio stands for *Multi-Modal Audio*, indicating it can use multiple modalities: it “watches” the video frames and “reads” the text prompt to decide what audio to generate. A key feature is that it outputs an **audio-augmented video file** (it returns the video with sound overlayed) ([MMAudio V2 | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/mmaudio-v2#:~:text=OpenDownload)), making integration easy. It’s fast and optimized for synchronization, meaning events in the video should align with audio cues (for example, if it sees an explosion in the video, it will generate an explosion sound at that moment). It’s particularly useful for automatically sound-designing AI-generated videos from other models.

**Prompting Best Practices:** Always provide **both inputs**: a video and a brief text describing the desired audio style. The video input is mandatory (the model needs visuals to sync with), and the text prompt guides what kind of audio. For instance, if you have a clip of a busy city street, you might prompt “city street ambiance with honking cars and footsteps” – MMAudio will then generate traffic noise, car horns, and footstep sounds in time with any visible cars or people in the video. Keep the text prompt concise and **focused on audio aspects** (e.g. genre, instruments, types of sounds). If you want music, specify genre or mood: *“upbeat electronic music”* or *“tense orchestral score”*. If you want real-world sounds, specify them: *“ocean waves and seagulls”*. The model will mix elements as appropriate. **Match the prompt to the video content** – if the video is of a dog barking but you prompt “quiet library ambiance,” that mismatch may confuse the result or just yield silence even though the dog’s mouth moves. So describe what *should* be heard given what is seen. Also, mention **timing cues** if needed: for example “start with silence then crescendo as the object appears” if that’s important, though the model primarily aligns automatically. Since MMAudio is v2, it’s likely improved at coherence and quality, but it’s still good to avoid extremely complex audio descriptions in one go. One or two sentences are enough. 

**Advanced Tips:** Use MMAudio in a **workflow chain**: generate video with another model (Flux, Kling, etc.), then pass that video to MMAudio with an appropriate prompt to add sound. This effectively creates a complete audiovisual clip. You can also feed **real videos** (not AI-generated) to MMAudio to test its Foley abilities – for example, give it a clip of someone juggling and prompt “gentle background music and soft thuds when balls caught.” This model can generate both music and sound effects simultaneously. If you want **music only or SFX only**, clarify that in the prompt (“no sound effects, only background music” or vice versa). Conversely, if you want a full mix, mention the elements (“rain sounds and soft piano music”). The synchronization is a standout feature, but if you find slight misalignments, you might experiment with prompt phrasing (e.g., explicitly saying “when X happens, [sound]”). Also, note Fal lists an alternate usage as `text-to-audio` for MMAudio ([Model Gallery - Fal.ai](https://fal.ai/models?=keywords=audio#:~:text=Model%20Gallery%20,audio.%20MMAudio)) – which suggests you can use it to generate an audio file from text alone (like a music generator akin to MusicGen). If you call the `.../text-to-audio` endpoint with just a prompt, it might produce sound unrelated to any video. That’s useful if you need a standalone sound. Another tip: combine MMAudio with **voice models** for a complete result. For example, if your video is a person talking, you’d use a voice TTS to generate speech audio, then use MMAudio to add background ambience and perhaps auto-duck (lower volume) the background when speech is happening. Currently, MMAudio doesn’t know about the speech content (it would only see the mouth flaps, not the actual words). So coordinate it by generating speech audio first and then instruct MMAudio accordingly (e.g. “add city ambient noise under dialogue”). If precise control is needed, you might generate separate layers (music vs SFX) by running MMAudio multiple times with different prompts and mixing audio tracks externally. But in most cases, a single run with a well-crafted prompt yields a complete synced audio track.

**Cost & Performance:** MMAudio V2’s pricing isn’t explicitly listed per second, but since it’s an **audio-generation** model, it’s likely billed by audio duration. Many TTS/audio models on Fal cost on the order of a few cents per minute. It’s reasonable to expect something like **$0.02–0.03 per second of audio** (which is roughly $1.80 per minute). This is in line with Fal’s TTS (PlayHT v3 is $0.03/minute) but likely higher since music/sound generation is heavier. If the video is, say, 5 seconds, the cost might be ~$0.10. Fal’s interface might state it like “$X per minute of audio.” In any case, adding audio is generally cheaper than generating the video itself. Performance: MMAudio is **fast – near real-time**. Generating a few seconds of audio should only take a couple of seconds of processing. It’s designed to not waste time; Fal even notes that adding the output audio is done in a background thread so it **doesn’t charge GPU time for non-inference tasks** ([Fastest FLUX Endpoint | fal.ai Docs](https://docs.fal.ai/fast-flux/#:~:text=Note)). You can expect low latency. The model can probably generate common sample rates (likely 22kHz or 48kHz audio). There might be a limit to audio length (likely tied to video length, which might max out ~10s unless you have a special plan). If you input a longer video, Fal might either cut it or require a separate processing approach. Quality-wise, MMAudio v2 improved on v1’s coherence, but occasionally the audio might sound a bit generic or “AI-ish.” For critical use, minor post-editing of the audio (equalizing levels, etc.) could help. However, for most projects, it’s a huge time-saver to get synchronized sound automatically. No known specific usage limits except the general Fal rate limiting; you can call this for each video you generate to quickly sound-enable them.

### Sync.So Lipsync 1.8.0 – Video-to-Video (Audio-driven Animation)
**About & Key Features:** The Sync.So Lipsync model (v1.8.0, now a 1.9.0-beta on Fal ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=sync.so%20,to%20Video))) is a specialized model that **animates a video (typically of a face) to lip-sync with a given audio**. Essentially, you give it a static face video (or even just an image treated as a video) and an audio clip of speech, and it outputs a video where the face’s mouth (and possibly expressions) move in sync with the speech ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=Video%20Url)) ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=Audio%20Url)). It uses advanced algorithms to ensure high-quality synchronization – the goal is a realistic talking head. This model does not generate the voice – you provide the voice/audio – but it generates the facial movements. It’s great for dubbing, creating virtual presenters, or giving a voice to still images. Sync.So is known in the space for good quality lipsync, and this model being on Fal indicates a collaboration or usage license for developers.

**Prompting Best Practices:** For this model, “prompting” is more about **providing the right inputs** rather than a text prompt. The inputs are: a **video (or image) of the person** and an **audio clip** of the speech ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=Video%20Url)) ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=Audio%20Url)). Ensure the video/image has the person’s face clearly visible, ideally facing the camera or only slightly turned – this gives the model a good template to animate. The audio should be clean speech (or singing). There is typically no text prompt needed, since the audio itself guides the lip movements. However, if Fal’s interface had an “Additional Settings” or textual input, it could allow some control like choosing **intensity of expression** or enabling head movements. In absence of that, just focus on input quality: e.g. a **neutral expression image** works best, so the model can animate mouth and slight expressions on top of it. If the person’s lips are closed in the image, that’s fine – the model will open them as needed. Avoid images where the mouth is extremely open or in odd positions initially. For the audio, note the language – the model likely supports multiple languages (depending on training), but English speech is the safest bet for best sync. The length of the output video will match the audio length. So if you have a 15-second audio, ensure your input face video or image is okay to be used for that duration (a single image can be used for 15 seconds; the background will just be static unless the model does slight head moves). One more tip: trim any silence at the start of the audio, because the model will still produce video for it (maybe with closed mouth). If you want a natural lead-in, you can leave a bit of silence – the person would appear listening then start talking. 

**Advanced Tips:** **Facial enhancements:** sometimes just moving lips isn’t enough for realism. The Sync.So model might also animate jaw movement, subtle head nods, eye blinks, etc. If you find the output too static except the lips, consider using an additional tool to add *micro-expressions*. For example, you could first use Sync.So to get the perfectly synced lips, then run the video through an **animation model (like an expression transfer or a subtle head-movement generator)** to add a bit of natural motion. Alternatively, when preparing your source image/video, you could provide a short video of the person blinking or nodding neutrally for a second, and use that as the input video (the model will use those frames to keep the person “alive” during speech, possibly). Another trick: if the voice audio has emotion (excited, sad), the model might not know to change expression; you could manually pick a source image with a matching expression (smiling for upbeat audio, etc.). That way the overall tone matches. Also, **backgrounds** – if your input is just an image with a background, that background will stay static. To avoid any perception of stutter, sometimes people blur the background or use a solid color. If needed, remove the background (Fal has a background removal model ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=fal,with%20evolved%20consistency%21%20animation%20stylized))) before lipsync, then you can composite a video background later. This avoids any weirdness like a moving jaw causing background pixels to flicker (if the model tries to move the whole head). The Sync.So model is likely robust, but these are considerations for polish. If you want to get very advanced, multiple speakers in one video could be done by switching inputs: e.g., for dialogue, run model on PersonA’s face with PersonA’s audio, then PersonB’s face with PersonB’s audio, then edit the two resulting videos together. The model itself doesn’t take multiple audio streams at once (no multi-speaker in one run). Finally, since this is a beta version, check for any artifacts around the mouth. If you see some, a light post-process in a video editor (color correction or slight blur on the mouth region) can help hide minor glitches.

**Cost & Performance:** Lipsync models are relatively heavy but not as bad as full video generation. You’re basically doing face motion, which involves a neural renderer. The pricing on Fal is around **$0.05 per minute of output** audio/video ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=)). That corresponds to 5¢ per minute (or about $0.00083 per second). This is actually quite affordable – indeed Fal’s UI said *“$0.05 per audio minute”* for the dialog TTS which is similar in cost. It means a 10-second clip costs less than a cent. The reason is that the model likely runs on efficient hardware (maybe it uses a GPU for a short time). Performance wise, **it’s fast**. Expect that a 10-second clip will generate in a few seconds. Even a minute of video might only take a handful of seconds to process on a modern GPU, since the model might leverage a precomputed facial geometry and then just render frames quickly. Possibly it could even work in real-time (some lipsync systems run at >20fps). On Fal, after submitting, you’ll likely get the result within the length of the audio or faster. There aren’t typically strict limits on duration from a technical standpoint – you could do a full minute monologue. However, extremely long inputs (e.g. 5 minutes) might be split or not allowed due to memory. Also, ensure the audio sampling rate is what the model expects (Fal likely handles this – just feed common formats like MP3/WAV). In terms of usage, since it’s a partner model (by SyncSo), Fal allows commercial use but presumably expects you to abide by ethical guidelines (e.g. don’t deepfake someone without consent, etc.). The output quality is usually high: as long as your input face is high-res, the output will be correspondingly high-res video with synched lips. Overall, cost is minor and speed is high, making it a very handy tool to use in your pipeline without much worry about expense or delays.

### Veo 2 – Text-to-Video (Google’s Model)
**About & Key Features:** Veo 2 is Google’s cutting-edge video generation model, recently integrated into Fal’s API ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=We%27re%20thrilled%20to%20announce%20that,most%20advanced%20AI%20video%20technologies)). It represents a significant leap in video synthesis, offering **unprecedented quality and control** in AI-generated videos ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=Setting%20New%20Standards%20in%20Video,Generation)). Key features include a superior understanding of **physics and human motion**, leading to natural animations, as well as **cinematographic excellence** – the model is aware of camera techniques and can produce complex shots (e.g. tracking, zoom, depth-of-field effects) ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=,Significantly%20fewer%20unwanted%20artifacts%20or)). Another major feature is its ability to output at **high resolution (up to 4K)** and handle extended durations (minutes-long videos) ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=understands%20the%20language%20of%20filmmaking,Significantly%20fewer%20unwanted%20artifacts%20or)), which is far beyond typical models. Veo2 also greatly reduces unwanted artifacts and flicker, making it *production-ready*. Essentially, it’s the state-of-the-art as of 2025 for text-to-video, encapsulating Google’s research advancements in this area. Fal’s integration allows developers to use this powerhouse via simple API calls, though at a premium cost.

**Prompting Best Practices:** **Think like a director.** When prompting Veo2, you have a lot of control, so writing a prompt akin to a movie scene description is effective. Include details on **setting, actors, actions, camera, and even lens/film style** if desired. For example: *“Cinematic shot of a female doctor in a dark yellow hazmat suit, illuminated by harsh fluorescent lab lighting. The camera slowly zooms in on her face, then pans gently to reveal the anxious expression in her eyes ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=Cinematic%20shot%20of%20a%20female,anxiety%20etched%20across%20her%20brow)). She stands before a laboratory table with beakers, slight motion blur on her hands as they tremble.”* This kind of prompt leverages Veo2’s strengths: it mentions lighting, camera movement, emotion, etc. The model will attempt to execute those directions: a slow zoom, a pan, and a focus on the facial expression. **Be specific but not overstuffed.** Veo2 can parse complex prompts, but ensure clarity – break the prompt into sentences for different parts of the scene (as in the example). You can also use commands like *“Shot with a 35mm lens on Kodak Portra 400 film”* to imbue a certain photographic quality (that was in Fal’s example prompt ([Veo 2 | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/veo2#:~:text=The%20camera%20floats%20gently%20through,film%2C%20the%20golden%20light%20creates))). If you want a certain duration or pacing, you might have to imply it (e.g. “slowly over 8 seconds, the scene unfolds…” though the actual duration depends on what you request via API). **Leverage physics**: if your scene has physical interactions (explosions, water, running, flying objects), describe them – Veo2 will handle the causality and realism better than other models. Also, utilize the fact that it can do longer videos: you can actually describe a beginning, middle, end in the prompt. For instance, “First, the camera does X… Then Y happens… Finally Z.” Veo2 might follow that sequence since it can generate more frames (though there’s no guarantee it splits exactly, it will try to incorporate the whole story). **Style**: if you want consistency with a particular visual style (say Pixar-like animation or monochrome noir film), state it – Veo2 is versatile and can emulate styles given its training on diverse footage. One more tip: Because Veo2 aims to reduce hallucinations, you likely *don’t* need to heavily negative-prompt. Still, avoid ambiguous references in your prompt that could confuse context. Keep the scene self-contained (the model doesn’t know earlier prompts or images unless provided).

**Advanced Tips:** **Use extended features if available:** Fal’s mention of “extensive camera controls” ([Veo 2 | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/veo2#:~:text=fal)) and Veo2’s abilities suggests there may be special parameters you can pass (possibly via the API rather than plain prompt) to control aspects like focal length, aperture (for depth of field), or even a **storyboard of keyframes**. If Fal’s API or OpenAPI schema for Veo2 allows inputting something like a sequence of text prompts with timestamps or keyframe images, you could truly control minute details of a multi-second video. This would be an advanced usage where you break down a 20-second video into 4 segments and prompt each for the segment’s content – Veo2 could then blend them if such interface exists (this is speculative; if not directly available, the model still might interpret a multi-sentence prompt sequentially). Also, explore **multi-modal inputs**: given its roots in research, Veo2 might accept a **reference image or video** (e.g. to anchor style or initial frame). If so, you could feed it a starting keyframe from which to build the video, ensuring consistency of a character or environment. Another advanced technique is using **Veo2 for longer outputs by chaining**: because Fal charges per second and the model can do minutes, you might still not want to do a full minute in one go (cost!). Instead, generate, say, a 15-second segment, then use its last frame as the first frame (image) for the next segment, with a prompt continuing the action. This can let you create a multi-part sequence that’s longer. You will have to manually stitch, but the continuity should be good if you carry over the last frame. Additionally, **adjust output resolution deliberately** – if you want a 4K output but are prototyping, start with 720p or 1080p to test prompt, then scale up the resolution parameter for final runs. Veo2 can do 4K ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=understands%20the%20language%20of%20filmmaking,Significantly%20fewer%20unwanted%20artifacts%20or)), but generating at that resolution is heavy; only do it when you’re satisfied with the scene at lower res. For the technically inclined, note that Veo2 likely uses a huge model (maybe >30B parameters and diffusion steps), so it’s slow – any trick to reduce required frames (like asking for 24 FPS instead of 30, or 5-second instead of 8-second) can save time and credits if it doesn’t sacrifice your vision. Because hallucinations are reduced, you can also trust it a bit more with **literal prompt** – e.g., it might actually render text on signs correctly (though that remains challenging). If it does handle text, you could even try including short dialogue or subtitles in the video by prompting a character to speak with visible speech (though more reliably, you’d overlay text in post). 

**Cost & Performance:** Veo2 is the most expensive model listed. Fal’s pricing for Veo2 is **$2.50 for the first 5 seconds, and $0.50 per additional second** ([Veo 2 | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/veo2#:~:text=For%205s%20video%20your%20request,50)). That means a 5-second video is $2.50, 6 seconds is $3.00, 10 seconds is $5.00, etc. Essentially **$0.50/second** after the 5-second minimum. This aligns with the model’s complexity and the fact it can produce high-res, lengthy clips. So, a 30-second 1080p video would cost $2.50 + 25*$0.50 = $15.00, which is steep but for what it is (perhaps replacing a whole video crew for a short B-roll) it could be worth it. Performance: It is heavy. Expect that 5 seconds of video may take on the order of a couple of minutes to generate (the exact speed depends on Fal’s hardware – they might allocate multiple GPUs or TPUs given the partnership with Google). Fal likely uses top-tier accelerators (maybe Google TPUs, since Google model) to serve this. Even so, generating each frame in 4K with a huge model could be slow. If time is critical, you might generate at lower resolution or fewer FPS. The model can do up to 4K@??fps, but you could request 720p@24fps to save time. The output will be delivered as a file URL; because it’s potentially large (4K frames * X frames), Fal might have a short retention on those outputs – make sure to download promptly. There are probably strict usage limits for Veo2: Fal might limit each user to certain seconds per day, or require an application for extended use, given the cost and load. They even mention giving out coupons on social media for free tries ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=Image%3A%20Veo2%3A%20State,Now%20on%20fal)). So treat this as a premium resource. In terms of quality, it’s state-of-art: you’ll see far fewer glitches, and the video will likely impress with how “film-like” it is. The physics and camera control mean it’s less trial-and-error – if your prompt is good, first output might be spot on. That helps offset the cost (you might not need many reruns). All in all, Veo2 offers top quality at high cost, suitable for when you need the **absolute best video generation Fal can provide** and are willing to pay for it.

## Music & Audio Generation Models

### MiniMax Music (Hailuo AI) – Text-to-Music
**About & Key Features:** MiniMax Music is a generative music model that creates **short musical compositions from text prompts**. It leverages advanced AI techniques (likely a transformer or diffusion in audio domain) to produce high-quality, diverse music ([MiniMax (Hailuo AI) Music | Text to Audio | AI Playground - Fal.ai](https://fal.ai/models/fal-ai/minimax-music#:~:text=Generate%20music%20from%20text%20prompts,quality%2C%20diverse%20musical%20compositions)). It can include not just instrumentals but potentially vocals or humming if lyrics are provided (some descriptions hint it can handle lyrics and vocals in style of a reference track ([image-01 replicate.com api & minimax image-01 github AI Model](https://www.toolify.ai/ai-model/minimax-image-01#:~:text=image,2K))). The model is geared to output up to around 1 minute of music per prompt (possibly around 20–30 seconds is typical). It’s a partner model (Hailuo AI, same family as MiniMax video), suggesting it might excel in certain styles or have unique training focusing on contemporary music. Key features: *multi-genre support* (classical, EDM, rock, etc.), *coherent musical structure* (with a start, development, and end), and *possibly the ability to follow provided lyrics/melody hints*.

**Prompting Best Practices:** Write prompts that describe the **genre, mood, tempo, and instrumentation** of the desired music. For example: *“Upbeat electronic dance track, 128 BPM, with a strong bass drop and melodic synth lead. Happy and energetic vibe.”* This gives the model clear guidance: style (EDM), tempo (128 BPM), mood (happy energetic), key elements (bass drop, synth lead). The model will attempt to generate music matching that description. If you want a certain **instrument or sound**, mention it (e.g. electric guitar riff, piano solo, orchestral strings). Keep prompts relatively short; one or two sentences suffice. Overly detailed prompts (like specifying exact chord progressions) might be beyond its understanding, as it’s not explicitly programming notes but rather style. However, you *can* specify structure in a loose way, e.g. “starts with a soft intro, then builds into a full orchestra climax” – the model might capture that progression. If the model supports **lyrics**, you could include a lyric snippet. For instance, “Pop song with female vocals singing: ‘In the night we shine, our hearts align’ – style similar to Taylor Swift.” The output might then include a vocal-like melody (though generating intelligible lyrics is still extremely hard; likely it would be more of a vocalise). If you don’t want vocals, explicitly say *“instrumental only”*. Conversely, for choral or vocal humming, say *“choir humming an ambient tune”*. Also, indicate **era or reference artist** if you want a specific flavor: *“80s synthwave style”*, *“jazzy piano like Bill Evans”*. The model knows “high-quality” from its prompt training, so you don’t usually need to say “high quality” – instead focus on musical descriptors. Since it’s multi-modal (MiniMax uses text prompt; no melody input in this case), the prompt is your main tool. One more tip: **tempo (BPM)** can often be understood (as seen above), and **emotion words** (sad, joyful, epic) are important since music is all about mood.

**Advanced Tips:** If you have access to **reference audio (melody or chord progression)** conditioning, that can be a game-changer. Some music models allow you to upload a reference track or humming and then continue or style-transfer it. MiniMax Music might not have that in Fal’s interface (the prompt-based usage is primary), but if it ever offers a “reference audio URL” input, you could feed a short melody you like and prompt “continue this in a symphonic style” – currently though, likely only text prompt is supported. Another advanced approach is to use **prompt interpolation**: generate two different music pieces with different prompts, then use an audio interpolation tool (if available, or manually cross-fade) to merge them. This is outside the model usage but can create a longer piece with multiple sections (since each generation might be short). If you need the music to fit a specific length or timing (say for a video), you might have to generate slightly longer and then cut or loop sections. The model’s output will have a natural end (maybe a fade-out or a final chord). If you want a loopable piece, mention *“loopable”* or *“seamless loop”* in the prompt – it might try to structure it so that it can repeat smoothly. For including **lyrics**: if you truly need the model to attempt singing, provide a very short lyric line. Keep in mind, current tech usually ends up with gibberish vocals (the melody might be there but the “words” won’t be clear). It can still be musically pleasing though, like ethereal vocals. Also, consider chaining this with other models: e.g., generate an instrumental track with MiniMax Music, and if you need spoken or sung words on top, use a TTS model to overlay spoken word or rap (some creative uses have the AI rap with a monotone over a beat). On the technical side, music models like this sometimes offer *different sampling or creativity settings* (like temperature or variation). Fal might not expose those to the user, but if they do, a lower temperature would make the output more conservative/repetitive (maybe safer for background music), while a higher might introduce novel riffs but also could get chaotic. If you find outputs too repetitive, you might not have control directly – try adjusting prompt by adding an instruction like “with variation and a bridge section” to coax it into complexity.

**Cost & Performance:** MiniMax Music is **billed per output file**. According to some references, it might be around **$0.035 per generated music piece** (this was an estimate from an external source for similar model) – Fal likely translates that to per second or per track cost. It could be something like *$0.03 per 20s clip* or *$0.10 for up to 1 min* etc. Without official numbers, assume it’s on the order of a few cents for a typical clip (cheaper than video models, more akin to TTS pricing but a bit higher since music is complex). Performance: generating a 30-second audio might take perhaps 5–10 seconds on Fal’s backend, depending on model size. It’s much faster than video generation. Fal’s infrastructure likely streams the audio result as soon as it’s ready. The model might output at CD quality (44.1 kHz). The resulting file could be in WAV or a high-bitrate MP3. If the model struggles, sometimes you might get a shorter clip than requested or a somewhat abrupt end – to mitigate, ensure your prompt implies an ending (or explicitly ask for a “fade out ending”). There’s no mention of usage limits beyond normal rate limiting; you can probably generate many music clips for different needs. Quality wise, expect *“high-quality, diverse compositions”* as the model’s description states ([MiniMax (Hailuo AI) Music | Text to Audio | AI Playground - Fal.ai](https://fal.ai/models/fal-ai/minimax-music#:~:text=Generate%20music%20from%20text%20prompts,quality%2C%20diverse%20musical%20compositions)) – meaning it usually adheres to genre conventions and produces listenable results. It may not produce a radio-ready masterpiece every time (some outputs might feel stock or repetitive), but it’s quite impressive for quick background scores or idea generation. In summary, it’s a low-cost, fast tool for on-demand music with just a carefully written prompt.

### Stable Audio (Open) – Text-to-Audio (Music & Sound Generation)
**About & Key Features:** Stable Audio (Open) is Stability AI’s open-source text-to-audio model. It generates audio (music, sound effects, or ambience) from text descriptions, and is “open” in that it’s available for commercial and research use without many restrictions ([Stable Audio Open | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-audio#:~:text=fal)). It’s a fast timing-conditioned latent diffusion model for audio, meaning it is particularly good at aligning sounds to a specified timing or beat (one can even specify BPM as part of the prompt). Key features include: ability to handle **beats/tempo in prompt**, decent audio quality for music snippets, and a focus on being **resource-efficient** (runs relatively quick). It may not produce full-length songs like closed models but can output shorter clips (the official Stable Audio API allowed up to ~45 seconds for the free model and 90 seconds for the advanced). As an open model on Fal, it’s likely the version that can do around 20–30 seconds of audio per request. It’s well-suited for generating loops, instrument samples, or background ambiance.

**Prompting Best Practices:** Use **concise, descriptive prompts focusing on sound**. For music, similar approach as with MiniMax: specify genre, instruments, mood, and optionally structure. E.g., *“Lo-fi hip hop beat, 80 BPM, soft piano chords and vinyl crackle, relaxing vibe.”* Stable Audio was known to pay attention to tempo (BPM) and instruments – include those clearly. If generating sound effects or ambience, describe the environment and sounds: *“Rainforest ambiance with light rain and distant animal calls”*. You can also combine concepts: *“sci-fi atmosphere with humming spacecraft engine and beeping computers”*. The model will mix them. Because it’s diffusion-based, it can also handle more abstract prompts like *“a single prolonged mystical synth pad swelling and fading”*. But clarity helps. Including **duration cues** can be useful if supported (some versions let you specify target duration or say “10-second loop”). If not explicitly, the prompt could say “short” or “loopable” or “with a clear ending chord” depending on your need. **Musical prompts** might benefit from specifying *“instrumental”* if you don’t want any vocalization. Conversely, you might try *“choral humming”* if you want human voice sounds. In testing, stable audio responds well to genre labels (rock, classical, EDM, etc.) and mood adjectives. It’s also good to mention **audio qualities** like “high quality recording” or “studio mix” if you want clean output, or “low quality, vintage record” if you want a filtered effect (though it might add hiss/pops to simulate that). Essentially, treat the prompt like telling a sound designer what to create.

**Advanced Tips:** Stable Audio’s model has a concept of **timing conditioning** – if you have an exact length or rhythm, you might try to impose it via the prompt. For instance, “a 4-bar blues riff at 120 BPM” – the model might actually follow the bar structure and give you roughly 4 bars of music. If Fal’s interface allows a *duration parameter*, use it (the official Stability API does allow specifying duration in seconds and tempo). If not, you can often infer by model: stable audio open might default to around 10 seconds unless told otherwise. So if you want longer, see if repeating the prompt might coax it (not guaranteed). Another trick: because it’s open, you could generate multiple layers (e.g., get a drum loop and a melody separately by prompting for each, then mix them yourself). This gives more control than prompting the model to do everything at once. For example, prompt1: “funky drum beat, 100 BPM, no melody”, prompt2: “funky bassline at 100 BPM”, then combine externally. That is akin to multitrack recording with AI instruments. It requires some manual audio editing, but yields better customization. If you want **consistency for looping**, sometimes generating a slightly longer clip and then trimming can help, or instruct “loopable end-to-beginning”. Keep an ear out: occasionally, generative audio might have a slight hiss or artifact; a simple post-process filter or noise reduction can clean that. Also, note that stable audio open might not be as “creative” as closed models (it could lean towards safe, somewhat generic outputs). To push creativity, you might add unusual descriptors (“haunting”, “ethereal”, “glitchy”) to get more experimental results. Finally, if you find the output too short or abrupt, you can always run another generation and butt them together, maybe cross-fading if needed, to extend the track. Because it’s free-license, you can also edit it freely without issue.

**Cost & Performance:** Stable Audio Open is **very low-cost or possibly free-tier on Fal**. As an open model, Fal might simply charge by compute time if anything. It might effectively cost **fractions of a cent per second** of audio. Synexa’s comparison showed stable diffusion XL at 0.004 per image ([Synexa Pricing - Affordable AI Computing Solutions](https://synexa.ai/pricing#:~:text=Billed%20per%20image,002%20per%20image)); by analogy stable audio might be similarly low per output. Likely, Fal just charges by GPU-seconds for this one, which might be on the order of $0.0001 per second of audio. In other words, the cost is negligible (a 10-second clip might be <$0.001). Performance: it’s **fast** – generating 10 seconds of audio might only take 1–2 seconds on GPU. The model’s efficient. You’ll get the result quickly, and because it’s not returning huge data (audio file of a few hundred KB for 10s MP3), download is instant. Fal positions it as open, meaning you don’t have heavy usage restrictions. You could probably generate a lot of audio, but remember audio generation can produce **unexpected content** sometimes (like maybe some copyright melody or something, though stable audio is trained to avoid known songs). But since it’s open and presumably licensed data, those concerns are minimal. Quality is decent: not as polished as a human composition, but for backing tracks, ambient sounds, game jams, etc., it’s quite useful. Also stable audio is known to actually adhere to given BPM fairly well (if you say 120 BPM, it uses an underlying timing mechanism to try to match that tempo). This is an advanced feature that you can rely on for rhythm-critical tasks. 

## Voice & Speech Models

### PlayHT Text-to-Speech v3 – Voice Synthesis (Single-Speaker)
**About & Key Features:** PlayHT’s Text-to-Speech v3 is a **state-of-the-art TTS model** known for its natural and expressive voices. It offers **multilingual support** and improved emotional tone rendering ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=Blazing,volume%20processing%20and%20efficient%20workflows)). This model can generate voice audio from text with different preset voices (Fal’s UI shows a dropdown of voices, e.g. “Jennifer (US)” as default ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=Input))). It’s optimized for speed (labeled “blazing-fast”) and is ideal for high-volume use cases where you need lots of narration or dialogue generated quickly. In v3, PlayHT has likely introduced more neural voices and possibly some controllable attributes like pitch or speaking rate, although the primary interface is selecting a voice persona. The key features: *very natural prosody*, *emotional intonations* (the voices can laugh, cry, whisper in tone if the text implies emotion), and support for many languages/accents (English US, UK, Spanish, etc., per voice availability).

**Prompting Best Practices:** The “prompt” here is actually the text you want spoken (not a descriptive prompt, but the actual script). So to use it effectively: feed it **well-punctuated, clear text** as input. The model will read exactly what’s written, so ensure you include commas, periods, question marks etc., to guide the speech rhythm. For example, writing “Let’s eat, grandma!” vs “Lets eat grandma” will drastically change meaning and intonation – just like any TTS. To get an **emotion or tone**, you can add **emotive cues or punctuation**: e.g. “I can’t believe it…” (with ellipsis) might be spoken with a tone of disbelief. Or “No! Absolutely not!” will come out forceful due to exclamation. Use exclamation points, question marks, or even typed out sounds (like “*[sigh]*”) to convey pauses or emotions. Some TTS systems even support **SSML (Speech Synthesis Markup Language)** – I’m not sure if Fal exposes that, but if so you could fine-tune prosody with `<say-as>`, `<break time="500ms"/>`, etc. In absence of SSML, a trick is to break input into multiple sentences or use commas to induce pauses. For example, to make the voice speak slower or with a thoughtful pause: “I… I’m not sure, that’s a good idea.” The ellipsis and comma will naturally slow it down. Also, choose the appropriate **voice persona** for your use case – Fal likely offers multiple. Jennifer (US) might be a generic female narrator. If you need a male voice, pick one of the male names. If you need a British accent or Spanish, select those if available. The content of text should match the language of the voice (don’t feed Spanish text to an English voice, though multilingual support means some voices might speak multiple languages, but best to use a native voice for each language). **Keep the text length reasonable** – very long paragraphs might be better broken into smaller chunks for generation to avoid any timeout and to allow you to adjust between sentences if needed. However, PlayHT v3 is designed for even long-form (like audiobooks) streaming, so it can handle long input, but Fal might have request limits. 

**Advanced Tips:** Use the **additional settings** if Fal’s API provides any (like speaking rate, pitch). If not directly in UI, sometimes you can insert SSML tags in the text (e.g., `<prosody rate="slow">Hello</prosody>`). Check Fal docs if they mention SSML for PlayHT; since it’s a partner model, they might allow raw SSML. With SSML, you can fine-tune pronunciation of certain words via the `<phoneme>` or `<sub alias=""/>` tags (for example, if a name is pronounced incorrectly, you can spell it out phonetically or use IPA). Also, this model being advanced likely handles **numbers and abbreviations** smartly (e.g. “Dr.” as “Doctor”), but if not, you may need to write them out (write “Doctor” instead of “Dr.” in the input to ensure correct expansion). For **emphasis**, you can use ALL CAPS for a word which often makes TTS stress it (e.g. “That was AMAZING!” will emphasize AMAZING). But don’t overuse all caps, it might sound unnatural if everything is emphasized. Another advanced trick: if you want a certain **style that’s not directly emotional** (like a flat robotic tone for effect), you may deliberately remove punctuation or write in a monotonic style – but given these voices are tuned for natural speech, getting a flat tone might be hard unless the voice itself is robotic. Instead, you could use a different model for that style (like “ElevenLabs monotone” etc., but that’s outside this list). For multilingual usage, ensure you provide text in the correct language if the voice supports it. PlayHT v3 does multilingual well, so you can even mix languages in one input and if the voice can handle both (some can code-switch), it might speak both appropriately. Test short sentences first when mixing languages to verify it doesn’t butcher one of them. If you have **large volume** to generate (like thousands of sentences), consider using Fal’s batch or streaming API to avoid re-loading the model each time; set up a pipeline feeding text and receiving audio. The model is fast, but there’s overhead per request. If allowed, maintain a single connection for multiple TTS conversions. Finally, if output doesn’t sound right (like mispronunciations or weird emphasis), experiment by slightly rephrasing your text. Sometimes adding a comma or changing a word spelling can trigger a better reading. It’s a bit of an art to coerce TTS to exactly the tone you want, but PlayHT v3 is one of the more expressive ones, so you’ll likely get a pleasing result by just writing naturally and including punctuation.

**Cost & Performance:** PlayHT TTS v3 is priced at **$0.03 per generated audio minute** ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=)). That’s 3 cents per minute of speech, or about **$0.0005 per second**. In other terms, you can get ~33 minutes of audio for $1 ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=)). This is very cost-effective, suitable for long narration (an hour of audio would be about $1.80). The performance is realtime or better – *blazing fast* as they say. Typically, such models can generate at many times faster than real time. So a 1 minute clip might generate in a few seconds. Fal might even stream the audio so playback can start near immediately if integrated that way. There’s likely no significant queue unless your usage is huge, since TTS is relatively lightweight. Usage limits: TTS might have a max text length per call (maybe a few thousand characters), but you can split text easily. Also, be mindful of not spamming too fast if you do massive volume – Fal might throttle to protect backend. But practically, generating even hundreds of pages of audiobook is feasible with this cost and speed. Since it’s a **Partner model**, it’s production ready – voices are thoroughly quality-checked for minimal errors. You might occasionally hear a mispronounced name or acronym, but overall clarity is excellent. If you need even higher fidelity or different voices, Fal also lists ElevenLabs (Turbo v2.5) as another option ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=fal,12)), but PlayHT v3 is already top-tier and more cost-effective (ElevenLabs typically is pricier per character). For multilingual, the cost stays the same per minute regardless of language. 

### PlayHT Text-to-Speech *Dialog* (PlayAI Dialog) – Multi-Speaker Dialogue TTS 
**About & Key Features:** PlayHT’s Text-to-Speech Dialog model (branded as PlayAI Dialog on Fal) is a system for generating **multi-speaker conversational audio** from a dialogue script ([PlayAI Text-to-Speech Dialog - Fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=PlayAI%20Text,games%2C%20animations%2C%20and%20interactive%20media)). Instead of having to synthesize each speaker line with separate calls, this model takes a formatted dialogue as input (with speaker labels and their lines) and produces a continuous audio of the conversation. It’s tailored for expressive, **natural-sounding multi-speaker interactions** ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=Generate%20natural,games%2C%20animations%2C%20and%20interactive%20media)) – ideal for storytelling, games, or animated dialogue. It likely uses distinct voice models for each speaker label and handles the timing to sound like a real back-and-forth conversation. It supports expressive output, meaning each speaker’s line will carry emotion and intonation appropriately. It simplifies the process of dialogue generation by letting you input the conversation in text and automatically picking suitable voices (maybe default male for Speaker 1, female for Speaker 2, etc., which you may map or it might randomize unless specified).

**Prompting Best Practices:** The input format is typically **“Speaker name: dialogue”** on each line ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=Input)). As shown in Fal’s example, you list each line prefixed by a speaker ID (could be generic like Speaker 1, Speaker 2 or character names) and a colon, then the spoken text. For example:  
```
Speaker 1: Hey, did you catch the game last night?  
Speaker 2: Of course! What a match—it had me on the edge of my seat.  
Speaker 1: Same here! That last-minute goal was unreal...  
Speaker 2: Gotta be the goalie. Those saves were unbelievable!  
```  
 ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=Input)). This format allows the model to know when one voice should stop and the other start. **Use distinct labels** for each speaker consistently. Likely, the first occurrence of a speaker label cues the model to assign a voice to it (possibly alternating genders or using a preset pairing like Speaker 1 male, Speaker 2 female by default). If you want specific voices, see if Fal documentation allows specifying a voice per speaker (maybe via some meta-instruction or by using actual names of known voices as speaker labels). If not, you get default distinct voices. **Write natural conversational text** – include interjections, pauses (you can use punctuation like ellipses or dashes to indicate pauses or cut-offs, which the model will render in speech), laughter (e.g. “haha” will be spoken laughingly if the model is good), and questions/exclamations with proper punctuation so the voice rises or falls. Keep each speaker’s line relatively short (a sentence or two) as in real conversation; long monologues can be done but note the model might still breathe/pauses naturally. Possibly insert **[sounds or actions]** in brackets if you want a pause or effect (though it might just skip reading them; it might not add sound effects, since it’s TTS not an SFX model). For example, “Speaker 1: *[laughs]* That was great.” might cause a brief pause or even a synthesized laugh if the model knows to interpret “[laughs]”. This might not happen unless specifically programmed, so you might instead just write “(laughs)” or literally “hahaha” for laughter. **Maintain conversational tone** – even in writing, use contractions (“I’m, you’re, let’s”) and brief responses (“Yeah.”, “Uh, maybe.”) to get the most natural cadence. The model tries to output as if two people talking, which means it might automatically add subtle *turn-taking cues* (like slight pauses between turns). 

**Advanced Tips:** If you want to influence how a particular speaker sounds, you might try giving an explicit cue in their first line or as a separate instruction. Possibly something like:  
```
Speaker 1 (calm, deep voice): I think we should head out now.  
Speaker 2 (cheerful, high-pitched): Sure, I'm ready!  
```  
Check if the model reads out the parenthetical or actually uses it as an instruction. Ideally, it would use it as instruction and not voice it. The safer approach if that’s not supported is to imply tone through word choice and punctuation (e.g. lots of exclamation and upbeat words for cheerful, short monotone replies for calm). Another advanced use: **narration in dialogues** – maybe you want a narrator voice too. You could include a third speaker “Narrator:” for descriptive lines. The model may then assign a third voice. Or you could incorporate narration in a speaker line like:
```
Narrator: [The two friends walk down the road.]
Speaker 1: It’s been a long day, hasn’t it?
``` 
If the model is well-designed, it might skip or read narration in a different style (or maybe you want it read). This is something to experiment with. Regarding voices, if the default choices aren’t what you want, see if Fal allows specifying speakers like “John:” and “Emily:” where John and Emily correspond to particular voice presets known to the system. If not documented, you might try using actual names of PlayHT voices. For example, PlayHT had voices named like “Will (US)” etc. Perhaps if you use “Will:” as a speaker, it might choose that voice. This is speculative, but some multi-speaker TTS allow naming the voice by the speaker label. The Fal example just used generic Speaker 1/2, which suggests it auto-assigns voices. If you need a specific pairing (male vs female, etc.), you might need to generate lines separately with single-speaker TTS and mix yourself. But the whole point of this model is convenience, so likely it picks two contrasting voices by default. 

One more advanced thing: because it’s conversation, pay attention to **overlap/interruptions**. Real dialogues sometimes have interjections before the other finishes. The model probably doesn’t do overlapping speech (it will produce sequential audio). To simulate an interruption, you can use a dash or ellipsis at the end of one’s line and immediately start the next line, like:
```
Speaker 1: I just wanted to say that I—
Speaker 2: –I know what you’re going to say.
``` 
This should make Speaker 1 cut off and Speaker 2 jump in. The model should respect the punctuation and make Speaker 1’s voice cut off in tone. 

**Cost & Performance:** The dialog TTS is priced at **$0.05 per minute of audio** ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=)), slightly higher than single-speaker (because it’s doing more complex processing). That’s 5 cents per minute, so still just **$0.000833 per second** of generated audio – very affordable. For $1 you get ~20 minutes of dialogue ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=)). The generation time is roughly similar to single-speaker TTS, maybe a tad more to stitch voices, but essentially real-time. For example, a 1-minute dialogue might generate in a second or two. The model handles splitting and voices behind the scenes, so you don’t have to make multiple calls. As input, you can likely feed quite a long dialogue (maybe a multi-minute conversation script). But if extremely long, consider chunking by scene to keep context clear and to avoid any potential buffer limits. The output audio will have the voices alternating with appropriate timing (the model adds slight pause between lines). It’s ready to use – you won’t need to manually splice anything. Quality: each speaker voice is nearly as good as the single-speaker PlayHT voices, with the added benefit of the model possibly capturing conversational flow, like slight differences in how one would speak in a dialogue vs narration (maybe quicker back-and-forth). It’s also context-aware: if one speaker asks a question, the next answer might have the right intonation of responding, which separate TTS calls might not achieve easily. As for usage, the same general rules as TTS apply: avoid extremely jargon-y or rare words (or provide phonetic hints), watch out for voice mismatches (if two speakers speak different languages in the same input, not sure if model can handle that – likely not well, better to stick to one language per conversation model call). With 2+ voices, ensure no speaker’s segment is super long monologue because the model might have memory constraints on that speaker’s prosody. But moderate lengths are fine. In summary, the cost is trivial for what you get – an entire dialogue voiced by AI. This opens up quick creation of radio-play style content or game NPC dialogues without hiring two voice actors, which is exactly its best use case.

### F5 TTS – Voice Cloning (Zero-Shot TTS)
**About & Key Features:** F5 TTS is a recent open-source text-to-speech model notable for its **zero-shot voice cloning** capability ([Play 3.0 mini – A lightweight, reliable, cost-efficient Multilingual TTS ...](https://news.ycombinator.com/item?id=41840872#:~:text=,under%2010G%20vram%20nvidia%20gpu)) ([F5 TTS | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/f5-tts#:~:text=Text%20to%20be%20converted%20to,speech)). It can take a short sample of a person’s voice (reference audio + the transcript of that audio) and then generate new speech in that voice saying any text. It’s a diffusion-transformer hybrid model (Flow Matching + Diffusion Transformer) that achieves highly fluent and natural speech. Key features: supports cloning with very little data (just a few seconds of reference audio), fast non-autoregressive generation, and it’s multilingual to an extent (though primarily English-focused). It’s state-of-the-art in voice cloning as of its release (335M parameters but very capable ([New State-of-the-Art TTS Model Released: F5-TTS : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/1g2giso/new_stateoftheart_tts_model_released_f5tts/#:~:text=New%20State,is%20designed%20for%20English))). On Fal, you provide a **Reference Audio URL and corresponding reference text** for that audio, plus the text you want generated ([F5 TTS | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/f5-tts#:~:text=Text%20to%20be%20converted%20to,speech)). The model then speaks the new text in the reference voice. This is ideal for creating custom voices – e.g. clone your own voice or a character’s voice, to produce any narration.

**Prompting Best Practices:** The main “prompt” is still the text to speak (like any TTS). But the crucial part is providing a **good reference audio + accurate reference transcript**. For best results: use a clear recording of the target voice, ideally a clean sample with no background noise and representative speaking style. You don’t need a long sample; something like 5–15 seconds of speech is usually enough (the paper mentions ~3 seconds can work, but a bit more helps quality). Make sure the **reference text exactly matches** what’s said in the reference audio, as the model uses that to understand the voice’s characteristics (tone, accent) aligned with phonetics. If they mismatch, the cloning might falter. On Fal’s interface, you’ll likely paste a URL to an audio file (maybe .wav) and then type the transcript in a provided field ([F5 TTS | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/f5-tts#:~:text=Text%20to%20be%20converted%20to,speech)). Do that carefully. Then for the new text input (the content you want spoken in that voice), treat it like normal TTS text – include punctuation, etc., to shape prosody. The model will try to keep the same speaking style as in the sample. If your reference speaker speaks very formally in the sample, the output will likely also sound formal. If you want a different tone from the same voice (say reference is calm but you want angry), the model might not perfectly extrapolate emotions not present in the sample. You may need to provide a reference sample that matches the style you want. Or try to coerce via text (like adding “!” etc., but voice timbre might remain calm if sample was calm). So choose the sample that has a similar energy/intonation to what you’ll generate. Also, note the **Model Type selection** – Fal shows “F5-TTS” as a dropdown (maybe if future versions come, you might choose them) ([F5 TTS | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/f5-tts#:~:text=Model%20Type)). Just leave it to F5-TTS unless you know of a variant. 

**Advanced Tips:** **Voice adaptation** – since it’s zero-shot, it can clone on the fly without fine-tuning, but sometimes providing a slightly longer sample (like multiple sentences) yields more robust cloning (the model can average out the voice traits). If possible, give ~30 seconds of audio rather than 5s. Also, ensure the reference audio has the **same audio quality** you want; if the sample is telephone-quality, the output might carry that muffled quality too. Use high-quality reference audio for high-quality output. If the output voice has minor artifacts or doesn’t perfectly match, an advanced trick: you can run the output through a voice conversion model or enhancer, but likely unnecessary. F5 is quite good natively. It might struggle with **singing** – if you input singing audio and then give it text, it’s not going to sing (it’s a TTS, not trained for singing intonation). It will speak in that person’s voice normally. For multilingual: if you give it an English sample, it captures an English accent of that voice. If you then feed non-English text, it may or may not do well (the model was primarily trained on English but possibly some multilingual ability via training data). It might speak other languages with an English accent of that voice. For cross-gender or totally different content, it usually retains voice identity but might not nail emotion that wasn’t in sample. Another tip: if generating a long piece in the cloned voice, consider breaking the text into paragraphs or sentences and generate each separately, possibly with slight variation in punctuation to keep it expressive. Because cloned voices can sometimes become a tad monotone if you generate a lot in one go (not always, but it’s a general TTS thing). By chunking, you allow resetting the model per sentence which can actually keep it crisp. If continuity of intonation is needed across sentences, then do it all at once. Also, be mindful of **embedding content** – if the reference voice is someone famous or proprietary, ensure you have rights if it’s for public/commercial use. F5 is open, but outputs could still raise issues if misused (Fal likely expects ethical use; they might even block obvious public figure cloning via some policy). Technically, to maximize similarity, include tricky phonemes in the sample if your target text will have them. For instance, if output text has a lot of “th” sounds but sample had none, the model might be a bit less tuned on how that voice says “th”. Usually not a big problem though – the model presumably builds a voice profile across all phonemes from the transcript. 

**Cost & Performance:** As an advanced model, F5-TTS is surprisingly efficient. The pricing on Fal isn’t explicitly given in the snippet, but likely it’s charged per second of output (like TTS) plus maybe overhead for using the reference. Given other TTS rates, it could be around **$0.05–$0.06 per minute** as well. Possibly Fal hasn’t separately priced it yet, but presumably similar ballpark because it’s open-source (the expensive part is that it uses diffusion internally, but it’s optimized for quick inference). It might be a bit slower than PlayHT’s model because cloning and diffusion add overhead, but still likely real-time or just above. You might see perhaps 2-3 seconds to generate 5 seconds of audio on moderate GPU. Still, very usable. From user reports, F5 can generate ~1000 words per minute of audio on decent hardware ([New State-of-the-Art TTS Model Released: F5-TTS : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/1g2giso/new_stateoftheart_tts_model_released_f5tts/#:~:text=r%2FStableDiffusion%20www,58%20minutes%20total)), which is basically real-time. As for performance: expect a small delay for the model to process the reference audio the first time (but it’s negligible with a short sample). Possibly Fal might cache embeddings if you use the same reference repeatedly in one session. There’s the step of uploading the file – Fal’s interface likely handles that (you either host the file yourself or upload via Fal UI). Once set, generation is one step. The voice similarity can be quite high – many find it quite close to ground truth voice in tests. There might be very slight differences or a tiny robotic undertone if compared side by side, but for most applications it’s convincingly the same speaker. That said, extremely unique voice qualities (like a raspy voice or heavy accents) might be somewhat averaged out. It’s better at cloning typical voices. Regarding usage limits: since it’s open, Fal probably just gates it behind the usual API key and costs. They likely allow use as long as it’s not disallowed content (e.g. using someone’s voice without consent might violate policies). Also note, **reference text must be accurate** – it’s required in Fal’s UI as a starred field ([F5 TTS | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/f5-tts#:~:text=Text%20to%20be%20converted%20to,speech)). If you don’t have an exact transcript, transcribe it first (even use an ASR model to get it, but correct any mistakes). The model’s success depends on that alignment. All in all, F5 TTS on Fal lets you do what was science fiction a few years ago – clone a voice in seconds – at minimal cost and fairly fast speeds, making it extremely powerful for custom voiceover needs.

---

**Comparison Table of Key Insights:**

| **Model** | **Category** | **Key Features** | **Best Use Cases** | **Cost & Performance (Fal API)** | **Prompting Techniques & Tips** |
|-----------|-------------|------------------|-------------------|----------------------------------|--------------------------------|
| **FLUX.1 [dev]** (fal-ai/flux/dev) | Image (Text→Image) | 12B flow-transformer model; high-quality outputs; general-purpose styles ([FLUX.1 [dev] | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/flux/dev#:~:text=fal)). | Detailed artistic images; versatile content generation for concept art, design. | ~$0.025 per 1MP image ([ZimmWriter Image API Integration | www.rankingtactics.com](https://www.rankingtactics.com/zimmwriter-image-api-integration/#:~:text=www,The%20main)). Moderate speed (needs ~20–40 steps for best quality). | Use **long, descriptive prompts** with subject, style, context (handles complex prompts well). Can include **weighted segments** (`::`) to emphasize parts ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Image%3A%20portrait%20,s%201000)). No explicit negative prompt field, so state undesired elements in prompt (“no text,” etc.). Leverage **Flux LoRAs** or image-to-image for fine control. |
| **FLUX.1 [schnell]** (fal-ai/flux/schnell) | Image (Text→Image) | Optimized “fast” FLUX; 1–4 diffusion steps for image ([Fastest FLUX Endpoint | fal.ai Docs](https://docs.fal.ai/fast-flux/#:~:text=fal,flux)). Very fast inference with quality near FLUX dev ([README.md · frankjoshua/FLUX.1-schnell at aa3b18d89cc7c592cc8291df10c9b5045cf3a5db](https://huggingface.co/frankjoshua/FLUX.1-schnell/blame/aa3b18d89cc7c592cc8291df10c9b5045cf3a5db/README.md#:~:text=1.%20Cutting,model%20can%20be%20used%20for)). | High-throughput image gen; quick previews or batch generation where speed matters. | ~$0.003 per image ([ZimmWriter Image API Integration | www.rankingtactics.com](https://www.rankingtactics.com/zimmwriter-image-api-integration/#:~:text=www,The%20main)) (very low cost). Blazing fast (few seconds per image). | **Concise or detailed prompts** both work. Supports **same syntax** as FLUX dev (use `|` and `::` weights) ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Image%3A%20portrait%20,s%201000)). Great for **rapid iteration** – try multiple prompt tweaks quickly. If quality needs a boost, increase steps slightly (up to 4). Use outputs as drafts to later upscale or refine with slower models. |
| **FLUX.1 [pro] v1.1 Ultra** (fal-ai/flux-pro/v1.1-ultra) | Image (Text→Image) | Latest pro-grade FLUX; up to 2K resolution, improved realism ([FLUX.1 [dev] | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/flux/dev#:~:text=improves%20image%20quality%2C%20typography%2C%20prompt,res%20realism%20%2013)). Photorealistic tendencies, high detail. | Photorealistic images, high-res needs (marketing material, prints). When quality trumps speed. | ~$0.05 per 1MP image ([Flux 1.1 Pro Ultra Mode Is Here | undefined - Flux Labs AI](https://www.fluxlabs.ai/blog/flux-11-pro-ultra-mode-is-here#:~:text=Flux%201%20Pro%3A%20fixed%20price,or%20Freepik%20if%20you)). Slower inference due to high res and detail (larger model). | **Photographic prompts** shine – include camera models, lighting, and realistic detail for lifelike results ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Image%3A%20Close,and%20altruism%20through%20scene%20details)) ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Background%20hints%20at%20charitable%20setting,and%20altruism%20through%20scene%20details)). Use for **final renders** (after prototyping with faster models). Add exact styles or artist names for specific looks. Ensure punctuation and grammar in prompt for clarity (model is sensitive to prompt clarity). |
| **Stable Diffusion 3.5 Large** (fal-ai/stable-diffusion-v35-large) | Image (Text→Image) | Stability AI’s MMDiT model; excels at complex prompts, typography, and coherent detail ([Generating Images from Text | fal.ai Docs](https://docs.fal.ai/guides/generating-images-from-text/#:~:text=%2A%20fal,efficiency)). Multimodal inputs (ControlNet, etc.). | General-purpose image gen; complex scenes, on-brand visuals (when using ControlNet/LoRAs). Great for when Stable Diffusion-style output or community familiarity is needed. | ~$0.06–$0.07 per image (approx. 15 images/$1) – moderate ([Stable Diffusion 3.5 Large | Text to Image | AI Playground - Fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Stable%20Diffusion%203,Related)). Good speed (slower than SD1.5 but optimized for faster inference than prior XL models). | **Paragraph-length prompts** are workable – describe scenes thoroughly ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Prompt)) ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=creating%20an%20otherworldly%20atmosphere,masterful%20photography%2C%20hyperdetailed%2C%20magical%20realism)). Use **ControlNet** for pose/structure control (Fal UI supports depth, canny, etc.). **Negative prompts** supported – use them to rule out unwanted traits. Can attach **LoRA** models for style/character consistency ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Loras)). Very flexible – fine-tune prompt for best results (it can handle a lot of detail). |
| **Minimax Video-01-Live** (fal-ai/minimax/video-01-live) | Video (Text→Video and Image→Video) | High-motion animation from text or single image; smooth camera moves; consistent subject rendering ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=static%20images%20to%20life%20and,of%20fal%E2%80%99s%20inference%20engine%2C%20this)). Short clips (~3–5s). | Animating static images (live portraits, artwork to video), action scenes with one main subject. Great for adding motion to illustrations or creating quick video content from an image. | ~$0.30–$0.40 per video (estimated similar to Hunyuan) ([Pricing | fal.ai](https://fal.ai/pricing#:~:text=Pricing%20,megapixel%2C%20Billed%20by%20the)). ~4s clip generates in ~30–60s (fast for video). | **Use image input for best results** (provide an image to animate along with prompt). **Describe motion and camera** clearly – e.g. “camera pans around X” for dynamic shots. Keep prompts focused on one scene/event. Good with **verbs** describing actions. Leverage it to **bring stills to life** – generate an image with Flux/SD, then animate with Minimax. |
| **Hunyuan Video** (fal-ai/hunyuan-video) | Video (Text→Video) | Tencent’s 13B model; cinematic quality, real-world physics, smooth continuous actions ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=animations%2C%20excelling%20particularly%20in%20high,HD%20output%20with%20enhanced%20physics)). ~4s clips with complex motion. | High-fidelity short videos (like mini movie scenes, product visuals). Ideal when realism and fluid motion are crucial – e.g. showcasing a concept with a sweeping camera shot. | **$0.40 per video** (flat) ([Pricing | fal.ai](https://fal.ai/pricing#:~:text=Pricing%20,megapixel%2C%20Billed%20by%20the)). Generates ~4s in <1 minute (optimized on Fal) ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=and%20realistic%20content,the%20fact%20that%20AI%20video)). | **Write prompts as mini movie scenes.** Include environment, actions, and camera cues (e.g. “tracking shot,” “slow motion”). Use **cinematic language** – the model excels with that. Ensure prompt elements can logically happen in ~4s. Let the model handle physics – just describe outcome (e.g. “glass falls and shatters” will be handled realistically). |
| **Kling 1.5 Pro** (fal-ai/kling-video/v1.5/pro) | Video (Image→Video, Text→Video) | Professional video gen at 1080p; enhanced physics and detail ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=model%20achieves%20record,also%20exponentially%20growing%20each%20month)). Slightly older than v1.6, but high quality and available for image or text input. | High-res video needs – ads, prototypes requiring HD. Good for **physically complex scenes** (explosions, water, etc.) where simulation of effects is needed. | **~$0.10 per second** of video ([Pricing | fal.ai](https://fal.ai/pricing#:~:text=Pricing%20,megapixel%2C%20Billed%20by%20the)) (≈$0.5 for 5s clip). Slower: expect a couple minutes for a clip (HD output). | **Prefer image-to-video** for consistency: supply a base image of scene/character. **Detail the scenario** including physical events (model will simulate accurately). Use **short lines** if doing dialogues – but Kling is more visual; focus on visual events. Prompt for **1080p** detail if needed (though output is inherently HD). |
| **Kling 1.0 Standard** (fal-ai/kling-video/v1/standard/text-to-video) | Video (Image→Video, Text→Video) | Standard-quality Kling model; lower resolution, faster, simpler physics. Good for basic animations. | Quick, lower-cost video tasks – concept testing, simple cartoon-style animations, or where 1080p isn’t required. | ~$0.05 per second (estimated). Very fast generation (~half the time of Kling Pro). | **Keep prompts simple and singular.** Best for one action or scene – doesn’t handle multiple complicated events as well as newer models. Use for **storyboards**: e.g. generate rough animation of a scene to visualize it, then refine elsewhere. For styles, explicitly say “cartoon” or “sketch” if desired – it can adapt style somewhat. Add **pauses/ellipses** in text to simulate timing if multiple things happen (though overlap isn’t explicit). |
| **Luma Dream Machine 1.5** (fal-ai/luma-dream-machine) | Video (Text→Video, Image→Video) | High-quality imaginative video gen; consistent multi-subject interactions, realistic physics + cinematic camera ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=Luma%20Dream%20Machine%20is%20a,more%20ideas%20and%20dream%20bigger)) ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=Dream%20Machine%E2%80%99s%20advanced%20capabilities%20ensure,some%20limitations%2C%20the%20team%20is)). Balanced for creative scenes. | Creative storytelling videos, fantasy or sci-fi scenes with rich detail. Also good for multi-character interactions in one scene (which some models struggle with). | **$0.50 per video** (flat) ([Luma Dream Machine | Text to Video | AI Playground - Fal.ai](https://fal.ai/models/fal-ai/luma-dream-machine#:~:text=Generate%20video%20clips%20from%20your,can%20run%20this%20model)). ~20–30s generation for ~4–5s clip (efficient engine). | **Write “dreamlike” or creative prompts** freely – model is built for imagination. Still, include concrete elements so it has something to render (structures, characters). Supports both **text-only and image+text**; use image input if you have a specific look to maintain. Great with **ambient and cinematic prompts** (mention camera movement, environment mood). Can handle a bit longer clips (some users get ~8s); for longer, you can chain multiple outputs. |
| **MMAudio V2** (fal-ai/mmaudio-v2) | Audio (Video→Audio, Text→Audio) | Audio generation for video: adds synchronized sound (music/SFX) to silent videos ([MMAudio V2 | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/mmaudio-v2#:~:text=fal)). Takes video + optional text description of desired audio, outputs video with audio track. | Automatically sound-designing AI-generated videos (e.g. add atmosphere to Minimax or Hunyuan outputs). Quickly getting background music or effects for a clip without manual editing. | Pricing per second of audio (likely on par with TTS, e.g. a few cents/minute). Real-time or faster performance (generates audio quickly relative to video length). | **Always provide a text prompt describing the audio** along with the video. E.g. “crowd noise and traffic sounds” for a city scene, or “tense orchestral music” for a dramatic scene. Be specific about what kind of audio and mood. It will align sounds to visible actions (e.g. add footstep sounds when it sees walking). If you only want music, mention no SFX (and vice versa). After generation, you can adjust the volume levels externally if needed (the mix is generally good, but you might lower music under dialogue, etc.). |
| **Sync.So Lipsync 1.8** (fal-ai/sync-lipsync) | Video (Audio→Video) | Lip-sync animation model: given a face video/image and an audio clip, produces a video of the face speaking the audio ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=Video%20Url)) ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=Audio%20Url)). High-quality synchronization of mouth movements to speech. | Creating talking avatar videos (dubbing characters, animating portraits). Re-dubbing videos in different languages (just swap audio and lipsync). Virtual presenters for text-to-video solutions (combined with TTS to provide the voice). | ~$0.05 per minute of output (speech) ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=)). Very fast (runs faster than real-time; 10s clip generates in a couple seconds). | **Input = video (or still image) + audio**. Use a clear front-facing image/video of the person. Provide an audio with clean speech. Ensure the video/image and audio durations align (the output length matches audio length). The model auto-syncs; you don’t need to prompt anything if inputs are set. For best results, the face in the image should have a neutral mouth (closed or slight smile) initially so it can animate fully. Also, match the audio language to the person – lipsync on mismatched language might look a bit off if the mouth shapes don’t align perfectly (though it generally just follows phonetics). |
| **Veo 2** (fal-ai/veo2) | Video (Text→Video) | Google’s latest SOTA video model; unparalleled physics & cinematography ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=,Significantly%20fewer%20unwanted%20artifacts%20or)), supports up to 4K resolution and extended durations ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=understands%20the%20language%20of%20filmmaking,Significantly%20fewer%20unwanted%20artifacts%20or)). Minimal artifacts (near production-ready output). | High-end video generation for professional content. Anywhere you need a fully realized video scene with complex interactions – e.g. film pre-visualization, ad creatives, VFX ideation. It can handle multi-second continuous shots with fine detail. | **$2.50 for 5s, +$0.50 each additional second** ([Veo 2 | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/veo2#:~:text=For%205s%20video%20your%20request,50)) (~$0.50/sec). Slowest model: heavy compute – ~a few minutes for a 5s HD clip (Fal uses multi-GPU/TPU to accelerate but it’s still resource-intensive). | **Write film scene-style prompts**. Include camera directions, detailed actions, and environment specifics ([Veo 2 | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/veo2#:~:text=The%20camera%20floats%20gently%20through,film%2C%20the%20golden%20light%20creates)) ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=,Significantly%20fewer%20unwanted%20artifacts%20or)). Veo2 can follow sequential descriptions, so you can outline a short story in one prompt (it will try to realize it in the given time). Use **specific lens/film terms** for style (e.g. “35mm film, shallow depth of field” for a cinematic look ([Veo 2 | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/veo2#:~:text=petals%20glowing%20in%20the%20warm,and%20weathered%20wood%20of%20the))). Because it can do long videos, you might explicitly state “~10 second scene” in prompt if you have that length (and of course adjust the Fal duration parameter accordingly). The cost is high, so **test at lower resolution or shorter length first** to tweak prompt, then scale up to 4K or full length for final. |
| **MiniMax Music** (fal-ai/minimax-music) | Audio (Text→Music) | AI music generator (by Hailuo); creates short musical compositions from prompts ([MiniMax (Hailuo AI) Music | Text to Audio | AI Playground - Fal.ai](https://fal.ai/models/fal-ai/minimax-music#:~:text=Generate%20music%20from%20text%20prompts,quality%2C%20diverse%20musical%20compositions)). Can produce instrumental or vocal-like sounds, multi-genre, with coherent structure (intro, beat, etc.). | Background music for videos or games, inspiration for composers, quick music samples without licensing issues. Great for when you need a quick soundtrack of a specific style/mood. | ~$0.03–$0.05 per generated track (estimated; very low cost). Generates a 20s–30s clip in a few seconds (fast). | **Prompt = description of music** (not natural language scene, but musical properties). State **genre, instruments, mood, tempo (BPM)**. E.g. “Calm acoustic guitar melody, 70 BPM, relaxing folk style.” The model respects BPM and instrument cues. Use **mood adjectives** (“uplifting, somber, energetic”) to shape tone. If you want certain sections (intro, buildup), you can mention them, but remember outputs are short (the model might pack a mini-structure in that time). For any vocals, you can try adding “with male vocals humming” – it may add vocal sounds (though lyrics are not precise). Keep the prompt concise – think of it like describing a music piece to a composer. |
| **Stable Audio (Open)** (fal-ai/stable-audio) | Audio (Text→Audio) | Stability’s open-source text-to-audio model. Can generate music or sound effects from text. Has timing conditioning (understands tempo and length cues) ([DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music ...](https://arxiv.org/html/2405.20289v1#:~:text=DITTO,conditioned%20latent%20audio)). Good at loops and shorter audio (e.g. 10–30s). | Royalty-free sound effect generation (environments, noises) and music loops for projects. Ideal for quick audio backgrounds – e.g. ambient sounds for a game level, or a drum loop for a song. | *Very* low cost (on par with compute pricing; likely <$0.01 for 10s clip). Near real-time generation. | **Prompt similar to MiniMax’s approach**: describe the audio content. For music: include **genre/instruments and BPM** (it was designed to honor tempo – e.g. “128 BPM techno beat with synth chords”). For ambiance: describe environment and elements (“busy café ambiance: chatter, clinking cups”). You can also request loops (“seamless loop of ocean waves”). Keep instructions clear and succinct – stable audio isn’t as good with long narrative prompts, it wants key audio descriptors. If target duration is important, specify (some stable audio interfaces allow “duration: X seconds” in prompt or separate parameter – check Fal docs). Otherwise it defaults to around 10 seconds for open model. |
| **PlayHT TTS v3** (fal-ai/playht/tts/v3) | Voice (Text→Speech) | Single-speaker neural TTS with **emotive, multilingual voices** ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=Blazing,volume%20processing%20and%20efficient%20workflows)). Very natural prosody; many voice options (male/female, various accents). Optimized for speed and quality. | Voiceover for articles, video narration, IVR systems, or audiobook creation. Any use case needing a human-like voice reading provided text. | **$0.03 per audio minute** ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=)) (~33 minutes per $1). Real-time or faster generation. | **Input actual script text** (not a description). Use punctuation to guide tone: “...” for pauses, “!” for excitement, “?” to prompt raising tone. **Select voice persona** if possible (Fal UI voice dropdown) to match the context (e.g. Joanna for friendly narration, a different voice for a serious tone). If needed, add minor cues in text like *“(whispering)”* or use an exclamation to convey shouting – the model will reflect it to some extent. For languages, just input the text in that language and pick a voice that supports it. Use SSML tags if Fal supports (to fine-tune speed, emphasis or pronunciation for acronyms, etc.). Keep sentences reasonably sized – the model handles long texts, but you might break on paragraph boundaries for control. |
| **PlayHT TTS Dialog** (fal-ai/playai/tts/dialog) | Voice (Text→Speech, multi-speaker) | Multi-speaker dialogue synthesis – reads a formatted script with multiple characters and generates a continuous conversation audio ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=Input)) ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=)). Assigns distinct voices to each speaker label, with natural turn-taking and interaction. | Voicing game dialogues, audiobooks with multiple characters, radio drama or conversational agents. Saves manually splicing individual TTS outputs. | **$0.05 per audio minute** ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=)). Real-time generation (very fast for dialogues). | **Format input as a script** with each line prefixed by a speaker name and colon ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=Input)). E.g. “Alice: … Bob: …”. The model auto-selects voices for each (likely one male, one female if not specified). Write dialogue naturally – include interjections (“uh,” “hmm”), interruptions (use ‘—’ or ‘…’ at line breaks to simulate cutting off). The model will include appropriate pause lengths between speakers. If you want a specific voice for a speaker, try using a known voice name as the speaker label (if supported), or generate those lines separately with single-speaker TTS as a workaround. But usually, letting it assign voices is easiest. Use punctuation and spelling to indicate tone (e.g. “nooo!” versus “no.”). The model handles up to 5+ speakers, but ensure each has a unique name and lines. |
| **F5 TTS** (fal-ai/f5-tts) | Voice (Voice Cloning TTS) | Diffusion-based zero-shot **voice cloning** – clones voice from a short sample and generates new speech in that voice ([Play 3.0 mini – A lightweight, reliable, cost-efficient Multilingual TTS ...](https://news.ycombinator.com/item?id=41840872#:~:text=,under%2010G%20vram%20nvidia%20gpu)). Supports high-quality, near-human mimicry of voice timbre. | Custom voice generation: cloning a particular person’s voice (with permission) for TTS – e.g. have a virtual narrator in a specific voice. Continuation of an actor’s voice for production, voice banking for someone who lost speech, or creating a new character voice from a sample. | *Likely ~$0.05–$0.06 per minute* (similar to advanced TTS). Slightly slower than standard TTS but still around real-time. | **Provide reference audio + its transcript**, and input text to speak. For best results, use ~10–30s of clear reference audio with an accurate transcript ([F5 TTS | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/f5-tts#:~:text=Text%20to%20be%20converted%20to,speech)) – this trains the voice clone on that speaker’s style. Ensure reference audio reflects the tone you want (e.g. energetic sample if you need energetic speech). Then input the desired speech text. Use normal TTS prompting for that text (punctuate for emotion, etc.). The output voice will mimic the reference. If output comes out with minor artifacts or off-emphasis, experiment by changing reference (maybe a different clip of the person’s voice) or tweaking the wording of your input text (the model might carry over some rhythms from reference transcript). Note: If the reference speaker has a strong accent and you write text in another language, the model may attempt it but with the speaker’s accent. Match language if possible. **Ethical tip:** only clone voices you have rights/consent for – e.g. your own voice or authorized voice talent – as the model will produce very realistic clones. |

================
File: docs/fal-integration-guide.md
================
# Fal.ai Integration Troubleshooting Guide

## Understanding the Flow

The Fal.ai integration follows this pattern:

1. **Client Side**: The app gets an API key from localStorage
2. **Client Side**: Makes request to our local proxy (`/api/fal`)
3. **Server Side**: Proxy extracts target URL from headers
4. **Server Side**: Proxy forwards request to Fal.ai
5. **Server Side**: Proxy returns Fal.ai response to client

## Common Issues

### 1. 400 Bad Request Error

This typically happens when the Fal.ai API doesn't understand your request.

#### Symptoms:
```
POST http://localhost:3000/api/fal 400 (Bad Request)
```

#### Solutions:

- **Check x-fal-target-url header**: Make sure your client is setting this header properly.

```typescript
// In fal.ts
requestMiddleware: async (request) => {
  // Make sure this header is being set
  request.headers = {
    ...request.headers,
    "x-fal-target-url": request.url
  };
  return request;
}
```

- **Check proxy implementation**: Ensure your route.ts correctly extracts and forwards to this URL.

```typescript
// In route.ts
const targetUrl = req.headers.get("x-fal-target-url");
if (!targetUrl) {
  return new NextResponse("Missing target URL", { status: 400 });
}
```

### 2. 401 Unauthorized Error

This happens when your API key isn't valid or isn't being passed correctly.

#### Symptoms:
```
❌ No API key available (neither in headers nor environment)
```

#### Solutions:

- **Check localStorage**: Make sure the API key is correctly saved.

```javascript
// In browser console
localStorage.getItem("falai_key") // Should return your API key
```

- **Check Authorization Header Format**: The header must be `Key YOUR_API_KEY`

```typescript
// In your code
headers.set("Authorization", `Key ${apiKey}`);
```

### 3. DNS Resolution Errors

These happen when your application can't connect to Fal.ai servers.

#### Symptoms:
```
ERR_NAME_NOT_RESOLVED when trying to fetch from "https://rest.fal.ai/..."
```

#### Solutions:

- Check your network connection
- Ensure no firewalls are blocking the connection
- Try a different network

## Detailed Implementation

### Client-Side (fal.ts)

```typescript
export const fal = createFalClient({
  // This tells the client to use our custom proxy
  proxyUrl: "/api/fal",
  
  // Gets the API key from localStorage
  credentials: () => {
    if (typeof window === "undefined") {
      return ""; // Empty string on server-side
    }
    return localStorage?.getItem("falai_key") || "";
  },
  
  // This runs before each request
  requestMiddleware: async (request) => {
    const apiKey = typeof window !== "undefined" ? 
      localStorage?.getItem("falai_key") || "" : "";
    
    if (apiKey) {
      request.headers = {
        ...request.headers,
        "Authorization": `Key ${apiKey}`,
        // THIS IS CRUCIAL - tells our proxy where to forward the request
        "x-fal-target-url": request.url
      };
    }
    
    return request;
  }
});
```

### Server-Side (route.ts)

```typescript
async function forwardToFal(req: NextRequest) {
  try {
    // Get the target URL from the header
    let targetUrl = req.headers.get("x-fal-target-url");
    
    if (!targetUrl) {
      // Fallback to path extraction if header not present
      const pathMatch = url.pathname.match(/\/api\/fal\/(.*)/);
      if (pathMatch && pathMatch[1]) {
        targetUrl = `https://rest.fal.ai/${pathMatch[1]}`;
      } else {
        return new NextResponse("Bad Request: No target URL specified", { status: 400 });
      }
    }
    
    // Get API key from headers or environment
    let authHeader = req.headers.get("authorization");
    
    if (!authHeader || !authHeader.startsWith("Key ")) {
      const envApiKey = process.env.FAL_KEY;
      
      if (envApiKey) {
        authHeader = `Key ${envApiKey}`;
      } else {
        return new NextResponse("Unauthorized: Missing API key", { status: 401 });
      }
    }
    
    // Forward the request to Fal.ai
    const forwardedResponse = await fetch(targetUrl, {
      method: req.method,
      headers: {
        ...headers,
        "Authorization": authHeader
      },
      body: requestBody
    });
    
    // Return the response to the client
    return new NextResponse(responseData, {
      status: forwardedResponse.status,
      headers: responseHeaders
    });
  } catch (error) {
    return new NextResponse(JSON.stringify({ error: String(error) }), {
      status: 500,
      headers: { "Content-Type": "application/json" }
    });
  }
}
```

## Debug Logging

If you're still having issues, add these debug logs to track the entire flow:

### In fal.ts
```typescript
console.log("🔍 API key from localStorage:", apiKey ? "Found" : "NOT FOUND");
console.log("🔍 Target URL:", request.url);
```

### In route.ts
```typescript
console.log("🔍 Request received at /api/fal");
console.log("🔍 Target URL from headers:", targetUrl);
console.log("🔍 Auth header present:", !!authHeader);
console.log("🔍 Response status:", forwardedResponse.status);
```

## Remember

The key components that make this work:
1. Setting the `x-fal-target-url` header in the client
2. Reading this header in the proxy
3. Proper authentication with the `Key` prefix
4. Correct forwarding of all request details

If any part of this chain breaks, the integration will fail.

================
File: docs/new_camera_components_guide.txt
================
# Camera Movement Component: User Guide

The Camera Movement component allows you to add dynamic motion effects to your videos, creating more professional and engaging content. The recent improvements make it more intuitive and responsive.

## How to Use the Camera Movement Component

1. **Access the Component**: 
   - The Camera Movement panel is located in the right sidebar of the video editor
   - Click on the panel header to expand/collapse it

2. **Preview Your Changes**:
   - The green box in the preview area represents your video frame
   - As you adjust settings, you'll see a real-time preview of how the camera movement will affect your video

3. **Select a Movement Type**:
   - Click the dropdown menu (currently showing "Default")
   - Choose from movement options:
     - **Horizontal**: Moves the camera side to side
     - **Vertical**: Moves the camera up and down
     - **Pan**: Rotates the camera horizontally
     - **Tilt**: Rotates the camera vertically
     - **Roll**: Rotates the camera around the center
     - **Zoom**: Moves the camera closer or further

4. **Adjust Movement Intensity**:
   - Once you select a movement type, a slider will appear
   - Move the slider left (negative values) or right (positive values) to control the direction and intensity
   - The value box on the right shows the exact setting (-10 to +10 range)

5. **Apply Your Changes**:
   - Click the "Generate" button at the bottom to apply the camera movement to your video

## Benefits of the Improved Camera Movement Component

1. **More Intuitive Visual Feedback**:
   - The new transition animations make it easier to understand how your adjustments affect the video
   - The cleaner UI design makes the component more readable and less cluttered

2. **Professional Video Effects**:
   - Create cinematic pans, zooms, and camera movements without complex editing software
   - Add visual interest to static footage with subtle movements

3. **Creative Control**:
   - Fine-tune movements with precise slider controls
   - Combine different movement types for custom effects by adjusting the timeline

4. **Real-time Preview**:
   - See exactly how your video will look before generating the final result
   - The visual preview helps you understand the effect of different settings

## Tips for Effective Camera Movements

- **Start Subtle**: Small camera movements often look more professional than dramatic ones
- **Match the Content**: Choose movements that enhance your story (e.g., zoom for emphasis, pan to reveal)
- **Combine with Timeline**: Add different camera movements to different segments of your video
- **Use Reset**: If you don't like your changes, click "Reset" to return to default settings

The improved Camera Movement component gives you more creative control while being easier to use, helping you create more engaging and professional-looking videos.

================
File: docs/video_images_music_models..md
================
I'll create a list of all models available in your SDK by checking the AVAILABLE_ENDPOINTS array in fal.ts:

Read file: /Users/fabiofurlano/Documents/app/video-starter-kit/src/lib/fal.ts
# Complete Model List in the Video Starter Kit SDK

## Video Models
1. **Veo 2**
   - Creates videos with realistic motion and high-quality output, up to 4K

2. **LTX Video v0.95 Multiconditioning** (New Addition)
   - Generates videos from prompts and images
   - Supports direct image input for frames

3. **Minimax Video 01 Live**
   - High-quality video with realistic motion and physics
   - Supports image input

4. **Hunyuan**
   - High visual quality with motion diversity and text alignment

5. **Kling 1.5 Pro**
   - High-quality video
   - Supports image input

6. **Kling 1.0 Standard**
   - High-quality video
   - Supports camera control features

7. **Luma Dream Machine 1.5**
   - High-quality video
   - Supports image input

8. **MMAudio V2**
   - Generates synchronized audio given video and/or text inputs
   - Can be combined with video models to create videos with audio

9. **sync.so -- lipsync 1.8.0**
   - Generates realistic lipsync animations from audio
   - Uses advanced algorithms for high-quality synchronization
   - Requires both video and audio inputs

## Image Models
1. **Flux Dev**
   - Generates video from text prompt

2. **Flux Schnell**
   - Generates video from text prompt

3. **Flux Pro 1.1 Ultra**
   - Generates video from text prompt

4. **Stable Diffusion 3.5 Large**
   - Excellent image quality, typography, and complex prompt understanding

## Music Models
1. **Minimax Music**
   - Creates high-quality, diverse musical compositions
   - Can use reference audio input

2. **Stable Audio**
   - Stable Diffusion music creation with high-quality tracks

## Voiceover Models
1. **PlayHT TTS v3**
   - Fluent and faithful speech with flow matching
   - Default voice: Dexter (English (US)/American)

2. **PlayAI Text-to-Speech Dialog**
   - Generates natural-sounding multi-speaker dialogues
   - Perfect for storytelling, games, animations, and interactive media
   - Supports multiple voices with turn prefixes

3. **F5 TTS**
   - Fluent and faithful speech with flow matching
   - Uses reference audio and text

This comprehensive list covers all models available in your Video Starter Kit SDK, organized by category. The newest addition, LTX Video v0.95, gives you multicondition capabilities that combine text prompts with image inputs for video generation.

================
File: public/key-checker.js
================
// This script checks if the falai_key is in localStorage
// If not, it redirects to the key-injector.html page

(function () {
  console.log("🔑 KEY CHECKER: Script loaded");

  // Check if the falai_key is in localStorage
  const key = localStorage.getItem("falai_key");
  console.log("🔑 KEY CHECKER: falai_key in localStorage:", key ? "YES" : "NO");

  if (!key) {
    // If the key is not in localStorage, redirect to the key-injector.html page
    console.log(
      "🔑 KEY CHECKER: No falai_key found in localStorage, redirecting to key-injector.html",
    );
    window.location.href = "/key-injector.html";
  } else {
    console.log(
      "🔑 KEY CHECKER: falai_key found in localStorage:",
      key.substring(0, 5) + "...",
    );
  }
})();

================
File: public/key-copy.js
================
// Script to receive the falai_key from parent via postMessage
console.log("🔑🔑🔑 KEY-COPY.JS LOADED 🔑🔑🔑");

// DIRECT ACCESS ATTEMPT - Try to directly access parent localStorage
try {
  console.log("🔑 DIRECT ACCESS: Attempting to access parent localStorage");
  const parentKey = window.parent.localStorage.getItem("falai_key");
  console.log("🔑 DIRECT ACCESS: Parent key found?", parentKey ? "YES" : "NO");

  if (parentKey) {
    localStorage.setItem("falai_key", parentKey);
    console.log(
      "🔑 DIRECT ACCESS: Successfully copied key to localStorage:",
      parentKey.substring(0, 5) + "...",
    );
  }
} catch (e) {
  console.error("🔑 DIRECT ACCESS: Error accessing parent localStorage:", e);
}

// POSTMESSAGE APPROACH - Listen for messages from the parent page
console.log("🔑 POSTMESSAGE: Setting up event listener for API key");
window.addEventListener("message", function (event) {
  // Log all messages for debugging
  console.log("🔑 POSTMESSAGE: Received message:", event.data?.type);

  // Check if this is a USER_DATA message
  if (event.data && event.data.type === "USER_DATA") {
    console.log("🔑 POSTMESSAGE: Received USER_DATA message");

    // Extract the falai_key from the message
    const falaiKey = event.data.apiKeys?.falai;

    if (falaiKey) {
      // Store the key in localStorage
      localStorage.setItem("falai_key", falaiKey);
      console.log(
        "🔑 POSTMESSAGE: Successfully stored falai_key:",
        falaiKey.substring(0, 5) + "...",
      );
    } else {
      console.warn("🔑 POSTMESSAGE: No falai_key found in USER_DATA message");
    }
  }
});

// Send a ready message to the parent
try {
  console.log("🔑 POSTMESSAGE: Sending IFRAME_READY message to parent");
  window.parent.postMessage({ type: "IFRAME_READY" }, "*");
  console.log("🔑 POSTMESSAGE: Sent IFRAME_READY message to parent");
} catch (e) {
  console.error("🔑 POSTMESSAGE: Error sending ready message:", e);
}

// MANUAL INJECTION - Directly set the key in localStorage
// This is a last resort if all else fails
console.log("🔑 MANUAL INJECTION: Setting hardcoded key in localStorage");
const hardcodedKey =
  "5eda4036-c86a-4637-873b-5f79ea72d588:b58feab09b3bd2bf165e5ba2ff7be918";
localStorage.setItem("falai_key", hardcodedKey);
console.log(
  "🔑 MANUAL INJECTION: Successfully set hardcoded key in localStorage",
);

// Check if the key is now in localStorage
const finalKey = localStorage.getItem("falai_key");
console.log(
  "🔑 FINAL CHECK: falai_key in localStorage:",
  finalKey ? "YES (" + finalKey.substring(0, 5) + "...)" : "NO",
);

================
File: public/key-injector.html
================
<!DOCTYPE html>
<html>
<head>
    <title>Key Injector</title>
    <script>
        // This script runs when the page loads
        window.onload = function() {
            console.log('🔑 KEY INJECTOR HTML: Page loaded');
            
            // Set the key in localStorage
            const key = '5eda4036-c86a-4637-873b-5f79ea72d588:b58feab09b3bd2bf165e5ba2ff7be918';
            localStorage.setItem('falai_key', key);
            console.log('🔑 KEY INJECTOR HTML: Set falai_key in localStorage:', key.substring(0, 5) + '...');
            
            // Check if the key is now in localStorage
            const storedKey = localStorage.getItem('falai_key');
            console.log('🔑 KEY INJECTOR HTML: falai_key in localStorage:', storedKey ? 'YES' : 'NO');
            
            // Redirect to the main page
            window.location.href = '/';
        };
    </script>
</head>
<body>
    <h1>Key Injector</h1>
    <p>Setting the falai_key in localStorage and redirecting...</p>
</body>
</html>

================
File: public/model-guide.md
================
# Video Starter Kit Model Guide

This comprehensive guide provides detailed information about all AI models available in the Video Starter Kit, including usage tips, pricing, and best practices for optimal results.

## Table of Contents

- [Video Models](#video-models) 🎬
  - [Veo 2](#veo-2)
  - [LTX Video v0.95](#ltx-video-v095)
  - [Minimax Video 01 Live](#minimax-video-01-live)
  - [Hunyuan](#hunyuan)
  - [Kling 1.5 Pro](#kling-15-pro)
  - [Kling 1.0 Standard](#kling-10-standard)
  - [Luma Dream Machine 1.5](#luma-dream-machine-15)
  - [MMAudio V2](#mmaudio-v2)
  - [sync.so -- lipsync 1.8.0](#syncso----lipsync-180)
- [Image Models](#image-models) 🖼️
  - [Flux Dev](#flux-dev)
  - [Flux Schnell](#flux-schnell)
  - [Flux Pro 1.1 Ultra](#flux-pro-11-ultra)
  - [Stable Diffusion 3.5 Large](#stable-diffusion-35-large)
- [Music Models](#music-models) 🎵
  - [Minimax Music](#minimax-music)
  - [Stable Audio](#stable-audio)
- [Voiceover Models](#voiceover-models) 🎙️
  - [PlayHT TTS v3](#playht-tts-v3)
  - [PlayAI Text-to-Speech Dialog](#playai-text-to-speech-dialog)
  - [F5 TTS](#f5-tts)

---

## Video Models

<div style="background-color: rgba(0, 123, 255, 0.1); border-left: 4px solid #007bff; padding: 1rem; margin-bottom: 1.5rem;">
<strong>💡 Pro Tip:</strong> Video models transform text prompts or images into dynamic video content. They vary in style, quality, and motion capabilities.
</div>

### Veo 2

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">PROFESSIONAL</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">HIGH QUALITY</span>
</div>

![Veo 2 Example](/images/models/veo2-placeholder.jpg)

Veo 2 creates videos with realistic motion and high-quality output, up to 4K resolution.

**Endpoint ID:** `fal-ai/veo2`

**✨ Capabilities:**
- Text-to-video generation
- High-quality 4K resolution output
- Realistic motion and physics
- Detailed textures and lighting

**💰 Pricing:**
- Base price: $1.25 for 5-second video
- Additional seconds: $0.25 per second (limited time offer)
- Regular pricing: $2.50 for 5-second video, $0.50 per additional second

**🔍 Best Prompts Include:**
- Detailed scene descriptions
- Lighting information
- Camera movement
- Stylistic preferences

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">The camera floats gently through rows of pastel-painted wooden beehives, buzzing honeybees gliding in and out of frame. The motion settles on a beekeeper standing at the center, wearing a pristine white beekeeping suit gleaming in the golden afternoon light. Behind him, tall sunflowers sway rhythmically in the breeze. Shot with a 35mm lens on Kodak Portra 400 film, the golden light creates rich textures.</pre>
</div>

**📝 Tips:**
- Include camera movement terms like "panning," "zooming," "dolly shot"
- Specify film stock for consistent aesthetics (e.g., "Kodak Portra 400")
- Mention specific lens types for different looks (e.g., "35mm lens," "85mm portrait lens")
- Describe lighting conditions in detail

---

### LTX Video v0.95

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">NEW</span>
<span style="background-color: #e6f7ee; color: #00843d; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">IMAGE-GUIDED</span>
</div>

![LTX Video Example](/images/models/ltx-video-placeholder.jpg)

LTX Video is a new multiconditioning model that can generate videos from both text prompts and reference images.

**Endpoint ID:** `fal-ai/ltx-video-v095/multiconditioning`

**✨ Capabilities:**
- Generate videos from text prompts
- Use reference images as visual guides
- Direct image-to-video conversion without separate endpoint

**🔧 Unique Features:**
- **imageForFrame:** Can use input images directly as reference for video frames

**⚙️ Best Practices:**
- For image-guided generation, use clear, high-quality reference images
- Balance text prompt specificity with visual guidance from images
- Use descriptive motion terms when you want specific movements

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt with Image:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A tranquil forest scene with sunlight filtering through leaves, gentle wind creating subtle movement in the branches</pre>
</div>

**📝 Tips:**
- Upload a reference image along with your prompt for better results
- The model will use both your text and image to guide the video generation

---

### Minimax Video 01 Live

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">VERSATILE</span>
<span style="background-color: #e6f7ee; color: #00843d; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">IMAGE-GUIDED</span>
</div>

![Minimax Video Example](/images/models/minimax-video-placeholder.jpg)

Generates high-quality videos with realistic motion and accurate physics simulation.

**Endpoint ID:** `fal-ai/minimax/video-01-live`

**✨ Capabilities:**
- Text-to-video generation
- Image-to-video transformation
- Realistic physics and motion

**🖼️ Input Assets:** Supports image input for video generation

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.</pre>
</div>

**📝 Tips:**
- Include specific character actions for better motion
- Describe detailed environments for richer scenes
- Specify clothing and items for better character rendering

---

### Hunyuan

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">HIGH QUALITY</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">DIVERSE MOTION</span>
</div>

![Hunyuan Example](/images/models/hunyuan-placeholder.jpg)

Produces videos with high visual quality, diverse motion patterns, and strong alignment with text prompts.

**Endpoint ID:** `fal-ai/hunyuan-video`

**✨ Capabilities:**
- High visual fidelity
- Diverse motion patterns
- Strong text-to-video alignment

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A serene lakeside at dawn, with mist rising from the water's surface, gentle ripples spreading across the lake as a fish jumps, mountains reflected in the clear water, golden sunlight gradually illuminating the scene</pre>
</div>

**📝 Tips:**
- Use detailed descriptions of lighting and atmosphere
- Include a mix of static and dynamic elements
- Specify the mood or emotional tone of the scene

---

### Kling 1.5 Pro

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">PROFESSIONAL</span>
<span style="background-color: #e6f7ee; color: #00843d; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">IMAGE-GUIDED</span>
</div>

![Kling Pro Example](/images/models/kling-pro-placeholder.jpg)

Generates high-quality videos with an emphasis on visual quality and smooth motion.

**Endpoint ID:** `fal-ai/kling-video/v1.5/pro`

**✨ Capabilities:**
- High-quality video generation
- Image-to-video transformation

**🖼️ Input Assets:** Supports image input for video generation

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A macro shot of a vivid blue morpho butterfly slowly opening and closing its wings while perched on a bright orange flower. Dew drops on the petals glisten in the soft morning light.</pre>
</div>

**📝 Tips:**
- Specify camera perspectives (macro, wide-angle, etc.)
- Include details about lighting and atmosphere
- Describe motions clearly and specifically

---

### Kling 1.0 Standard

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">CAMERA CONTROL</span>
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">STANDARD</span>
</div>

![Kling Standard Example](/images/models/kling-standard-placeholder.jpg)

Standard video generation model with camera control capabilities.

**Endpoint ID:** `fal-ai/kling-video/v1/standard/text-to-video`

**✨ Capabilities:**
- Text-to-video generation
- Camera movement control

**🎥 Features:**
- **cameraControl:** Allows for specifying camera movements during generation

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt with Camera Control:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A drone shot of a Mediterranean coastal town, white buildings with blue roofs against the deep blue sea, boats in the harbor, people walking along narrow streets</pre>
</div>

**📝 Tips for Camera Control:**
- Use the Camera Control panel to specify movement type
- Options include pan, tilt, zoom, rotation, and dolly movements
- Adjust the intensity of the movement using the slider

---

### Luma Dream Machine 1.5

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">HIGH QUALITY</span>
<span style="background-color: #e6f7ee; color: #00843d; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">IMAGE-GUIDED</span>
</div>

![Luma Dream Machine Example](/images/models/luma-placeholder.jpg)

High-quality video generation with support for image-guided generation.

**Endpoint ID:** `fal-ai/luma-dream-machine`

**✨ Capabilities:**
- High-quality video generation
- Image-to-video transformation

**🖼️ Input Assets:** Supports image input for video generation

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A futuristic cityscape at night, glowing holographic advertisements reflect in puddles on the street, flying cars zoom between towering skyscrapers, neon lights cast colorful shadows</pre>
</div>

**📝 Tips:**
- Include both static and dynamic elements
- Specify lighting details for better atmosphere
- When using image input, ensure it matches your text description

---

### MMAudio V2

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">AUDIO</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">SYNCED SOUND</span>
</div>

![MMAudio Example](/images/models/mmaudio-placeholder.jpg)

Generates synchronized audio for videos using text and video inputs.

**Endpoint ID:** `fal-ai/mmaudio-v2`

**✨ Capabilities:**
- Generate synchronized audio for videos
- Create sound effects based on text descriptions
- Combine with video models for complete audio-visual experiences

**🎞️ Input Assets:** Requires video input

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">Background city ambient noise with distant traffic, occasional car horns, people chatting, and footsteps on pavement</pre>
</div>

**📝 Tips:**
- Be specific about the type of sounds you want
- Describe layered audio elements for richer soundscapes
- Mention timing details if specific audio cues are needed

---

### sync.so -- lipsync 1.8.0

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">AUDIO</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">LIP SYNC</span>
</div>

![Lip Sync Example](/images/models/lipsync-placeholder.jpg)

Generates realistic lip-syncing animations from audio inputs.

**Endpoint ID:** `fal-ai/sync-lipsync`

**✨ Capabilities:**
- Generate lip-sync animations from audio
- Create realistic mouth movements for characters
- Synchronize speech with video content

**🎞️ Input Assets:** Requires both video and audio inputs

**📝 Tips:**
- Use clear, high-quality audio for best results
- Close-up videos of faces work best for lip-syncing
- Ensure the face in the video is well-lit and clearly visible

---

## Image Models

<div style="background-color: rgba(25, 135, 84, 0.1); border-left: 4px solid #198754; padding: 1rem; margin-bottom: 1.5rem;">
<strong>💡 Pro Tip:</strong> Image models convert text prompts into static images with various styles and qualities. They're perfect for creating reference images or standalone visual content.
</div>

### Flux Dev

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">VERSATILE</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">GENERAL PURPOSE</span>
</div>

![Flux Dev Example](/images/models/flux-dev-placeholder.jpg)

General-purpose text-to-image model for generating images from text prompts.

**Endpoint ID:** `fal-ai/flux/dev`

**✨ Capabilities:**
- Text-to-image generation
- General-purpose image creation

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A photorealistic image of a coastal city at sunset, golden light reflecting off skyscraper windows, palm trees lining the boulevard, small boats in the harbor, wispy clouds in the orange and purple sky</pre>
</div>

**📝 Tips:**
- Include detailed descriptions of visual elements
- Specify lighting and atmosphere
- Mention style preferences if you have any

---

### Flux Schnell

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">FAST</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">QUICK ITERATIONS</span>
</div>

![Flux Schnell Example](/images/models/flux-schnell-placeholder.jpg)

Fast variant of the Flux text-to-image model for quicker generations.

**Endpoint ID:** `fal-ai/flux/schnell`

**✨ Capabilities:**
- Faster text-to-image generation
- Similar quality to Flux Dev with faster processing

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A detailed watercolor painting of a medieval castle on a hilltop, autumn trees with red and orange leaves surrounding it, a winding path leading to the gate, small figures of knights and horses in the foreground</pre>
</div>

**📝 Tips:**
- Similar to Flux Dev but optimized for speed
- Great for rapid iterations and testing

---

### Flux Pro 1.1 Ultra

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">PROFESSIONAL</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">HIGH QUALITY</span>
</div>

![Flux Pro Example](/images/models/flux-pro-placeholder.jpg)

Professional-grade text-to-image model with enhanced visual quality.

**Endpoint ID:** `fal-ai/flux-pro/v1.1-ultra`

**✨ Capabilities:**
- High-quality text-to-image generation
- Enhanced detail and composition

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A hyper-realistic portrait of an elderly fisherman with weathered skin and deep wrinkles, wearing a faded blue cap, warm golden hour lighting, detailed texture of beard stubble and salt-crusted skin, shot with a Canon EOS R5 85mm f/1.2 lens</pre>
</div>

**📝 Tips:**
- Include camera and lens details for photorealistic outputs
- Specify lighting conditions in detail
- Use technical photography terms for better results

---

### Stable Diffusion 3.5 Large

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">ADVANCED</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">TYPOGRAPHY</span>
</div>

![Stable Diffusion Example](/images/models/sd35-placeholder.jpg)

Advanced text-to-image model with improved typography and complex prompt understanding.

**Endpoint ID:** `fal-ai/stable-diffusion-v35-large`

**✨ Capabilities:**
- High-quality image generation
- Excellent typography rendering
- Complex prompt understanding

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A detailed cyberpunk street scene with Japanese and English neon signs saying "DIGITAL DREAMS" and "未来の都市", holographic advertisements, people with cybernetic implants walking under umbrella drones in the rain, reflective puddles on the ground</pre>
</div>

**📝 Tips:**
- Great for scenes requiring text or typography
- Can handle multi-language text prompts
- Excels at complex, detailed scenes

---

## Music Models

<div style="background-color: rgba(108, 117, 125, 0.1); border-left: 4px solid #6c757d; padding: 1rem; margin-bottom: 1.5rem;">
<strong>💡 Pro Tip:</strong> Music models generate original audio compositions based on text descriptions. They can create everything from simple beats to complex arrangements.
</div>

### Minimax Music

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">VERSATILE</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">REFERENCE AUDIO</span>
</div>

![Minimax Music Example](/images/models/minimax-music-placeholder.jpg)

Creates high-quality, diverse musical compositions with advanced AI techniques.

**Endpoint ID:** `fal-ai/minimax-music`

**✨ Capabilities:**
- Generate diverse musical compositions
- Create various music styles and genres
- Option to use reference audio for style matching

**🎵 Input Assets:** Optionally supports audio reference input

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">Upbeat electronic dance music with energetic synth leads, deep bass, and a driving drum beat at 128 BPM, building to a euphoric drop with arpeggiated melodies</pre>
</div>

**📝 Tips:**
- Specify BPM (beats per minute) for rhythm control
- Include instrument details
- Describe mood and energy level
- Mention genre or style references

---

### Stable Audio

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">VERSATILE</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">SOUND EFFECTS</span>
</div>

![Stable Audio Example](/images/models/stable-audio-placeholder.jpg)

Generate high-quality music tracks and sound effects using Stability AI's audio model.

**Endpoint ID:** `fal-ai/stable-audio`

**✨ Capabilities:**
- Generate music in various styles
- Create sound effects and ambient sounds
- Produce variable-length audio outputs

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompts:</strong>

For Music:
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">128 BPM tech house drum loop with deep bass, crisp hi-hats, and a driving kick drum</pre>

For Sound Effects:
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">Door slam. High-quality, stereo.</pre>
</div>

**📝 Tips:**
- For music, specify BPM, genre, and instruments
- For sound effects, add "High-quality, stereo" to your prompt
- Keep prompts concise but descriptive
- Describe the mood or feeling you want to convey

---

## Voiceover Models

<div style="background-color: rgba(220, 53, 69, 0.1); border-left: 4px solid #dc3545; padding: 1rem; margin-bottom: 1.5rem;">
<strong>💡 Pro Tip:</strong> Voiceover models convert text into natural-sounding speech. They're ideal for narration, dialogue, and character voices in your projects.
</div>

### PlayHT TTS v3

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">NATURAL</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">EMOTIONAL</span>
</div>

![PlayHT Example](/images/models/playht-placeholder.jpg)

Generates fluent and natural speech with improved emotional tones.

**Endpoint ID:** `fal-ai/playht/tts/v3`

**✨ Capabilities:**
- High-quality text-to-speech
- Natural-sounding voice synthesis
- Fast processing for efficient workflows

**🗣️ Default Voice:** Dexter (English (US)/American)

**💰 Pricing:** $0.03 per minute per audio minute generated

**🎯 Example Usage:**
- Narration for videos
- Voiceovers for presentations
- Audio content creation

**📝 Tips:**
- Use natural language patterns for best results
- Add punctuation to control pacing and pauses
- For emphasis, use italics or capitalize words

---

### PlayAI Text-to-Speech Dialog

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">DIALOGUE</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">MULTI-SPEAKER</span>
</div>

![PlayAI Dialog Example](/images/models/playai-dialog-placeholder.jpg)

Generates natural-sounding multi-speaker dialogues for storytelling and interactive media.

**Endpoint ID:** `fal-ai/playai/tts/dialog`

**✨ Capabilities:**
- Multi-speaker dialogue generation
- Natural-sounding conversations
- Enhanced expressiveness for storytelling

**🗣️ Default Voices:**
- Speaker 1: Jennifer (English (US)/American)
- Speaker 2: Furio (English (IT)/Italian)

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Input:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">Speaker 1: Have you heard about the new AI technologies?
Speaker 2: Yes, they're fascinating! I've been reading about them extensively.
Speaker 1: What interests you the most about them?
Speaker 2: The way they can create content that seems so human-like.</pre>
</div>

**📝 Tips:**
- Use speaker prefixes consistently
- Include natural conversational elements
- Vary sentence length and structure for realism
- Include emotional cues in parentheses for better expression

---

### F5 TTS

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">VOICE CLONING</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">REFERENCE AUDIO</span>
</div>

![F5 TTS Example](/images/models/f5-tts-placeholder.jpg)

Fluent and faithful speech synthesis with flow matching technology.

**Endpoint ID:** `fal-ai/f5-tts`

**✨ Capabilities:**
- High-quality text-to-speech
- Reference audio matching
- Natural speech patterns

**⚙️ Default Settings:**
- Uses a reference audio and text
- Removes silence by default

**🎯 Example Usage:**
- Voice cloning with reference audio
- Consistent voice styling across projects
- Voice preservation for legacy content

**📝 Tips:**
- Provide clear reference audio for better voice matching
- Ensure reference text matches the audio for accurate voice learning
- Keep generated text in a similar style to the reference for best results

---

## General Tips for All Models

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 8px; padding: 1.5rem; margin: 2rem 0;">

### 🧠 Prompt Engineering

1. **Be Specific:** The more details you provide, the better the AI can match your vision
2. **Use Visual Language:** Describe what you want to see, hear, or experience
3. **Include Technical Details:** For appropriate models, include camera specs, music terminology, or voice characteristics
4. **Experiment:** Try different prompts and variations to find what works best

### 🔄 Optimal Workflow

1. **Start Simple:** Begin with basic prompts and add complexity
2. **Iterate:** Use generated results to inform your next prompt
3. **Combine Models:** Use multiple models together for comprehensive projects
4. **Save Successful Prompts:** Keep a library of prompts that work well

### 💼 Resource Usage

1. **Consider Costs:** Be aware of the pricing for each model
2. **Batch Processing:** Plan your generations to maximize efficiency
3. **Test First:** Use shorter generations initially to test concepts

</div>

---

*This guide will be regularly updated as new models are added or existing models are updated.*

================
File: public/model-hints.json
================
{
  "fal-ai/veo2": {
    "label": "Veo 2",
    "category": "Video",
    "headline": "High-quality videos with realistic motion and 4K resolution",
    "bestFor": [
      "Professional videos",
      "High-resolution content",
      "Realistic motion",
      "Detailed scenes"
    ],
    "tips": [
      "Include camera movement terms like 'panning', 'zooming', 'dolly shot'",
      "Specify film stock for consistent aesthetics (e.g., 'Kodak Portra 400')",
      "Mention specific lens types for different looks (e.g., '35mm lens')",
      "Describe lighting conditions in detail"
    ],
    "pricePerCreditUSD": 1.25,
    "secondsPerDollar": 4,
    "examplePrompt": "The camera floats gently through rows of pastel-painted wooden beehives, buzzing honeybees gliding in and out of frame. Shot with a 35mm lens on Kodak Portra 400 film, the golden light creates rich textures."
  },
  "fal-ai/ltx-video-v095/multiconditioning": {
    "label": "LTX Video v0.95",
    "category": "Video",
    "headline": "Generate videos from both text prompts and reference images",
    "bestFor": [
      "Image-guided video generation",
      "Visual reference-based videos",
      "Mixed media projects"
    ],
    "tips": [
      "Upload a reference image along with your prompt for better results",
      "Balance text prompt specificity with visual guidance from images",
      "Use descriptive motion terms when you want specific movements"
    ],
    "pricePerCreditUSD": 0.5,
    "secondsPerDollar": 10,
    "examplePrompt": "A tranquil forest scene with sunlight filtering through leaves, gentle wind creating subtle movement in the branches"
  },
  "fal-ai/minimax/video-01-live": {
    "label": "Minimax Video 01 Live",
    "category": "Video",
    "headline": "High-quality text-to-video with realistic motion and physics",
    "bestFor": [
      "Lifestyle b-roll",
      "Short social videos",
      "Image-guided clips"
    ],
    "tips": [
      "Include specific character actions for better motion",
      "Describe detailed environments for richer scenes",
      "Specify clothing and items for better character rendering"
    ],
    "pricePerCreditUSD": 0.2,
    "secondsPerDollar": 25,
    "examplePrompt": "A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse."
  },
  "fal-ai/hunyuan-video": {
    "label": "Hunyuan",
    "category": "Video",
    "headline": "High visual fidelity with diverse motion patterns",
    "bestFor": ["Scenic videos", "Nature scenes", "Atmospheric content"],
    "tips": [
      "Use detailed descriptions of lighting and atmosphere",
      "Include a mix of static and dynamic elements",
      "Specify the mood or emotional tone of the scene"
    ],
    "pricePerCreditUSD": 0.3,
    "secondsPerDollar": 16.7,
    "examplePrompt": "A serene lakeside at dawn, with mist rising from the water's surface, gentle ripples spreading across the lake as a fish jumps, mountains reflected in the clear water, golden sunlight gradually illuminating the scene"
  },
  "fal-ai/kling-video/v1.5/pro": {
    "label": "Kling 1.5 Pro",
    "category": "Video",
    "headline": "High-quality videos with smooth motion and image guidance",
    "bestFor": [
      "Professional videos",
      "Image-to-video transformation",
      "Detailed close-ups"
    ],
    "tips": [
      "Specify camera perspectives (macro, wide-angle, etc.)",
      "Include details about lighting and atmosphere",
      "Describe motions clearly and specifically"
    ],
    "pricePerCreditUSD": 0.4,
    "secondsPerDollar": 12.5,
    "examplePrompt": "A macro shot of a vivid blue morpho butterfly slowly opening and closing its wings while perched on a bright orange flower. Dew drops on the petals glisten in the soft morning light."
  },
  "fal-ai/kling-video/v1/standard/text-to-video": {
    "label": "Kling 1.0 Standard",
    "category": "Video",
    "headline": "Standard video generation with camera control capabilities",
    "bestFor": [
      "Camera movement effects",
      "Standard quality videos",
      "Quick iterations"
    ],
    "tips": [
      "Use the Camera Control panel to specify movement type",
      "Options include pan, tilt, zoom, rotation, and dolly movements",
      "Adjust the intensity of the movement using the slider"
    ],
    "pricePerCreditUSD": 0.2,
    "secondsPerDollar": 25,
    "examplePrompt": "A drone shot of a Mediterranean coastal town, white buildings with blue roofs against the deep blue sea, boats in the harbor, people walking along narrow streets"
  },
  "fal-ai/luma-dream-machine": {
    "label": "Luma Dream Machine 1.5",
    "category": "Video",
    "headline": "High-quality video generation with image guidance support",
    "bestFor": [
      "Cinematic scenes",
      "Futuristic content",
      "Image-guided videos"
    ],
    "tips": [
      "Include both static and dynamic elements",
      "Specify lighting details for better atmosphere",
      "When using image input, ensure it matches your text description"
    ],
    "pricePerCreditUSD": 0.35,
    "secondsPerDollar": 14.3,
    "examplePrompt": "A futuristic cityscape at night, glowing holographic advertisements reflect in puddles on the street, flying cars zoom between towering skyscrapers, neon lights cast colorful shadows"
  },
  "fal-ai/mmaudio-v2": {
    "label": "MMAudio V2",
    "category": "Audio",
    "headline": "Synchronized audio generation for videos",
    "bestFor": [
      "Video sound effects",
      "Ambient audio",
      "Synchronized soundtracks"
    ],
    "tips": [
      "Be specific about the type of sounds you want",
      "Describe layered audio elements for richer soundscapes",
      "Mention timing details if specific audio cues are needed"
    ],
    "pricePerCreditUSD": 0.15,
    "secondsPerDollar": 33.3,
    "examplePrompt": "Background city ambient noise with distant traffic, occasional car horns, people chatting, and footsteps on pavement"
  },
  "fal-ai/sync-lipsync": {
    "label": "sync.so -- lipsync 1.8.0",
    "category": "Audio",
    "headline": "Realistic lip-syncing animations from audio inputs",
    "bestFor": [
      "Character dialogue",
      "Talking head videos",
      "Animated content"
    ],
    "tips": [
      "Use clear, high-quality audio for best results",
      "Close-up videos of faces work best for lip-syncing",
      "Ensure the face in the video is well-lit and clearly visible"
    ],
    "pricePerCreditUSD": 0.1,
    "secondsPerDollar": 50,
    "examplePrompt": "A character speaking clearly with natural pauses and emphasis on key words"
  },
  "fal-ai/flux/dev": {
    "label": "Flux Dev",
    "category": "Image",
    "headline": "General-purpose text-to-image generation",
    "bestFor": [
      "Quick image creation",
      "General visual concepts",
      "Versatile styles"
    ],
    "tips": [
      "Include detailed descriptions of visual elements",
      "Specify lighting and atmosphere",
      "Mention style preferences if you have any"
    ],
    "pricePerCreditUSD": 0.05,
    "secondsPerDollar": 20,
    "examplePrompt": "A photorealistic image of a coastal city at sunset, golden light reflecting off skyscraper windows, palm trees lining the boulevard, small boats in the harbor, wispy clouds in the orange and purple sky"
  },
  "fal-ai/flux/schnell": {
    "label": "Flux Schnell",
    "category": "Image",
    "headline": "Fast text-to-image generation for quick iterations",
    "bestFor": ["Rapid prototyping", "Quick iterations", "Testing concepts"],
    "tips": [
      "Similar to Flux Dev but optimized for speed",
      "Great for rapid iterations and testing",
      "Use for initial concept exploration"
    ],
    "pricePerCreditUSD": 0.03,
    "secondsPerDollar": 33.3,
    "examplePrompt": "A detailed watercolor painting of a medieval castle on a hilltop, autumn trees with red and orange leaves surrounding it, a winding path leading to the gate"
  },
  "fal-ai/flux-pro/v1.1-ultra": {
    "label": "Flux Pro 1.1 Ultra",
    "category": "Image",
    "headline": "Professional-grade text-to-image with enhanced quality",
    "bestFor": [
      "Professional imagery",
      "High-detail photos",
      "Portfolio-quality images"
    ],
    "tips": [
      "Include camera and lens details for photorealistic outputs",
      "Specify lighting conditions in detail",
      "Use technical photography terms for better results"
    ],
    "pricePerCreditUSD": 0.1,
    "secondsPerDollar": 10,
    "examplePrompt": "A hyper-realistic portrait of an elderly fisherman with weathered skin and deep wrinkles, wearing a faded blue cap, warm golden hour lighting, shot with a Canon EOS R5 85mm f/1.2 lens"
  },
  "fal-ai/stable-diffusion-v35-large": {
    "label": "Stable Diffusion 3.5 Large",
    "category": "Image",
    "headline": "Advanced text-to-image with typography and complex prompts",
    "bestFor": [
      "Text-heavy images",
      "Multi-language content",
      "Complex scenes"
    ],
    "tips": [
      "Great for scenes requiring text or typography",
      "Can handle multi-language text prompts",
      "Excels at complex, detailed scenes"
    ],
    "pricePerCreditUSD": 0.08,
    "secondsPerDollar": 12.5,
    "examplePrompt": "A detailed cyberpunk street scene with Japanese and English neon signs saying \"DIGITAL DREAMS\" and \"未来の都市\", holographic advertisements, people with cybernetic implants walking under umbrella drones in the rain"
  },
  "fal-ai/minimax-music": {
    "label": "Minimax Music",
    "category": "Music",
    "headline": "High-quality, diverse musical compositions",
    "bestFor": [
      "Original soundtracks",
      "Background music",
      "Style-matched compositions"
    ],
    "tips": [
      "Specify BPM (beats per minute) for rhythm control",
      "Include instrument details",
      "Describe mood and energy level",
      "Mention genre or style references"
    ],
    "pricePerCreditUSD": 0.15,
    "secondsPerDollar": 33.3,
    "examplePrompt": "Upbeat electronic dance music with energetic synth leads, deep bass, and a driving drum beat at 128 BPM, building to a euphoric drop with arpeggiated melodies"
  },
  "fal-ai/stable-audio": {
    "label": "Stable Audio",
    "category": "Music",
    "headline": "High-quality music and sound effects generation",
    "bestFor": ["Sound effects", "Music tracks", "Ambient sounds"],
    "tips": [
      "For music, specify BPM, genre, and instruments",
      "For sound effects, add \"High-quality, stereo\" to your prompt",
      "Keep prompts concise but descriptive",
      "Describe the mood or feeling you want to convey"
    ],
    "pricePerCreditUSD": 0.1,
    "secondsPerDollar": 50,
    "examplePrompt": "128 BPM tech house drum loop with deep bass, crisp hi-hats, and a driving kick drum"
  },
  "fal-ai/playht/tts/v3": {
    "label": "PlayHT TTS v3",
    "category": "Voiceover",
    "headline": "Natural-sounding speech with emotional tones",
    "bestFor": ["Video narration", "Presentations", "Content creation"],
    "tips": [
      "Use natural language patterns for best results",
      "Add punctuation to control pacing and pauses",
      "For emphasis, use italics or capitalize words"
    ],
    "pricePerCreditUSD": 0.03,
    "secondsPerDollar": 33.3,
    "examplePrompt": "Welcome to our video tutorial. Today, we'll explore the fascinating world of artificial intelligence and its applications in creative content generation."
  },
  "fal-ai/playai/tts/dialog": {
    "label": "PlayAI Text-to-Speech Dialog",
    "category": "Voiceover",
    "headline": "Multi-speaker dialogues for interactive media",
    "bestFor": ["Character dialogues", "Interview simulations", "Storytelling"],
    "tips": [
      "Use speaker prefixes consistently",
      "Include natural conversational elements",
      "Vary sentence length and structure for realism",
      "Include emotional cues in parentheses for better expression"
    ],
    "pricePerCreditUSD": 0.05,
    "secondsPerDollar": 20,
    "examplePrompt": "Speaker 1: Have you heard about the new AI technologies?\nSpeaker 2: Yes, they're fascinating! I've been reading about them extensively.\nSpeaker 1: What interests you the most about them?\nSpeaker 2: The way they can create content that seems so human-like."
  },
  "fal-ai/f5-tts": {
    "label": "F5 TTS",
    "category": "Voiceover",
    "headline": "Voice cloning with reference audio matching",
    "bestFor": [
      "Voice cloning",
      "Consistent voice styling",
      "Voice preservation"
    ],
    "tips": [
      "Provide clear reference audio for better voice matching",
      "Ensure reference text matches the audio for accurate voice learning",
      "Keep generated text in a similar style to the reference for best results"
    ],
    "pricePerCreditUSD": 0.08,
    "secondsPerDollar": 12.5,
    "examplePrompt": "This is a demonstration of voice cloning technology. The system can match the tone, accent, and speaking style of the reference audio to create a natural-sounding voice that maintains consistency."
  }
}

================
File: src/app/api/download/route.ts
================
import { NextRequest } from "next/server";

export const GET = async (req: NextRequest) => {
  const url = req.nextUrl.searchParams.get("url");
  if (!url) {
    return new Response("Missing 'url' query parameter", {
      status: 400,
    });
  }
  try {
    const parsedUrl = new URL(url);
    return fetch(parsedUrl.toString());
  } catch (error) {
    return new Response("Invalid 'url' query parameter", {
      status: 400,
    });
  }
};

================
File: src/app/api/fal/route.ts
================
import { NextResponse, NextRequest } from "next/server";

// Function to forward request to Fal.ai
async function forwardToFal(req: NextRequest) {
  console.log("🔍 API ROUTE: Request received at /api/fal");
  console.log("🟢 /api/fal route hit");

  try {
    // Parse the URL
    const url = new URL(req.url);
    console.log(`🔍 Request: ${req.method} ${url.pathname}${url.search}`);

    // CHECKPOINT B: Incoming headers
    const incomingHeadersObj: Record<string, string> = {};
    req.headers.forEach((value, key) => {
      incomingHeadersObj[key] = value;
    });
    console.log(
      "🔍 Incoming headers:",
      JSON.stringify(incomingHeadersObj, null, 2),
    );

    // IMPORTANT: The path should be extracted differently based on the official docs
    // For the fal.ai client, we need to:
    // 1. Get the target URL from the x-fal-target-url header if it exists
    // 2. Otherwise, extract it from the path segments

    // First check if the client set the target URL in headers (this is used by the official client)
    let targetUrl = req.headers.get("x-fal-target-url");

    if (!targetUrl) {
      // If not, extract from path - this is a fallback for direct calls
      // Extract everything after /api/fal/ from the path
      const pathMatch = url.pathname.match(/\/api\/fal\/(.*)/);
      if (pathMatch && pathMatch[1]) {
        targetUrl = `https://rest.fal.ai/${pathMatch[1]}`;
      } else {
        console.error("❌ No target URL found in headers or path");
        return new NextResponse("Bad Request: No target URL specified", {
          status: 400,
        });
      }
    }

    console.log(`🔍 Target URL: ${targetUrl}`);
    // CHECKPOINT C: Target URL
    console.log("🎥 Target URL:", targetUrl);

    // Get request body if it exists
    let requestBody: any = null;
    let contentType = req.headers.get("content-type") || "";

    if (req.method !== "GET" && req.method !== "HEAD") {
      try {
        if (contentType.includes("application/json")) {
          requestBody = await req.json();
          console.log(
            "🔍 Request body:",
            JSON.stringify(requestBody).substring(0, 200) + "...",
          );
          // CHECKPOINT C: Request body
          console.log("📝 Request body:", JSON.stringify(requestBody, null, 2));
        } else {
          // Handle other content types if needed
          requestBody = await req.text();
          console.log(
            "🔍 Request body (non-JSON):",
            (requestBody as string).substring(0, 200) + "...",
          );
        }
      } catch (err) {
        console.log("⚠️ Could not parse request body:", err);
      }
    }

    // Get auth header from request or from the client app
    let authHeader = req.headers.get("authorization");

    // Check for API key in request headers first (client-side)
    if (!authHeader || !authHeader.startsWith("Key ")) {
      console.log("⚠️ No Authorization header found in request");

      // Fall back to environment variable if available (server-side)
      const envApiKey = process.env.FAL_KEY;

      if (envApiKey) {
        console.log("🔍 Using API key from environment variable");
        authHeader = `Key ${envApiKey}`;
      } else {
        console.error(
          "❌ No API key available (neither in headers nor environment)",
        );
        return new NextResponse("Unauthorized: Missing API key", {
          status: 401,
        });
      }
    } else {
      console.log("🔍 Using API key from request headers");
    }

    // Clone the headers to avoid modifying the original request
    const headers = new Headers();

    // Copy all headers from the original request
    req.headers.forEach((value, key) => {
      // Skip the host header as it would be invalid for the forwarded request
      if (key.toLowerCase() !== "host") {
        headers.set(key, value);
      }
    });

    // Set the content type and authorization headers
    headers.set("Content-Type", contentType);
    headers.set("Authorization", authHeader);

    console.log(`🔍 Forwarding to: ${targetUrl}`);

    // Log detailed request information for debugging
    // Convert headers to a plain object for logging
    const headerObj: Record<string, string> = {};
    headers.forEach((value, key) => {
      headerObj[key] = value;
    });
    console.log(`🔍 Headers: ${JSON.stringify(headerObj).substring(0, 500)}`);
    console.log(
      `🔍 Body: ${requestBody ? JSON.stringify(requestBody).substring(0, 200) + "..." : "undefined"}`,
    );

    try {
      // Forward the request - hardcode POST to match curl behavior
      // Create a plain object for headers (Test A5 style)
      const plainHeaders: Record<string, string> = {
        Authorization: authHeader,
        "Content-Type": "application/json",
        "x-fal-target-url": targetUrl,
      };

      // Copy any other important headers from the original request
      headers.forEach((value, key) => {
        const lowerKey = key.toLowerCase();
        // Skip headers we set manually AND skip the 'connection' header
        if (
          ![
            "authorization",
            "content-type",
            "x-fal-target-url",
            "connection",
          ].includes(lowerKey)
        ) {
          plainHeaders[key] = value;
        }
      });

      // CHECKPOINT D: Outgoing headers to Fal
      console.log(
        "📤 Outgoing headers to Fal.ai:",
        JSON.stringify(plainHeaders, null, 2),
      );

      // CHECKPOINT E: Fetch success/failure
      let forwardedResponse;
      try {
        forwardedResponse = await fetch(targetUrl, {
          // Use the original request's method (GET, POST, etc.)
          method: req.method,
          headers: plainHeaders,
          // Only include body for methods that typically have one
          body:
            req.method !== "GET" && req.method !== "HEAD"
              ? JSON.stringify(requestBody)
              : undefined,
        });

        // Log response status
        console.log(`🔍 Fal.ai response status: ${forwardedResponse.status}`);
        console.log("✅ Fetch status:", forwardedResponse.status);
      } catch (error) {
        console.error(
          "❌ FETCH FAILED:",
          error instanceof Error ? error.message : String(error),
        );
        throw error; // Re-throw to be caught by the outer try/catch
      }

      // Get response data
      try {
        // Check content type to determine how to handle the response
        const contentType = forwardedResponse.headers.get("Content-Type") || "";

        if (contentType.includes("application/json")) {
          // For JSON responses, parse the JSON and use NextResponse.json()
          const jsonData = await forwardedResponse.json();
          console.log(
            `🔍 Response body (JSON): ${JSON.stringify(jsonData).substring(0, 200)}...`,
          );
          console.log(
            "📦 Response body (JSON):",
            JSON.stringify(jsonData).substring(0, 500),
          );

          // Return properly formatted JSON response
          const response = NextResponse.json(jsonData, {
            status: forwardedResponse.status,
            statusText: forwardedResponse.statusText,
            // NextResponse.json() handles Content-Type: application/json automatically
          });

          if (forwardedResponse.ok) {
            console.log("✅ Successfully proxied JSON request to Fal.ai");
          } else {
            console.error(
              `❌ Fal.ai returned error status: ${forwardedResponse.status}`,
            );
          }

          return response;
        } else {
          // For non-JSON responses, use text() and preserve the original Content-Type
          const responseData = await forwardedResponse.text();
          console.log(
            `🔍 Response body (text): ${responseData.substring(0, 200)}...`,
          );
          console.log(
            "📦 Response body (text):",
            responseData.substring(0, 500),
          );

          // Create response with same status and headers
          const responseHeaders = new Headers();
          forwardedResponse.headers.forEach((value, key) => {
            responseHeaders.set(key, value);
          });

          const response = new NextResponse(responseData, {
            status: forwardedResponse.status,
            statusText: forwardedResponse.statusText,
            headers: responseHeaders,
          });

          if (forwardedResponse.ok) {
            console.log("✅ Successfully proxied text request to Fal.ai");
          } else {
            console.error(
              `❌ Fal.ai returned error status: ${forwardedResponse.status}`,
            );
          }

          return response;
        }
      } catch (error) {
        console.error("❌ Error reading response body:", error);
        const errorMessage =
          error instanceof Error ? error.message : "Failed to read response";
        return new NextResponse(JSON.stringify({ error: errorMessage }), {
          status: 500,
          headers: { "Content-Type": "application/json" },
        });
      }
    } catch (error) {
      console.error("❌ Error in Fal.ai fetch:", error);
      // Handle error safely with type checking
      const errorMessage =
        error instanceof Error ? error.message : String(error);
      console.error("❌ Error details:", errorMessage);
      return new NextResponse(JSON.stringify({ error: errorMessage }), {
        status: 500,
        headers: { "Content-Type": "application/json" },
      });
    }
  } catch (error) {
    console.error("❌ Error in Fal.ai proxy:", error);
    const errorMessage = error instanceof Error ? error.message : String(error);
    return new NextResponse(JSON.stringify({ error: errorMessage }), {
      status: 500,
      headers: { "Content-Type": "application/json" },
    });
  }
}

// Export the HTTP methods
export async function GET(req: NextRequest) {
  return forwardToFal(req);
}

export async function POST(req: NextRequest) {
  return forwardToFal(req);
}

export async function PUT(req: NextRequest) {
  return forwardToFal(req);
}

================
File: src/app/api/share/route.ts
================
import { IS_SHARE_ENABLED, shareVideo, ShareVideoParams } from "@/lib/share";
import { NextRequest, NextResponse } from "next/server";

export const POST = async (req: NextRequest) => {
  if (!IS_SHARE_ENABLED) {
    return NextResponse.json({ error: "Sharing is disabled" }, { status: 503 });
  }
  const payload: ShareVideoParams = await req.json();
  const id = await shareVideo({
    ...payload,
    createdAt: Date.now(),
  });
  return NextResponse.json({
    id,
    params: payload,
  });
};

================
File: src/app/api/uploadthing/core.ts
================
import { createUploadthing, type FileRouter } from "uploadthing/next";
import { UploadThingError } from "uploadthing/server";

const f = createUploadthing();

const auth = (req: Request) => ({ id: "fakeId" }); // Fake auth function

// FileRouter for your app, can contain multiple FileRoutes
export const ourFileRouter = {
  // Define as many FileRoutes as you like, each with a unique routeSlug
  fileUploader: f({
    /**
     * For full list of options and defaults, see the File Route API reference
     * @see https://docs.uploadthing.com/file-routes#route-config
     */
    image: {
      maxFileCount: 1,
      maxFileSize: "16MB",
    },
    video: {
      maxFileCount: 1,
      maxFileSize: "512MB",
    },
    audio: {
      maxFileCount: 1,
      maxFileSize: "64MB",
    },
  })
    // Set permissions and file types for this FileRoute
    .middleware(async ({ req }) => {
      // This code runs on your server before upload
      const user = await auth(req);

      // If you throw, the user will not be able to upload
      if (!user) throw new UploadThingError("Unauthorized");

      // Whatever is returned here is accessible in onUploadComplete as `metadata`
      return { userId: user.id };
    })
    .onUploadComplete(async ({ metadata, file }) => {
      // This code RUNS ON YOUR SERVER after upload
      console.log("Upload complete for userId:", metadata.userId);

      console.log("file url", file.url);

      // !!! Whatever is returned here is sent to the clientside `onClientUploadComplete` callback
      return { uploadedBy: metadata.userId };
    }),
} satisfies FileRouter;

export type OurFileRouter = typeof ourFileRouter;

================
File: src/app/api/uploadthing/route.ts
================
import { createRouteHandler } from "uploadthing/next";

import { ourFileRouter } from "./core";

export const { GET, POST } = createRouteHandler({
  router: ourFileRouter,
});

================
File: src/app/app/layout.tsx
================
import type { Metadata } from "next";
import "../globals.css";

export const metadata: Metadata = {
  title: "AI Video Developer Starter Kit | fal.ai",
  description: "Open-source AI video editor built for developers.",
};

export default function AppLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return <div className="app-container">{children}</div>;
}

================
File: src/app/app/page.tsx
================
import { App } from "@/components/main";
import { PROJECT_PLACEHOLDER } from "@/data/schema";
import { cookies } from "next/headers";

export default function IndexPage() {
  const cookieStore = cookies();
  const lastProjectId = cookieStore.get("__aivs_lastProjectId");
  return <App projectId={lastProjectId?.value ?? PROJECT_PLACEHOLDER.id} />;
}

================
File: src/app/docs/page.tsx
================
"use client";

import { useEffect, useState } from "react";
import Header from "@/components/header";
import Footer from "@/components/footer";
import {
  FilmIcon,
  ImageIcon,
  MusicIcon,
  MicIcon,
  LightbulbIcon,
  AlertTriangleIcon,
  ChevronDownIcon,
  ChevronUpIcon,
  DollarSignIcon,
  BookOpenIcon,
} from "lucide-react";

interface ModelCardProps {
  type: string;
  color: string;
  name: string;
  endpointId: string;
  description: string;
  cost: string;
  bestFor: string[];
  promptTips: string[];
  features: string[];
  inputAsset?: (string | { type: string; key: string })[];
  imageForFrame?: boolean;
  cameraControl?: boolean;
}

// Model card component for better reusability
function ModelCard({
  type,
  color,
  name,
  endpointId,
  description,
  cost,
  bestFor,
  promptTips,
  features,
  inputAsset,
  imageForFrame,
  cameraControl,
}: ModelCardProps) {
  const [expandedSection, setExpandedSection] = useState<string | null>(null);

  const toggleSection = (section: string) => {
    if (expandedSection === section) {
      setExpandedSection(null);
    } else {
      setExpandedSection(section);
    }
  };

  // Convert bestFor to array if it's a string
  const bestForArray = typeof bestFor === "string" ? [bestFor] : bestFor;

  return (
    <div className="bg-gray-800 rounded-xl p-6 border border-gray-700">
      <div
        className={`inline-block px-3 py-1 rounded-full bg-${color}-500/10 text-${color}-400 text-sm font-medium mb-4`}
      >
        {type}
      </div>
      <h3 className="font-bold text-lg mb-2">{name}</h3>
      <div className="text-xs text-gray-400 mb-4">{endpointId}</div>

      {/* Basic Info */}
      <div className="space-y-3 text-sm text-gray-300">
        <p className="text-gray-300">{description}</p>
        <div className="flex justify-between">
          <span className="text-gray-400">Cost:</span>
          <span>{cost}</span>
        </div>
        {inputAsset && inputAsset.length > 0 && (
          <div className="flex justify-between">
            <span className="text-gray-400">Accepts:</span>
            <span>{inputAsset.join(", ")}</span>
          </div>
        )}
      </div>

      {/* Best Use Cases Expandable Section */}
      {bestForArray && bestForArray.length > 0 && (
        <div className="mt-4 border-t border-gray-700 pt-4">
          <button
            onClick={() => toggleSection("bestFor")}
            className="flex items-center justify-between w-full text-left"
          >
            <div className="flex items-center">
              <AlertTriangleIcon className="w-4 h-4 mr-2 text-green-400" />
              <span className="font-medium">Best Use Cases</span>
            </div>
            {expandedSection === "bestFor" ? (
              <ChevronUpIcon className="w-4 h-4" />
            ) : (
              <ChevronDownIcon className="w-4 h-4" />
            )}
          </button>

          {expandedSection === "bestFor" && (
            <div className="mt-3 pl-6">
              <ul className="space-y-2">
                {bestForArray.map((use, index) => (
                  <li
                    key={index}
                    className="flex items-start text-sm text-gray-300"
                  >
                    <span className="text-green-400 mr-2">•</span>
                    <span>{use}</span>
                  </li>
                ))}
              </ul>
            </div>
          )}
        </div>
      )}

      {/* Prompt Techniques Section */}
      {promptTips && promptTips.length > 0 && (
        <div className="mt-4 border-t border-gray-700 pt-4">
          <button
            onClick={() => toggleSection("promptTips")}
            className="flex items-center justify-between w-full text-left"
          >
            <div className="flex items-center">
              <LightbulbIcon className="w-4 h-4 mr-2 text-yellow-400" />
              <span className="font-medium">Prompt Techniques</span>
            </div>
            {expandedSection === "promptTips" ? (
              <ChevronUpIcon className="w-4 h-4" />
            ) : (
              <ChevronDownIcon className="w-4 h-4" />
            )}
          </button>

          {expandedSection === "promptTips" && (
            <div className="mt-3 pl-6">
              <ul className="space-y-2">
                {promptTips.map((tip, index) => (
                  <li
                    key={index}
                    className="flex items-start text-sm text-gray-300"
                  >
                    <span className="text-yellow-400 mr-2">•</span>
                    <span>{tip}</span>
                  </li>
                ))}
              </ul>
            </div>
          )}
        </div>
      )}

      {/* Features Expandable Section */}
      {features && features.length > 0 && (
        <div className="mt-4 border-t border-gray-700 pt-4">
          <button
            onClick={() => toggleSection("features")}
            className="flex items-center justify-between w-full text-left"
          >
            <div className="flex items-center">
              <DollarSignIcon className="w-4 h-4 mr-2 text-purple-400" />
              <span className="font-medium">Features</span>
            </div>
            {expandedSection === "features" ? (
              <ChevronUpIcon className="w-4 h-4" />
            ) : (
              <ChevronDownIcon className="w-4 h-4" />
            )}
          </button>

          {expandedSection === "features" && (
            <div className="mt-3 pl-6">
              <ul className="space-y-2">
                {features.map((feature, index) => (
                  <li
                    key={index}
                    className="flex items-start text-sm text-gray-300"
                  >
                    <span className="text-purple-400 mr-2">•</span>
                    <span>{feature}</span>
                  </li>
                ))}
              </ul>
            </div>
          )}
        </div>
      )}
    </div>
  );
}

export default function DocsPage() {
  return (
    <div className="flex flex-col min-h-screen bg-gray-900">
      <Header />
      <main className="flex-grow p-6 text-white">
        <div className="container mx-auto max-w-6xl">
          <h1 className="text-3xl font-bold mb-8 text-center">
            Video Starter Kit - Model Guide
          </h1>

          {/* Navigation Cards */}
          <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-4 mb-10">
            <a
              href="#video-section"
              className="bg-gray-800 rounded-xl p-6 border border-gray-700 hover:border-blue-500 transition-all"
            >
              <FilmIcon className="w-8 h-8 mb-4 text-blue-400" />
              <h3 className="font-bold text-xl mb-2 text-blue-400">
                Video Models
              </h3>
              <p className="text-gray-300 text-sm">
                Professional video generation
              </p>
            </a>

            <a
              href="#image-section"
              className="bg-gray-800 rounded-xl p-6 border border-gray-700 hover:border-purple-500 transition-all"
            >
              <ImageIcon className="w-8 h-8 mb-4 text-purple-400" />
              <h3 className="font-bold text-xl mb-2 text-purple-400">
                Image Models
              </h3>
              <p className="text-gray-300 text-sm">
                High-quality image generation
              </p>
            </a>

            <a
              href="#audio-section"
              className="bg-gray-800 rounded-xl p-6 border border-gray-700 hover:border-green-500 transition-all"
            >
              <MusicIcon className="w-8 h-8 mb-4 text-green-400" />
              <h3 className="font-bold text-xl mb-2 text-green-400">
                Audio Models
              </h3>
              <p className="text-gray-300 text-sm">Music and sound effects</p>
            </a>

            <a
              href="#voice-section"
              className="bg-gray-800 rounded-xl p-6 border border-gray-700 hover:border-yellow-500 transition-all"
            >
              <MicIcon className="w-8 h-8 mb-4 text-yellow-400" />
              <h3 className="font-bold text-xl mb-2 text-yellow-400">
                Voice Models
              </h3>
              <p className="text-gray-300 text-sm">
                Voiceover and dialogue generation
              </p>
            </a>
          </div>

          {/* Video Models Section */}
          <section id="video-section" className="mb-16">
            <div className="flex items-center mb-8">
              <FilmIcon className="w-8 h-8 mr-4 text-blue-400" />
              <h2 className="text-2xl font-bold">Video Models</h2>
            </div>

            <div className="grid grid-cols-1 lg:grid-cols-3 gap-6 mb-6">
              <ModelCard
                type="Professional"
                color="blue"
                name="Veo 2"
                endpointId="fal-ai/veo2"
                description="Google's cutting-edge video generation model, offering unprecedented quality, physics understanding, and cinematic excellence."
                cost="$1.25/5s ($0.25/additional second)"
                bestFor={[
                  "High-quality cinematic videos",
                  "Complex physical interactions",
                  "Extended durations (minutes)",
                  "High resolution (up to 4K)",
                ]}
                promptTips={[
                  "Write prompts like a movie script with detailed scene descriptions",
                  "Include camera movements and lens choices",
                  "Specify lighting details and atmosphere",
                  "Structure narrative with clear beginning, middle, end",
                ]}
                features={[
                  "Realistic motion physics",
                  "Professional cinematography",
                  "Reduced artifacts and flickering",
                  "Production-ready output quality",
                ]}
              />

              <ModelCard
                type="Fast Generation"
                color="green"
                name="LTX Video v0.95"
                endpointId="fal-ai/ltx-video-v095/multiconditioning"
                description="Generate videos from prompts or images using LTX Video-0.9.5 with multi-conditioning capabilities."
                cost="~$0.04-0.08/second"
                bestFor={[
                  "Rapid video prototyping",
                  "Image-to-video conversion",
                  "Quick concept testing",
                  "Budget-friendly video generation",
                ]}
                promptTips={[
                  "Use clear, concise descriptions",
                  "For image input, provide high-quality references",
                  "Specify motion direction explicitly",
                  "Describe simple, focused actions or scenes",
                ]}
                features={[
                  "Multi-conditional inputs",
                  "Fast generation times",
                  "Image guidance capability",
                  "Efficient for rapid iterations",
                ]}
                inputAsset={["image"]}
              />

              <ModelCard
                type="Physics Simulation"
                color="purple"
                name="Minimax Video-01-Live"
                endpointId="fal-ai/minimax/video-01-live"
                description="Specialized in turning static inputs into fluid animations with exceptional temporal consistency."
                cost="~$0.40/video"
                bestFor={[
                  "Bringing static images to life",
                  "Dynamic portrait animations",
                  "Face and character consistency",
                  "Both stylized and realistic content",
                ]}
                promptTips={[
                  "Describe specific actions and movements",
                  "Use image-to-video mode for best results",
                  "Include camera directions (pan, zoom)",
                  "Keep prompts in present tense like a film scene",
                ]}
                features={[
                  "Exceptional temporal consistency",
                  "Strong face and character handling",
                  "Smooth camera movements",
                  "Live2D-style animation capability",
                ]}
                inputAsset={["image"]}
              />
            </div>

            <div className="grid grid-cols-1 lg:grid-cols-3 gap-6 mb-6">
              <ModelCard
                type="High Quality"
                color="blue"
                name="Hunyuan Video"
                endpointId="fal-ai/hunyuan-video"
                description="Large-scale (13B parameter) video foundation model by Tencent focusing on cinematic quality and physical realism."
                cost="$0.40/video"
                bestFor={[
                  "Cinematic video quality",
                  "Realistic physical interactions",
                  "Coherent scene transitions",
                  "Movie-like footage generation",
                ]}
                promptTips={[
                  "Write detailed scene descriptions like movie snippets",
                  "Keep prompts logically consistent",
                  "Use temporal words (slowly, suddenly) to indicate timing",
                  "Specify camera and lighting details for cinematic results",
                ]}
                features={[
                  "Real-world physics simulation",
                  "Highly optimized (generates in under a minute)",
                  "High-fidelity HD output",
                  "Continuous action sequences",
                ]}
              />

              <ModelCard
                type="Professional"
                color="purple"
                name="Kling 1.5 Pro"
                endpointId="fal-ai/kling-video/v1.5/pro"
                description="Advanced video generator known for HD output (1080p) and enhanced physics simulation."
                cost="$0.10/second (~$0.50/5s video)"
                bestFor={[
                  "Professional-quality video",
                  "Complex physics interactions",
                  "High-resolution (1080p) output",
                  "Realistic physical simulations",
                ]}
                promptTips={[
                  "Start with an image when possible",
                  "Describe both scene and movement clearly",
                  "Emphasize physical interactions for best results",
                  "Use present tense and descriptive language",
                ]}
                features={[
                  "1080p HD output capability",
                  "Realistic physics engine",
                  "Support for image conditioning",
                  "Extended duration capabilities",
                ]}
                inputAsset={["image"]}
              />

              <ModelCard
                type="Standard"
                color="green"
                name="Kling 1.0 Standard"
                endpointId="fal-ai/kling-video/v1/standard/text-to-video"
                description="Earlier Kling model offering more accessible, faster generation at lower resolution."
                cost="~$0.05/second (Variable)"
                bestFor={[
                  "Quick video prototyping",
                  "Simpler scenes and actions",
                  "Faster turnaround times",
                  "Budget-conscious projects",
                ]}
                promptTips={[
                  "Use simpler, more concise prompts",
                  "Focus on single, clear actions or scenes",
                  "Specify style (cartoon, realistic) explicitly",
                  "Keep chronology short (one main scene)",
                ]}
                features={[
                  "Faster generation time",
                  "Lower resource requirements",
                  "Good for storyboarding",
                  "Camera movement controls",
                ]}
                cameraControl={true}
              />
            </div>

            <div className="grid grid-cols-1 lg:grid-cols-3 gap-6">
              <ModelCard
                type="Dream Machine"
                color="blue"
                name="Luma Dream Machine"
                endpointId="fal-ai/luma-dream-machine"
                description="Video generation model known for immersive scene generation and consistent character interactions."
                cost="$0.50/video"
                bestFor={[
                  "Creative storytelling scenes",
                  "Dreamlike visual sequences",
                  "Multiple subjects interacting",
                  "Cinematic camera movements",
                ]}
                promptTips={[
                  "Include imaginative but visually concrete elements",
                  "Always specify some form of motion or change",
                  "Request specific camera movements for best results",
                  "Structure prompts to indicate sequence of events",
                ]}
                features={[
                  "Handles both realistic and stylized content",
                  "Relatively fast generation",
                  "Maintains physical accuracy",
                  "Supports both text and image inputs",
                ]}
                inputAsset={["image"]}
              />

              <ModelCard
                type="Audio-Video"
                color="purple"
                name="MMAudio V2"
                endpointId="fal-ai/mmaudio-v2"
                description="Audio generation model that creates synchronized soundtracks for videos from visual content and text prompts."
                cost="~$0.02-0.03/second of audio"
                bestFor={[
                  "Adding audio to silent videos",
                  "Synchronized sound effects",
                  "Background music generation",
                  "Complete audiovisual experiences",
                ]}
                promptTips={[
                  "Specify genre, mood, or sound types you want",
                  "Match audio description to video content",
                  "Be specific about musical style or ambient sounds",
                  "Include timing cues if needed",
                ]}
                features={[
                  "Multimodal inputs (video + text)",
                  "Synchronized audio generation",
                  "Capable of both music and sound effects",
                  "Returns complete audio-augmented video",
                ]}
                inputAsset={["video"]}
              />

              <ModelCard
                type="Animation"
                color="green"
                name="Sync.so LipSync"
                endpointId="fal-ai/sync-lipsync"
                description="Specialized model that animates a face video or image to lip-sync with speech audio."
                cost="$0.05/minute"
                bestFor={[
                  "Creating talking avatars",
                  "Dubbing videos in new languages",
                  "Virtual presenters",
                  "Giving voice to still images",
                ]}
                promptTips={[
                  "Provide clear face image/video with visible mouth",
                  "Use high-quality clean audio",
                  "Choose matching emotional expressions",
                  "Trim silence from start of audio if needed",
                ]}
                features={[
                  "High-quality synchronized lip movements",
                  "Works with images or videos as input",
                  "Fast processing",
                  "Supports multiple languages",
                ]}
                inputAsset={["video", "audio"]}
              />
            </div>
          </section>

          {/* Image Models Section */}
          <section id="image-section" className="mb-16">
            <div className="flex items-center mb-8">
              <ImageIcon className="w-8 h-8 mr-4 text-purple-400" />
              <h2 className="text-2xl font-bold">Image Models</h2>
            </div>

            <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6">
              <ModelCard
                type="Professional"
                color="purple"
                name="Flux Pro 1.1 Ultra"
                endpointId="fal-ai/flux-pro/v1.1-ultra"
                description="Latest professional-grade FLUX model with 2K output capability and improved photorealism."
                cost="$0.10-0.30/image"
                bestFor={[
                  "High-resolution realistic images",
                  "Marketing and commercial work",
                  "Highly detailed scenes",
                  "Professional-grade visuals",
                ]}
                promptTips={[
                  "Include photography terminology (camera, lens, lighting)",
                  "Structure prompts clearly (subject, setting, style)",
                  "Be specific and provide vivid details",
                  "Consider reference images for style guidance",
                ]}
                features={[
                  "Up to 2K resolution output",
                  "Enhanced photorealism",
                  "Better text handling",
                  "High-fidelity detail rendering",
                ]}
              />

              <ModelCard
                type="Quick"
                color="blue"
                name="Flux Dev"
                endpointId="fal-ai/flux/dev"
                description="12 billion parameter flow transformer for high-quality image generation balancing quality and speed."
                cost="$0.05-0.15/image"
                bestFor={[
                  "Balanced quality and speed",
                  "Detailed artistic creations",
                  "Complex scene composition",
                  "Long-form descriptive prompts",
                ]}
                promptTips={[
                  "Use long-form prompts with rich detail",
                  "Employ weighted prompt segments with :: notation",
                  "Structure prompts into subject, style, background",
                  "20-40 inference steps for best quality",
                ]}
                features={[
                  "High resolution capabilities",
                  "Strong prompt understanding",
                  "Supports personal and commercial use",
                  "Handles complex, lengthy prompts",
                ]}
              />

              <ModelCard
                type="Fast"
                color="green"
                name="Flux Schnell"
                endpointId="fal-ai/flux/schnell"
                description="Speed-optimized FLUX variant generating high-quality images in just 1-4 diffusion steps."
                cost="$0.03-0.10/image"
                bestFor={[
                  "Ultra-fast generation",
                  "Rapid prototype iteration",
                  "High-volume image needs",
                  "Quick concept exploration",
                ]}
                promptTips={[
                  "Use same syntax as FLUX [dev]",
                  "Keep prompts focused and concise",
                  "Emphasize key elements with higher weights",
                  "Structure into segments (subject | style | etc.)",
                ]}
                features={[
                  "1-4 diffusion steps (fastest FLUX)",
                  "Output quality comparable to larger models",
                  "Perfect for testing prompt ideas quickly",
                  "Extremely cost-efficient",
                ]}
              />

              <ModelCard
                type="Typography"
                color="purple"
                name="SD 3.5 Large"
                endpointId="fal-ai/stable-diffusion-v35-large"
                description="Stability AI's latest diffusion model with improved image quality, typography, and prompt understanding."
                cost="$0.0006-0.0012/second of GPU time"
                bestFor={[
                  "Complex prompt handling",
                  "Text and typography needs",
                  "Multimodal generation (text+image)",
                  "High-quality detailed outputs",
                ]}
                promptTips={[
                  "Write rich, descriptive prompts",
                  "Organize by subject, setting, style, technical details",
                  "Use negative prompting to avoid issues",
                  "Include artist/style references for specific looks",
                ]}
                features={[
                  "Multimodal Diffusion Transformer architecture",
                  "Better text rendering in images",
                  "ControlNet and LoRA support",
                  "Resource efficiency improvements",
                ]}
              />
            </div>
          </section>

          {/* Audio Models Section */}
          <section id="audio-section" className="mb-16">
            <div className="flex items-center mb-8">
              <MusicIcon className="w-8 h-8 mr-4 text-green-400" />
              <h2 className="text-2xl font-bold">Audio Models</h2>
            </div>

            <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
              <ModelCard
                type="Music"
                color="green"
                name="Minimax Music"
                endpointId="fal-ai/minimax-music"
                description="Generative music model creating high-quality, diverse musical compositions from text prompts."
                cost="~$0.30-0.50/audio clip"
                bestFor={[
                  "Original music composition",
                  "Custom soundtracks",
                  "Genre-specific music creation",
                  "Mood-based audio generation",
                ]}
                promptTips={[
                  "Specify genre, mood, tempo, and instruments",
                  "Include BPM for rhythmic control",
                  "Mention era or artist references for style",
                  "Describe structure (intro, build, climax)",
                ]}
                features={[
                  "Multi-genre support",
                  "Coherent musical structure",
                  "High-quality output",
                  "Reference audio capability",
                ]}
                inputAsset={[
                  {
                    type: "audio",
                    key: "reference_audio_url",
                  },
                ]}
              />

              <ModelCard
                type="Audio"
                color="yellow"
                name="Stable Audio"
                endpointId="fal-ai/stable-audio"
                description="Stability AI's open-source text-to-audio model for music, sound effects, or ambience."
                cost="$0.0006-0.0012/second of GPU time"
                bestFor={[
                  "Music loops and samples",
                  "Sound effects generation",
                  "Ambient soundscapes",
                  "Background music",
                ]}
                promptTips={[
                  "Be concise with sound-focused descriptions",
                  "Specify tempo (BPM) for rhythmic content",
                  "Include genre and instrument details",
                  "Mention duration or 'loopable' if needed",
                ]}
                features={[
                  "Timing-conditioned latent diffusion",
                  "Efficient resource usage",
                  "Open for commercial use",
                  "Good tempo/beat adherence",
                ]}
              />
            </div>
          </section>

          {/* Voice Models Section */}
          <section id="voice-section" className="mb-16">
            <div className="flex items-center mb-8">
              <MicIcon className="w-8 h-8 mr-4 text-yellow-400" />
              <h2 className="text-2xl font-bold">Voice Models</h2>
            </div>

            <div className="grid grid-cols-1 md:grid-cols-3 gap-6">
              <ModelCard
                type="Natural Speech"
                color="yellow"
                name="PlayHT TTS v3"
                endpointId="fal-ai/playht/tts/v3"
                description="State-of-the-art TTS model with natural and expressive voices, multilingual support, and emotional rendering."
                cost="$0.03/minute"
                bestFor={[
                  "High-quality voice narration",
                  "Emotional and expressive speech",
                  "Long-form content (audiobooks)",
                  "Multilingual content",
                ]}
                promptTips={[
                  "Use well-punctuated text for natural rhythm",
                  "Add emotive cues and punctuation for tone",
                  "Choose appropriate voice for content",
                  "Split long content into manageable chunks",
                ]}
                features={[
                  "Blazing-fast generation",
                  "Natural prosody and intonation",
                  "Support for multiple languages",
                  "Expressive emotional rendering",
                ]}
              />

              <ModelCard
                type="Dialogues"
                color="purple"
                name="PlayAI Dialog"
                endpointId="fal-ai/playai/tts/dialog"
                description="Multi-speaker dialogue generator creating natural-sounding conversational audio from formatted scripts."
                cost="$0.05/minute"
                bestFor={[
                  "Character dialogues",
                  "Storytelling with multiple voices",
                  "Games and interactive media",
                  "Animation voice-overs",
                ]}
                promptTips={[
                  "Format as 'Speaker name: dialogue' on each line",
                  "Use punctuation to guide natural speech patterns",
                  "Include emotive cues for better expression",
                  "Use consistent speaker labels throughout",
                ]}
                features={[
                  "Multiple distinct voice models",
                  "Natural back-and-forth timing",
                  "Expressive dialogue capability",
                  "Simple formatting requirements",
                ]}
              />

              <ModelCard
                type="Voice Clone"
                color="blue"
                name="F5 TTS"
                endpointId="fal-ai/f5-tts"
                description="Voice replication model that creates speech matching a reference voice sample."
                cost="$0.0006-0.0012/second of GPU time"
                bestFor={[
                  "Voice cloning applications",
                  "Custom voice creation",
                  "Matching specific voice characteristics",
                  "Personalized voice content",
                ]}
                promptTips={[
                  "Provide high-quality reference audio",
                  "Include matching reference text",
                  "Keep reference audio clean and noise-free",
                  "Use natural speech patterns in input text",
                ]}
                features={[
                  "Voice replication technology",
                  "Reference-based generation",
                  "Flow matching for natural speech",
                  "Seamless voice matching",
                ]}
              />
            </div>
          </section>

          {/* Quick Reference */}
          <section className="mb-16">
            <div className="bg-gradient-to-r from-gray-900 to-gray-800 rounded-xl p-8 border border-gray-700">
              <h2 className="text-2xl font-bold mb-6">Quick Reference</h2>

              <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6">
                <div>
                  <h3 className="font-bold mb-3 text-blue-400">Video Costs</h3>
                  <ul className="space-y-2 text-sm text-gray-300">
                    <li>• Veo 2: $1.25/5s, +$0.25/s after</li>
                    <li>• LTX: ~$0.04-0.08/s</li>
                    <li>• Minimax: ~$0.40/video</li>
                    <li>• Hunyuan: $0.40/video</li>
                    <li>• Kling Pro: $0.10/s (~$0.50/5s)</li>
                    <li>• Kling Standard: ~$0.05/s</li>
                    <li>• Luma: $0.50/video</li>
                  </ul>
                </div>
                <div>
                  <h3 className="font-bold mb-3 text-purple-400">
                    Image Costs
                  </h3>
                  <ul className="space-y-2 text-sm text-gray-300">
                    <li>• Flux Pro Ultra: $0.10-0.30/image</li>
                    <li>• Flux Dev: $0.05-0.15/image</li>
                    <li>• Flux Schnell: $0.03-0.10/image</li>
                    <li>• SD 3.5: $0.0006-0.0012/s of GPU time</li>
                  </ul>
                </div>
                <div>
                  <h3 className="font-bold mb-3 text-green-400">Audio Costs</h3>
                  <ul className="space-y-2 text-sm text-gray-300">
                    <li>• MMAudio V2: ~$0.02-0.03/s</li>
                    <li>• Minimax Music: ~$0.30-0.50/clip</li>
                    <li>• Stable Audio: $0.0006-0.0012/s of GPU time</li>
                  </ul>
                </div>
                <div>
                  <h3 className="font-bold mb-3 text-yellow-400">
                    Voice Costs
                  </h3>
                  <ul className="space-y-2 text-sm text-gray-300">
                    <li>• PlayHT TTS v3: $0.03/min</li>
                    <li>• PlayAI Dialog: $0.05/min</li>
                    <li>• F5 TTS: $0.0006-0.0012/s of GPU time</li>
                    <li>• Sync.so LipSync: $0.05/min</li>
                  </ul>
                </div>
              </div>

              <div className="mt-4 p-3 bg-blue-900/20 border border-blue-800/30 rounded-lg text-sm text-gray-300">
                <span className="font-medium text-blue-400">
                  Cost-saving tip:
                </span>{" "}
                GPU time-based models (like SD 3.5, Stable Audio, F5 TTS) often
                cost less than fixed-price models for simple generations. For
                example, a basic SD 3.5 image might only cost $0.01-0.02 if
                generated in 10-20 seconds, compared to $0.10+ for Flux models.
              </div>

              {/* Best Practices Tips */}
              <div className="mt-8 pt-6 border-t border-gray-700">
                <div className="flex items-center mb-4">
                  <AlertTriangleIcon className="w-5 h-5 mr-2 text-yellow-400" />
                  <h3 className="font-bold text-lg">Best Practices</h3>
                </div>
                <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6 text-sm text-gray-300">
                  <div>
                    <h4 className="font-medium mb-2 text-blue-400">
                      Video Tips
                    </h4>
                    <ul className="space-y-1">
                      <li>• Start with short clips (5-10s)</li>
                      <li>• Be specific with camera movements</li>
                      <li>• Describe lighting and atmosphere</li>
                      <li>• Test concepts with cheaper models first</li>
                    </ul>
                  </div>
                  <div>
                    <h4 className="font-medium mb-2 text-purple-400">
                      Image Tips
                    </h4>
                    <ul className="space-y-1">
                      <li>• Use detailed visual descriptions</li>
                      <li>• Specify style and artistic references</li>
                      <li>• Include composition details</li>
                      <li>• Iterate on successful outputs</li>
                    </ul>
                  </div>
                  <div>
                    <h4 className="font-medium mb-2 text-green-400">
                      Audio Tips
                    </h4>
                    <ul className="space-y-1">
                      <li>• Define genre and mood clearly</li>
                      <li>• Specify tempo and duration</li>
                      <li>• Reference similar tracks</li>
                      <li>• Note key transitions or moments</li>
                    </ul>
                  </div>
                  <div>
                    <h4 className="font-medium mb-2 text-yellow-400">
                      Voice Tips
                    </h4>
                    <ul className="space-y-1">
                      <li>• Use proper punctuation for pacing</li>
                      <li>• Mark emphasis appropriately</li>
                      <li>• Include pronunciation guides</li>
                      <li>• Provide high-quality voice samples</li>
                    </ul>
                  </div>
                </div>
              </div>
            </div>
          </section>

          {/* Help Section */}
          <section>
            <div className="bg-gray-800 rounded-xl p-6 flex flex-col md:flex-row md:items-center md:justify-between">
              <div className="mb-4 md:mb-0">
                <h2 className="text-xl font-bold mb-2">Need Help?</h2>
                <p className="text-gray-400 text-sm">
                  Contact support or check the documentation
                </p>
              </div>
              <div className="flex space-x-4">
                <a
                  href="/docs"
                  className="px-4 py-2 bg-blue-600 text-sm font-medium rounded-lg hover:bg-blue-700 transition-colors"
                >
                  Documentation
                </a>
                <a
                  href="/support"
                  className="px-4 py-2 bg-gray-700 text-sm font-medium rounded-lg hover:bg-gray-600 transition-colors"
                >
                  Support
                </a>
              </div>
            </div>
          </section>
        </div>
      </main>
      <Footer />
    </div>
  );
}

================
File: src/app/share/[id]/page.tsx
================
import Header from "@/components/header";
import { Button } from "@/components/ui/button";
import { fetchSharedVideo } from "@/lib/share";
import { DownloadIcon } from "lucide-react";
import type { Metadata, ResolvingMetadata } from "next";
import { notFound } from "next/navigation";

type PageParams = {
  id: string;
};

type PageProps = {
  params: PageParams;
};

export async function generateMetadata(
  { params }: PageProps,
  parent: ResolvingMetadata,
): Promise<Metadata> {
  const video = await fetchSharedVideo(params.id);
  if (!video) {
    return {
      title: "Video Not Found",
    };
  }

  const previousImages = (await parent).openGraph?.images || [];

  return {
    title: video.title,
    description: video.description || "Watch on Video AI Studio",

    // Open Graph metadata
    openGraph: {
      title: video.title,
      description: video.description,
      type: "video.other",
      videos: [
        {
          url: video.videoUrl,
          width: video.width,
          height: video.height,
          type: "video/mp4",
        },
      ],
      images: [
        {
          url: video.thumbnailUrl,
          width: video.width,
          height: video.height,
          alt: video.title,
        },
        ...previousImages,
      ],
    },

    // Twitter Card metadata
    twitter: {
      card: "player",
      title: video.title,
      description: video.description,
      players: [
        {
          playerUrl: video.videoUrl,
          streamUrl: video.videoUrl,
          width: video.width,
          height: video.height,
        },
      ],
      images: [video.thumbnailUrl],
    },

    // Additional metadata
    other: {
      // TODO resolve duration
      "og:video:duration": "15",
      "video:duration": "15",
      "video:release_date": new Date(video.createdAt).toISOString(),
    },
  };
}

export default async function SharePage({ params }: PageProps) {
  const shareId = params.id;
  const shareData = await fetchSharedVideo(shareId);
  if (!shareData) {
    return notFound();
  }

  return (
    <div className="flex flex-col h-screen bg-background">
      <Header />
      <main className="flex overflow-hidden h-full">
        <div className="container mx-auto py-8 h-full">
          <div className="flex flex-col gap-8 items-center justify-center h-full">
            <h1 className="font-semibold text-2xl">{shareData.title}</h1>
            <p className="text-muted-foreground max-w-3xl w-full sm:w-3xl text-center">
              {shareData.description}
            </p>
            <div className="max-w-4xl">
              <video
                src={shareData.videoUrl}
                poster={shareData.thumbnailUrl}
                controls
                className="w-full h-full aspect-video"
              />
            </div>
            <div className="flex flex-row gap-2 items-center justify-center">
              <Button variant="secondary" asChild size="lg">
                <a href={shareData.videoUrl} download>
                  <DownloadIcon className="w-4 h-4 opacity-50" />
                  Download
                </a>
              </Button>
              <Button variant="secondary" size="lg" asChild>
                <a href="/">Start your project</a>
              </Button>
            </div>
          </div>
        </div>
      </main>
    </div>
  );
}

================
File: src/app/globals.css
================
@tailwind base;
@tailwind components;
@tailwind utilities;

html {
  font-size: 14px;
}

body {
  font-family: "Poppins", -apple-system, Arial, sans-serif;
  transition: background-color 0.3s ease, color 0.3s ease;
  background: linear-gradient(
    to bottom right,
    var(--tw-gradient-from),
    var(--tw-gradient-to)
  );
  min-height: 100vh;
}

@layer utilities {
  .text-balance {
    text-wrap: balance;
  }
}

@layer base {
  :root {
    /* Light mode variables */
    --background: 220 20% 97%;
    --background-light: 220 20% 100%;
    --background-dark: 220 20% 95%;
    --foreground: 220 20% 3.9%;
    --card: 0 0% 100%;
    --card-foreground: 220 20% 3.9%;
    --popover: 0 0% 100%;
    --popover-foreground: 220 20% 3.9%;
    --primary: 220 85% 57%;
    --primary-foreground: 0 0% 98%;
    --secondary: 220 20% 96.1%;
    --secondary-foreground: 220 20% 9%;
    --muted: 220 20% 96.1%;
    --muted-foreground: 220 20% 45%;
    --accent: 220 20% 96.1%;
    --accent-foreground: 220 20% 9%;
    --destructive: 0 84.2% 60.2%;
    --destructive-foreground: 0 0% 98%;
    --border: 220 20% 89.8%;
    --input: 220 20% 89.8%;
    --ring: 220 85% 57%;
    --chart-1: 220 70% 50%;
    --chart-2: 160 60% 45%;
    --chart-3: 30 80% 55%;
    --chart-4: 280 65% 60%;
    --chart-5: 340 75% 55%;
    --radius: 0.5rem;

    /* Background gradient - softer light tone matching homepage */
    --tw-gradient-from: #f4f4f6;
    --tw-gradient-to: #e8e8ec;
  }
  .dark {
    /* Dark mode variables */
    --background: 240 10% 3.9%;
    --background-light: 240 10% 5%;
    --background-dark: 240 10% 3%;
    --foreground: 0 0% 98%;
    --card: 240 10% 3.9%;
    --card-foreground: 0 0% 98%;
    --popover: 240 10% 3.9%;
    --popover-foreground: 0 0% 98%;
    --primary: 217.2 91.2% 59.8%;
    --primary-foreground: 222.2 47.4% 11.2%;
    --secondary: 217.2 32.6% 17.5%;
    --secondary-foreground: 210 40% 98%;
    --muted: 217.2 32.6% 17.5%;
    --muted-foreground: 215 20.2% 65.1%;
    --accent: 217.2 32.6% 17.5%;
    --accent-foreground: 210 40% 98%;
    --destructive: 0 62.8% 30.6%;
    --destructive-foreground: 210 40% 98%;
    --border: 217.2 32.6% 17.5%;
    --input: 217.2 32.6% 17.5%;
    --ring: 224.3 76.3% 48%;
    --chart-1: 220 70% 50%;
    --chart-2: 160 60% 45%;
    --chart-3: 30 80% 55%;
    --chart-4: 280 65% 60%;
    --chart-5: 340 75% 55%;

    /* Background gradient for dark mode - matches homepage */
    --tw-gradient-from: #0a1128;
    --tw-gradient-via: #1b3a6b;
    --tw-gradient-to: #16213e;
  }
}

@layer base {
  * {
    @apply border-border;
  }
  body {
    @apply bg-background text-foreground;
  }
}

/* Glassmorphism effects */
.glassmorphism {
  background: rgba(255, 255, 255, 0.15);
  backdrop-filter: blur(12px);
  border-radius: var(--radius);
  border: 1px solid rgba(255, 255, 255, 0.2);
  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
  transition: all 0.3s ease-in-out;
}

.dark .glassmorphism {
  background: rgba(26, 26, 26, 0.75);
  border-color: rgba(255, 255, 255, 0.05);
  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
  backdrop-filter: blur(10px);
}

/* Enhanced UI Elements */
.panel-shadow {
  box-shadow: 0 10px 30px -15px rgba(0, 0, 0, 0.3);
}

.dark .panel-shadow {
  box-shadow: 0 10px 30px -15px rgba(0, 0, 0, 0.5);
}

.hover-lift {
  transition: transform 0.2s ease, box-shadow 0.2s ease;
}

.hover-lift:hover {
  transform: translateY(-2px);
  box-shadow: 0 10px 30px -10px rgba(0, 0, 0, 0.3);
}

/* Button Styling */
button,
.btn {
  border-radius: var(--radius);
  transition: all 0.2s ease-in-out;
}

button:hover,
.btn:hover {
  transform: translateY(-1px);
}

/* Scroll Bar Styles */
::-webkit-scrollbar {
  height: 0.25rem;
  width: 0.25rem;
}

::-webkit-scrollbar-track {
  background-color: rgb(38 38 38 / 0.2);
}

::-webkit-scrollbar-thumb {
  background-color: rgb(64 64 64 / 0.5);
  border-radius: 0.25rem;
}

::-webkit-scrollbar-thumb:hover {
  background-color: rgb(82 82 82 / 0.8);
}

/* Left panel glassmorphism effects */
.dark [class*="left-panel"] {
  background: linear-gradient(
    to bottom right,
    rgba(10, 17, 40, 0.75),
    rgba(27, 58, 107, 0.75),
    rgba(22, 33, 62, 0.75)
  );
  backdrop-filter: blur(12px);
  border: 1px solid rgba(255, 255, 255, 0.05);
}

[class*="left-panel"] {
  background: rgba(255, 255, 255, 0.15);
  backdrop-filter: blur(12px);
  border: 1px solid rgba(255, 255, 255, 0.2);
}

/* Hide the dark mode toggle that might be coming from the parent page */
#darkModeToggle {
  display: none !important;
}

================
File: src/app/layout.tsx
================
import type { Metadata } from "next";
import { GeistSans } from "geist/font/sans";
import "./globals.css";
import "@/styles/main-app-theme.css";
import "@/styles/right-panel.css";
import { ThemeProvider } from "@/components/theme-provider";

export const metadata: Metadata = {
  title: "AI Visual Studio",
  description: "Create videos from your story using AI",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en" suppressHydrationWarning>
      <head>
        <link
          href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap"
          rel="stylesheet"
        />
        <script src="/key-copy.js"></script>
      </head>
      <body className={GeistSans.className}>
        <ThemeProvider
          attribute="class"
          defaultTheme="dark"
          enableSystem
          disableTransitionOnChange={false}
          themes={["light", "dark"]}
        >
          {children}
        </ThemeProvider>
      </body>
    </html>
  );
}

================
File: src/app/page.tsx
================
"use client";

import { useEffect, useState } from "react"; // Removed useRef as it wasn't used here
import { UserData } from "./session-manager";
import sessionManager from "./session-manager";
import Link from "next/link";
import { ThemeToggle } from "@/components/theme-toggle";
import { Logo } from "@/components/logo";
import { Button } from "@/components/ui/button";
import {
  SettingsIcon,
  Edit3Icon,
  Users,
  FileTextIcon,
  Home,
  DownloadIcon,
  LayoutDashboard,
  AlertCircle,
} from "lucide-react";
import config from "@/lib/config";
import { useRouter } from "next/navigation";
import { Textarea } from "@/components/ui/textarea";
import { Label } from "@/components/ui/label";
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs";
import {
  Tooltip,
  TooltipContent,
  TooltipProvider,
  TooltipTrigger,
} from "@/components/ui/tooltip";

export default function IndexPage() {
  // Original state variables (will be populated after auth check)
  const [sdkUserData, setSdkUserData] = useState<UserData | null>(null); // Renamed from userData
  // const [isLoading, setIsLoading] = useState(true); // Replaced by isSessionReady/isAuthenticated logic
  // const [debugInfo, setDebugInfo] = useState<string>("Initializing..."); // Can be removed or kept for debugging
  const [expandedChapters, setExpandedChapters] = useState<
    Record<number, boolean>
  >({});
  // Auth/Session State
  const [isSessionReady, setIsSessionReady] = useState(false);
  const [isAuthenticated, setIsAuthenticated] = useState<boolean | null>(null);
  const [parentOrigin, setParentOrigin] = useState<string | null>(null);
  const [slideSelections, setSlideSelections] = useState<
    Record<number, string>
  >({});
  // Iframe detection state
  const [isInIframe, setIsInIframe] = useState<boolean>(false);

  // New state for storyboard from scratch feature
  const [storyInput, setStoryInput] = useState("");
  const [customSlideCount, setCustomSlideCount] = useState("3");
  const [customImageStyle, setCustomImageStyle] = useState("fantasy");
  const [inputError, setInputError] = useState("");

  const router = useRouter();

  const toggleChapter = (index: number) => {
    setExpandedChapters((prev) => ({
      ...prev,
      [index]: !prev[index],
    }));
  };

  const handleSlideCountChange = (index: number, value: string) => {
    setSlideSelections((prev) => ({
      ...prev,
      [index]: value,
    }));
  };

  // Function to validate text input
  const validateStoryInput = (text: string) => {
    // Check if text is too short (minimum 20 words)
    const wordCount = text.trim().split(/\s+/).length;
    if (wordCount < 20) {
      setInputError(
        "Your story is too short! Please add more details for better visuals.",
      );
      return false;
    }

    // Check if text is too vague (very basic check)
    if (text.length < 100) {
      setInputError(
        "This text is too vague. Try adding names, locations, and actions.",
      );
      return false;
    }

    // Clear any previous errors
    setInputError("");
    return true;
  };

  // Function to generate storyboard from chapters (original function)
  const generateStoryboard = (index: number) => {
    // Check if a slide count is selected
    if (!slideSelections[index] || slideSelections[index] === "") {
      alert("Please select the number of slides first");
      return;
    }

    const chapter = sdkUserData?.chapters?.[index]; // Fix: Use sdkUserData
    if (!chapter) return;

    const slideCount = parseInt(slideSelections[index]);
    console.log(
      `Generating ${slideCount} slides for chapter ${chapter.number}`,
    );

    // Generate prompts for each slide
    const slides = Array.from({ length: slideCount }, (_, i) => {
      // Create different prompts for different parts of the chapter
      const contentLength = chapter.content.length;
      const segmentSize = Math.floor(contentLength / slideCount);
      const startPos = i * segmentSize;
      const endPos = Math.min(startPos + segmentSize, contentLength);
      const segmentContent = chapter.content.substring(startPos, endPos);

      // Create a preview version for UI display, but keep the full content in the prompt
      const previewText =
        segmentContent.length > 200
          ? segmentContent.substring(0, 200) + "..."
          : segmentContent;

      // Create a prompt that focuses on different parts of the chapter
      return {
        chapterNumber: chapter.number,
        prompt: `Create a visual representation for chapter ${chapter.number}: ${chapter.title}. Scene description based on the following excerpt: ${segmentContent}`,
        promptPreview: `Create a visual representation for chapter ${chapter.number}: ${chapter.title}. Scene description based on the following excerpt: ${previewText}`,
        imageUrl: undefined,
      };
    });

    // Store storyboard data in localStorage
    const storyboardData = {
      slides,
      metadata: {
        source: "chapter",
        chapterNumber: chapter.number,
        chapterTitle: chapter.title,
        style: "fantasy", // Default style for chapter-based storyboards
        location: sdkUserData?.location || "", // Fix: Use sdkUserData
        timeline: sdkUserData?.timeline || "", // Fix: Use sdkUserData
      },
    };

    try {
      localStorage.setItem("storyboardData", JSON.stringify(storyboardData));
      console.log(
        "Successfully stored chapter storyboard data in localStorage:",
        storyboardData,
      );

      // Redirect to the video editor app with a clear URL indicator
      router.push("/app?storyboard=true");
    } catch (error) {
      console.error("Error storing storyboard data:", error);
      alert("Error preparing storyboard data. Please try again.");
    }
  };

  // Function to generate storyboard from custom text
  const generateStoryboardFromScratch = () => {
    // Validate the input first
    if (!validateStoryInput(storyInput)) {
      return;
    }

    if (!customSlideCount || customSlideCount === "") {
      setInputError("Please select the number of slides first");
      return;
    }

    const slideCount = parseInt(customSlideCount);
    console.log(`Generating ${slideCount} slides from custom story`);

    // Generate basic slide descriptions from the input text
    // This will be enhanced by the AI in storyboard-panel.tsx
    const contentLength = storyInput.length;
    const segmentSize = Math.floor(contentLength / slideCount);

    const slides = Array.from({ length: slideCount }, (_, i) => {
      const startPos = i * segmentSize;
      const endPos = Math.min(startPos + segmentSize, contentLength);
      const segmentContent = storyInput.substring(startPos, endPos);

      // Create a visual preview for UI that's truncated, but keep full content in prompt
      const previewText =
        segmentContent.length > 200
          ? segmentContent.substring(0, 200) + "..."
          : segmentContent;

      return {
        chapterNumber: "Custom", // Mark as custom to differentiate from chapter-based slides
        prompt: `Create a visual representation for the following scene: ${segmentContent}`,
        promptPreview: `Create a visual representation for the following scene: ${previewText}`,
        imageUrl: undefined,
      };
    });

    // Store storyboard data in localStorage
    const storyboardData = {
      slides,
      metadata: {
        source: "custom",
        style: customImageStyle,
        fullStory: storyInput,
      },
    };

    try {
      localStorage.setItem("storyboardData", JSON.stringify(storyboardData));
      console.log(
        "Successfully stored custom storyboard data in localStorage:",
        storyboardData,
      );

      // Also store the original input for reference
      localStorage.setItem("story_input", storyInput);

      // Redirect to the video editor app
      router.push("/app?storyboard=true");
    } catch (error) {
      console.error("Error storing storyboard data:", error);
      setInputError("Error preparing storyboard data. Please try again.");
    }
  };

  // Detect if running in iframe
  useEffect(() => {
    try {
      // Check if window.self is different from window.top
      const inIframe = window.self !== window.top;
      console.log(`SDK: Running in iframe: ${inIframe}`);
      setIsInIframe(inIframe);

      // If in iframe, send a message to parent to notify we're ready for fullscreen
      if (inIframe) {
        console.log("SDK: Running in iframe - will auto-expand to fullscreen");
        // The actual fullscreen trigger will happen after authentication
      }
    } catch (e) {
      // If accessing window.top throws an error, we're definitely in an iframe with different origin
      console.log(
        "SDK: Error checking iframe status - assuming iframe with restrictions",
      );
      setIsInIframe(true);
    }
  }, []);

  // Listen for messages from the parent page
  useEffect(() => {
    console.log(
      "IndexPage component mounted - setting up postMessage listener",
    );
    // setDebugInfo("Setting up message listener..."); // Removed

    // Check for storyboard data in localStorage
    try {
      const storyboardData = localStorage.getItem("storyboardData");
      if (storyboardData) {
        console.log(
          "Found storyboard data in localStorage in page.tsx:",
          storyboardData,
        );
      } else {
        console.log("No storyboard data found in localStorage in page.tsx");
      }
    } catch (error) {
      console.error("Error checking storyboard data:", error);
    }

    // Check for session data first
    const currentData = sessionManager.getUserData();
    if (!currentData) {
      // Try to load from localStorage if no data in session
      try {
        const savedData = localStorage.getItem("videoProjectSessionData");
        if (savedData) {
          const parsedData = JSON.parse(savedData);
          sessionManager.initializeSession(parsedData);
          // setUserData(sessionManager.getUserData()); // Data is set later after auth check
          // setIsLoading(false); // Removed
          console.log(
            "Session data loaded from localStorage cache (pre-auth check)",
          );
          // setDebugInfo("Session data loaded from localStorage"); // Removed
        }
      } catch (error) {
        console.error("Error loading from localStorage:", error);
      }
    } else {
      // We already have data in the session from a previous load or message
      // Set the SDK user data state here as well, but wait for auth check effect
      setSdkUserData(currentData); // Restore setting the state
      console.log(
        "Session data already available in sessionManager (pre-auth check), set in state.",
      );
    }

    function handleMessage(event: MessageEvent) {
      console.log(
        "Message received from:",
        event.origin,
        "Type:",
        event.data?.type,
      );
      // setDebugInfo( // Removed
      //   `Message received from: ${event.origin} at ${new Date().toISOString()}`,
      // );

      // Log full message data for debugging
      console.log("Full message data:", JSON.stringify(event.data, null, 2));

      // Validate the origin - be more permissive in development
      if (
        event.origin.includes("localhost") ||
        event.origin.includes("novelvisionai.art") ||
        event.origin === "null" || // For local file:// testing
        event.origin.includes("127.0.0.1")
      ) {
        console.log("Authorized origin:", event.origin);
        // setDebugInfo(`Origin validated: ${event.origin}`); // Removed
        setParentOrigin(event.origin); // Store the validated parent origin

        // Handle ping test
        if (event.data && event.data.type === "PING_TEST") {
          console.log("Ping test received:", event.data.message);
          // setDebugInfo(`Ping test received: ${event.data.message}`); // Removed

          // Respond to ping - use * for origin in development to be more permissive
          try {
            window.parent.postMessage(
              {
                type: "PING_RESPONSE",
                message: "Hello from AI Visual Studio!",
              },
              "*",
            ); // Less strict for testing
            console.log("Ping response sent");
            // setDebugInfo("Ping response sent, waiting for data..."); // Removed
          } catch (error) {
            console.error("Error sending ping response:", error);
            // setDebugInfo(`Error sending ping response: ${error}`); // Removed
          }
          return; // Important: Stop processing after handling ping
        }

        // Handle FALAI_KEY_RESPONSE message
        if (event.data && event.data.type === "FALAI_KEY_RESPONSE") {
          console.log("FALAI_KEY_RESPONSE received");
          const falaiKey = event.data.falai_key;

          if (falaiKey) {
            console.log(
              "SDK: Received falai_key from parent, saving to localStorage",
            );
            sessionManager.saveFalApiKey(falaiKey);
          } else {
            console.warn(
              "SDK: Received FALAI_KEY_RESPONSE but no key was provided",
            );
          }
          return; // Stop processing after handling key response
        }

        // Handle any message with user data - be more tolerant of message format
        // Accept USER_DATA or AI_VISUAL_STUDIO_DATA or any message with the right fields
        if (event.data) {
          let userData = null;

          // Try different message formats
          if (event.data.type === "USER_DATA") {
            console.log("USER_DATA message format detected");
            userData = event.data;
          } else if (
            event.data.type === "AI_VISUAL_STUDIO_DATA" &&
            event.data.userData
          ) {
            console.log("AI_VISUAL_STUDIO_DATA message format detected");
            userData = event.data.userData;
          } else if (
            event.data.apiKeys ||
            event.data.language ||
            event.data.title
          ) {
            // If it has any expected fields, try to use it directly
            console.log("Non-standard message format with user data detected");
            userData = event.data;
          }

          if (userData) {
            // Log what we found
            console.log("User data detected, summary:");

            // Check for API keys which might be in different formats
            const openrouterKey =
              userData.apiKeys?.openrouter || userData.openrouterApiKey || "";
            const openaiKey =
              userData.apiKeys?.openai || userData.openaiApiKey || "";
            const falaiKey =
              userData.apiKeys?.falai || userData.falaiApiKey || "";

            console.log(
              "- API Keys present:",
              !!openrouterKey,
              !!openaiKey,
              !!falaiKey,
            );
            console.log("- Language:", userData.language);
            console.log("- Genre:", userData.genre);
            console.log("- Title:", userData.title);
            console.log(
              "- Characters count:",
              userData.characters?.length || 0,
            );
            console.log("- Outline count:", userData.outline?.length || 0);
            console.log("- Chapters count:", userData.chapters?.length || 0);

            // Initialize the session with the full received data (including auth status)
            try {
              console.log("Initializing session with received data");
              sessionManager.initializeSession(userData); // Pass the whole payload

              // Update local state to reflect session readiness and auth status
              setIsAuthenticated(sessionManager.getIsAuthenticated());
              setIsSessionReady(true); // Mark session as ready

              console.log("Session initialized via postMessage.");
            } catch (error) {
              console.error("Error initializing session:", error);
              setIsSessionReady(true); // Still mark as ready, but auth might be null/false
              setIsAuthenticated(null); // Indicate error state
            }
          } else {
            console.log(
              "Message received but no valid user data structure found",
            );
            // Potentially handle this case - maybe mark session ready but unauthenticated?
            setIsAuthenticated(false);
            setIsSessionReady(true);
          }
        }
      } else {
        console.warn("Unauthorized origin:", event.origin);
      }
    }

    window.addEventListener("message", handleMessage);
    // setDebugInfo("Message listener active, waiting for messages..."); // Removed
    return () => window.removeEventListener("message", handleMessage);
  }, []); // Effect runs once on mount

  // Effect to check authentication status once session is ready
  useEffect(() => {
    if (isSessionReady) {
      console.log("SDK: Session is ready. Checking authentication status.");
      // Use the locally tracked isAuthenticated state which was set by the message listener
      if (isAuthenticated === false) {
        console.log(
          "SDK: User is not authenticated. Sending REDIRECT_AUTH to parent.",
        );
        if (parentOrigin) {
          // Ensure we know where to send the message
          window.parent.postMessage({ type: "REDIRECT_AUTH" }, parentOrigin);
          console.log("SDK: REDIRECT_AUTH sent to:", parentOrigin);
        } else {
          console.error(
            "SDK: Cannot send REDIRECT_AUTH, parent origin not known.",
          );
          // Fallback for safety, though parentOrigin should be set
          window.parent.postMessage({ type: "REDIRECT_AUTH" }, "*");
        }
      } else if (isAuthenticated === true) {
        console.log("SDK: User is authenticated. Proceeding with rendering.");
        // Load data from session manager into local state now that auth is confirmed
        setSdkUserData(sessionManager.getUserData());

        // DIRECT KEY INJECTION: Try to get the falai_key from the userData and save it to localStorage
        const userData = sessionManager.getUserData();
        if (userData && userData.falaiApiKey) {
          console.log(
            "SDK: Direct key injection - Found falai_key in userData, saving to localStorage",
          );
          sessionManager.saveFalApiKey(userData.falaiApiKey);
        } else {
          console.warn(
            "SDK: Direct key injection - No falai_key found in userData",
          );

          // Try to get the key from the parent's localStorage via postMessage
          console.log("SDK: Requesting falai_key from parent via postMessage");
          if (parentOrigin) {
            window.parent.postMessage(
              { type: "REQUEST_FALAI_KEY" },
              parentOrigin,
            );
          } else {
            window.parent.postMessage({ type: "REQUEST_FALAI_KEY" }, "*");
          }
        }

        // If we're in an iframe, notify the parent to trigger fullscreen mode
        if (isInIframe && parentOrigin) {
          console.log(
            "SDK: Authenticated and in iframe - requesting fullscreen mode",
          );
          window.parent.postMessage(
            { type: "TRIGGER_FULLSCREEN" },
            parentOrigin,
          );
        } else if (isInIframe) {
          // Fallback if we don't have the parent origin
          window.parent.postMessage({ type: "TRIGGER_FULLSCREEN" }, "*");
        }
      }
    }
  }, [isSessionReady, isAuthenticated, parentOrigin, isInIframe]); // Re-run if session readiness or auth status changes

  // --- Conditional Rendering Logic ---
  if (!isSessionReady) {
    // Still waiting for the initial message from the parent
    return (
      <div className="flex items-center justify-center min-h-screen bg-gradient-to-br from-[#0A1128] via-[#1B3A6B] to-[#16213E] text-white">
        Initializing Session... Waiting for Parent Application...
      </div>
    );
  }

  if (isAuthenticated === false) {
    // User is not authenticated, waiting for parent to redirect
    return (
      <div className="flex items-center justify-center min-h-screen bg-gradient-to-br from-[#0A1128] via-[#1B3A6B] to-[#16213E] text-white">
        Authentication required. Redirecting to login...
      </div>
    );
  }

  if (isAuthenticated === true && sdkUserData) {
    // --- Render the main UI using sdkUserData ---
    return (
      <div className="min-h-screen bg-gradient-to-br from-[#0A1128] via-[#1B3A6B] to-[#16213E] text-white relative">
        <header className="px-4 py-2 flex justify-between items-center border-b border-border glassmorphism mb-8 bg-gradient-to-br from-[#0A1128] via-[#1B3A6B] to-[#16213E]">
          <div className="flex items-center">
            <Logo />
            <span className="mx-2 text-gray-400">|</span>
            <h2 className="text-lg font-medium">AI Visual Studio</h2>
          </div>

          <nav className="flex flex-row items-center justify-end gap-2">
            <ThemeToggle />

            <Button
              variant="ghost"
              size="sm"
              onClick={() => (window.location.href = "/app")}
            >
              <LayoutDashboard className="w-4 h-4 mr-1" />
              <span className="hidden sm:inline">Video Studio</span>
            </Button>

            <Button variant="ghost" size="sm" asChild>
              <Link href={config.urls.main} target="_blank">
                <Home className="w-4 h-4 mr-1" />
                <span className="hidden sm:inline">Home</span>
              </Link>
            </Button>

            <Button variant="ghost" size="sm" asChild>
              <Link href={config.urls.writingWorkspace}>
                <Edit3Icon className="w-4 h-4 mr-1" />
                <span className="hidden sm:inline">Writing Space</span>
              </Link>
            </Button>

            <Button variant="ghost" size="sm" asChild>
              <Link href={config.urls.characterSetup} target="_blank">
                <Users className="w-4 h-4 mr-1" />
                <span className="hidden sm:inline">Characters</span>
              </Link>
            </Button>

            <Button variant="ghost" size="sm" asChild>
              <Link href={config.urls.storyOutline} target="_blank">
                <FileTextIcon className="w-4 h-4 mr-1" />
                <span className="hidden sm:inline">Outline</span>
              </Link>
            </Button>

            <Button variant="ghost" size="sm" asChild>
              <Link href={config.urls.settings} target="_blank">
                <SettingsIcon className="w-4 h-4 mr-1" />
                <span className="hidden sm:inline">Settings</span>
              </Link>
            </Button>
          </nav>
        </header>

        <main className="max-w-7xl mx-auto p-6">
          {/* Main content is now rendered directly if authenticated */}
          <div className="space-y-8">
            {/* Tabs Only Section at Top */}
            <div className="glassmorphism p-6 border-gray-800 bg-gradient-to-br from-[#0A1128] via-[#1B3A6B] to-[#16213E]">
              <h2 className="text-2xl font-bold mb-6 text-white">
                Create Storyboard
              </h2>
              <Tabs defaultValue="chapters" className="w-full">
                <TabsList className="grid w-full grid-cols-2 mb-6 p-1 bg-gray-800/70 rounded-lg">
                  <TabsTrigger
                    value="chapters"
                    className="transition-all duration-200 hover:bg-gray-700/50 data-[state=active]:bg-blue-600/70 data-[state=active]:text-white rounded-md py-2"
                  >
                    From Chapters
                  </TabsTrigger>
                  <TabsTrigger
                    value="scratch"
                    className="transition-all duration-200 hover:bg-gray-700/50 data-[state=active]:bg-blue-600/70 data-[state=active]:text-white rounded-md py-2"
                  >
                    Start from Scratch
                  </TabsTrigger>
                </TabsList>

                <TabsContent value="chapters">
                  <div className="text-center py-4">
                    <p className="text-gray-300">
                      Scroll down to view your chapters and create storyboards
                      from them.
                    </p>
                  </div>
                </TabsContent>

                <TabsContent value="scratch">
                  {/* Start from Scratch UI */}
                  <div className="space-y-5">
                    <div className="bg-gray-900/50 p-5 rounded-lg border border-gray-800">
                      <div className="flex justify-between items-start mb-2">
                        <Label
                          htmlFor="story-input"
                          className="text-sm font-medium flex items-center"
                        >
                          Your Story
                          <TooltipProvider>
                            <Tooltip>
                              <TooltipTrigger asChild>
                                <span className="ml-1.5 cursor-help">
                                  <AlertCircle
                                    size={14}
                                    className="text-gray-400"
                                  />
                                </span>
                              </TooltipTrigger>
                              <TooltipContent className="max-w-xs">
                                <p>
                                  For best results, describe what happens, where
                                  it happens, and any important details.
                                </p>
                              </TooltipContent>
                            </Tooltip>
                          </TooltipProvider>
                        </Label>
                      </div>

                      <Textarea
                        id="story-input"
                        placeholder="Write a short story or scene description. Include actions, characters, and locations."
                        className="h-48 resize-none mb-3 bg-gray-800/70 border-gray-700 focus:border-blue-500 focus:ring-1 focus:ring-blue-500"
                        value={storyInput}
                        onChange={(e) => {
                          setStoryInput(e.target.value);
                          if (inputError) setInputError("");
                        }}
                      />

                      {inputError && (
                        <div className="bg-red-900/30 border border-red-700/50 p-3 rounded-md mb-4">
                          <p className="text-red-200 text-sm flex items-center">
                            <AlertCircle
                              size={14}
                              className="mr-1.5 flex-shrink-0"
                            />
                            {inputError}
                          </p>
                        </div>
                      )}

                      <div className="grid grid-cols-1 md:grid-cols-2 gap-4 mt-4">
                        <div>
                          <Label
                            htmlFor="slide-count"
                            className="text-sm font-medium mb-1.5 block"
                          >
                            Number of Slides
                          </Label>
                          <select
                            id="slide-count"
                            className="w-full bg-gray-800/70 border border-gray-700 text-gray-200 text-sm py-2 px-3 rounded-lg focus:ring-1 focus:ring-blue-500 focus:border-blue-500 transition-colors duration-200 hover:border-blue-400"
                            value={customSlideCount}
                            onChange={(e) =>
                              setCustomSlideCount(e.target.value)
                            }
                          >
                            <option value="1">1 Slide</option>
                            <option value="2">2 Slides</option>
                            <option value="3">3 Slides</option>
                            <option value="4">4 Slides</option>
                            <option value="5">5 Slides</option>
                            <option value="6">6 Slides</option>
                          </select>
                        </div>

                        <div>
                          <Label
                            htmlFor="image-style"
                            className="text-sm font-medium mb-1.5 block"
                          >
                            Image Style
                          </Label>
                          <select
                            id="image-style"
                            className="w-full bg-gray-800/70 border border-gray-700 text-gray-200 text-sm py-2 px-3 rounded-lg focus:ring-1 focus:ring-blue-500 focus:border-blue-500 transition-colors duration-200 hover:border-blue-400"
                            value={customImageStyle}
                            onChange={(e) =>
                              setCustomImageStyle(e.target.value)
                            }
                          >
                            <option value="fantasy">Fantasy</option>
                            <option value="cyberpunk">Cyberpunk</option>
                            <option value="gothic">Gothic</option>
                            <option value="historical">Historical</option>
                            <option value="surreal">Surreal</option>
                            <option value="anime">Anime</option>
                            <option value="scifi">SciFi</option>
                            <option value="watercolor">Watercolor</option>
                          </select>
                        </div>
                      </div>

                      <Button
                        onClick={generateStoryboardFromScratch}
                        className="w-full mt-6 bg-gradient-to-r from-blue-600 to-indigo-600 hover:from-blue-500 hover:to-indigo-500 transition-all duration-300"
                      >
                        <svg
                          xmlns="http://www.w3.org/2000/svg"
                          className="h-4 w-4 mr-2"
                          viewBox="0 0 20 20"
                          fill="currentColor"
                        >
                          <path d="M11 3a1 1 0 100 2h2.586l-6.293 6.293a1 1 0 101.414 1.414L15 6.414V9a1 1 0 102 0V4a1 1 0 00-1-1h-5z" />
                          <path d="M5 5a2 2 0 00-2 2v8a2 2 0 002 2h8a2 2 0 002-2v-3a1 1 0 10-2 0v3H5V7h3a1 1 0 000-2H5z" />
                        </svg>
                        Create Storyboard
                      </Button>
                    </div>
                  </div>
                </TabsContent>
              </Tabs>
            </div>

            {/* Story Info Panel */}
            <div className="glassmorphism p-6 border-gray-800 bg-gradient-to-br from-[#0A1128] via-[#1B3A6B] to-[#16213E]">
              <h2 className="text-2xl font-bold mb-4 text-white">
                Story Information
              </h2>
              {sdkUserData.title ||
              sdkUserData.genre ||
              sdkUserData.language ? (
                <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
                  <div className="bg-gray-900/40 p-4 rounded-lg border border-gray-800 hover:border-gray-700 transition-colors duration-200">
                    <h3 className="text-gray-400 text-sm mb-1">Title</h3>
                    <p className="text-lg font-medium">
                      {sdkUserData.title || "Untitled Story"}
                    </p>
                  </div>
                  <div className="bg-gray-900/40 p-4 rounded-lg border border-gray-800 hover:border-gray-700 transition-colors duration-200">
                    <h3 className="text-gray-400 text-sm mb-1">Genre</h3>
                    <p className="text-lg font-medium">
                      {sdkUserData.genre || "Not specified"}
                    </p>
                  </div>
                  <div className="bg-gray-900/40 p-4 rounded-lg border border-gray-800 hover:border-gray-700 transition-colors duration-200">
                    <h3 className="text-gray-400 text-sm mb-1">Language</h3>
                    <p className="text-lg font-medium">
                      {sdkUserData.language || "Not specified"}
                    </p>
                  </div>
                  <div className="bg-gray-900/40 p-4 rounded-lg border border-gray-800 hover:border-gray-700 transition-colors duration-200">
                    <h3 className="text-gray-400 text-sm mb-1">Location</h3>
                    <p className="text-lg font-medium">
                      {sdkUserData.location || "Not specified"}
                    </p>
                  </div>
                </div>
              ) : (
                <div className="text-center py-4">
                  <p className="text-gray-300">
                    No story details yet. You can start creating content from
                    scratch!
                  </p>
                </div>
              )}
            </div>

            {/* Characters Panel */}
            <div className="glassmorphism p-6 border-gray-800 bg-gradient-to-br from-[#0A1128] via-[#1B3A6B] to-[#16213E]">
              <h2 className="text-2xl font-bold mb-6 text-white">Characters</h2>
              {sdkUserData.characters && sdkUserData.characters.length > 0 ? (
                <div className="grid grid-cols-1 sm:grid-cols-2 md:grid-cols-3 gap-5">
                  {sdkUserData.characters.map((character, index) => (
                    <div
                      key={index}
                      className="bg-gray-900/50 p-5 rounded-lg border border-gray-800 hover:border-blue-500/50 transition-all duration-300"
                    >
                      <h3 className="font-bold text-white">{character.name}</h3>
                      <p className="text-sm text-blue-300 mt-1">
                        {character.role || "No role specified"}
                      </p>
                      <p className="text-xs mt-3 text-gray-400 line-clamp-3">
                        {character.description || "No description"}
                      </p>
                    </div>
                  ))}
                </div>
              ) : (
                <p className="text-gray-400 text-center py-6">
                  No characters found. You can create visual content without
                  defined characters.
                </p>
              )}
            </div>

            {/* Full Storyboard Creation Section with Chapters */}
            <div className="glassmorphism p-6 border-gray-800 bg-gradient-to-br from-[#0A1128] via-[#1B3A6B] to-[#16213E]">
              <Tabs defaultValue="chapters" className="w-full">
                <TabsContent value="chapters">
                  {/* Existing Chapters Panel */}
                  {sdkUserData.chapters && sdkUserData.chapters.length > 0 ? (
                    <div className="grid grid-cols-1 gap-6">
                      {sdkUserData.chapters.map((chapter, index) => (
                        <div
                          key={index}
                          className="bg-gray-900/50 p-5 rounded-lg shadow-lg border border-gray-800 hover:border-blue-500/50 transition-all duration-300"
                        >
                          <h1 className="text-xl font-bold mb-3 text-white border-b border-gray-700/50 pb-2">
                            Chapter {chapter.number}: {chapter.title}
                          </h1>
                          <div className="text-sm mt-3 text-gray-300 leading-relaxed">
                            {expandedChapters[index] ? (
                              <div
                                dangerouslySetInnerHTML={{
                                  __html: chapter.content || "No content",
                                }}
                              />
                            ) : (
                              <div className="line-clamp-4">
                                {chapter.content ? (
                                  <div
                                    dangerouslySetInnerHTML={{
                                      __html:
                                        chapter.content.substring(0, 250) +
                                        "...",
                                    }}
                                  />
                                ) : (
                                  "No content"
                                )}
                              </div>
                            )}

                            <button
                              onClick={() => toggleChapter(index)}
                              className="mt-2 px-3 py-1 bg-gray-800/50 hover:bg-gray-700/50 rounded-md text-blue-400 hover:text-blue-300 transition-colors duration-200 text-xs"
                            >
                              {expandedChapters[index]
                                ? "Show Less"
                                : "Show More"}
                            </button>

                            <div className="flex items-center mt-3 space-x-2">
                              <select
                                className="bg-gray-800/70 border border-gray-700 text-gray-200 text-xs py-1.5 px-3 rounded-lg focus:ring-1 focus:ring-blue-500 focus:border-blue-500 transition-colors duration-200 hover:border-blue-400"
                                value={slideSelections[index] || ""}
                                onChange={(e) =>
                                  handleSlideCountChange(index, e.target.value)
                                }
                              >
                                <option value="">Slides</option>
                                <option value="1">1 Slide</option>
                                <option value="2">2 Slides</option>
                                <option value="3">3 Slides</option>
                                <option value="4">4 Slides</option>
                                <option value="5">5 Slides</option>
                                <option value="6">6 Slides</option>
                              </select>

                              <button
                                onClick={() => generateStoryboard(index)}
                                className="px-3 py-1.5 bg-gradient-to-r from-blue-600/80 to-indigo-600/80 hover:from-blue-500/80 hover:to-indigo-500/80 rounded-lg text-white text-xs font-medium shadow-sm flex items-center space-x-1 transition-all duration-200"
                              >
                                <svg
                                  xmlns="http://www.w3.org/2000/svg"
                                  className="h-3.5 w-3.5 mr-1"
                                  viewBox="0 0 20 20"
                                  fill="currentColor"
                                >
                                  <path
                                    fillRule="evenodd"
                                    d="M4 3a2 2 0 00-2 2v10a2 2 0 002 2h12a2 2 0 002-2V5a2 2 0 00-2-2H4zm12 12H4a.5.5 0 01-.5-.5V5.5A.5.5 0 014 5h12a.5.5 0 01.5.5v9a.5.5 0 01-.5.5z"
                                    clipRule="evenodd"
                                  />
                                  <path d="M6 7a1 1 0 011-1h6a1 1 0 110 2H7a1 1 0 01-1-1zm1 3a1 1 0 100 2h6a1 1 0 100-2H7z" />
                                </svg>
                                Generate Storyboard
                              </button>
                            </div>
                          </div>
                        </div>
                      ))}
                    </div>
                  ) : (
                    <p className="text-gray-400 text-center py-6">
                      No chapters found. Try creating a storyboard from scratch
                      instead.
                    </p>
                  )}
                </TabsContent>
              </Tabs>
            </div>
          </div>
        </main>
      </div>
    );
  }

  // Fallback loading state if authenticated but data not yet loaded into state
  // This might occur briefly between isSessionReady=true and sdkUserData being set.
  return (
    <div className="flex items-center justify-center min-h-screen bg-gradient-to-br from-[#0A1128] via-[#1B3A6B] to-[#16213E] text-white">
      Loading authenticated session data...
    </div>
  );

  // --- Original Missing API Key Fallback (Removed as auth check handles this now) ---
  /* REMOVED THIS ENTIRE BLOCK
   else {
     // This case should ideally not be reached if the redirect logic works
     return (
       <div className="flex min-h-screen w-full flex-col items-center justify-center relative transition-colors duration-300">
         <div className="absolute top-4 right-4">
           <ThemeToggle />
         </div>
         <div className="mb-6 inline-block p-6 rounded-full bg-indigo-900 bg-opacity-50">
           <svg
             className="w-12 h-12 text-indigo-400"
                fill="none"
                stroke="currentColor"
                viewBox="0 0 24 24"
                xmlns="http://www.w3.org/2000/svg"
              >
                <path
                  strokeLinecap="round"
                  strokeLinejoin="round"
                  strokeWidth={2}
                  d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z"
                />
              </svg>
            </div>
            <h2 className="text-2xl font-bold mb-2">Missing API Key</h2>
            <p className="text-gray-300 max-w-md mx-auto mb-6">
              To use the AI Visual Studio, you need to add a Fal.ai API key in
              your settings.
            </p>
            <div className="max-w-md mx-auto bg-gray-800 p-4 rounded-lg">
              <h3 className="text-lg font-semibold mb-2">
                How to add your API key:
              </h3>
              <ol className="text-left text-gray-300 list-decimal pl-5 space-y-2">
                <li>Go back to the writing workspace</li>
                <li>Click on the Settings icon or menu</li>
                <li>Enter your Fal.ai API key in the appropriate field</li>
                <li>Save your settings</li>
                <li>Return to AI Visual Studio</li>
              </ol>
            </div>
          </div>
        )}
      </main>
    </div>
   );
  }
  */ // END REMOVED BLOCK
}

================
File: src/app/session-manager.ts
================
/**
 * Session Manager for AI Video SDK
 * Handles storing and managing user session data received via postMessage
 */

// Define the type for user data
export interface UserData {
  // Auth Status
  isAuthenticated: boolean; // ADDED
  userId: string | null; // ADDED

  // API keys
  openaiApiKey?: string;
  openrouterApiKey?: string;
  falaiApiKey?: string;

  // Story elements
  language?: string;
  genre?: string;
  title?: string;
  location?: string;
  timeline?: string;

  // Characters and chapters
  characters?: any[];
  outline?: any[];
  chapters?: {
    number: string;
    title: string;
    content: string;
  }[];
}

// New interface for the data format coming from the parent
export interface ParentPostMessageData {
  apiKeys?: {
    openrouter?: string;
    openai?: string;
    falai?: string;
  };
  language?: string;
  genre?: string;
  title?: string;
  location?: string;
  timeline?: string;
  characters?: any[];
  outline?: any[];
  chapters?: any[];
}

// Define the global session state
class SessionManager {
  private userData: UserData | null = null;
  private initialized: boolean = false;
  private isAuthenticated: boolean = false; // ADDED
  private userId: string | null = null; // ADDED

  /**
   * Initialize the SDK session with user data
   * @param userData The user data to initialize with
   */
  initializeSession(userData: Record<string, any>): void {
    console.log("SDK receiving session data:", userData);

    // Store auth status
    this.isAuthenticated = userData.isAuthenticated ?? false; // ADDED
    this.userId = userData.userId ?? null; // ADDED
    console.log(
      `SDK Auth Status Received: isAuthenticated=${this.isAuthenticated}, userId=${this.userId}`,
    );

    // Process API keys from different message formats
    let apiKeys: Record<string, string> = {};

    // Process direct keys in the userData object
    if (userData.apiKeys) {
      apiKeys = userData.apiKeys;
    }

    // Extract the falai_key and save it to localStorage immediately
    const falaiKey = apiKeys.falai || userData.falai_key || "";
    if (falaiKey) {
      console.log(
        "SDK: Found falai_key in session data, saving to localStorage",
      );
      this.saveFalApiKey(falaiKey);
    } else {
      console.warn("SDK: No falai_key found in session data");
    }

    // Set user data with properly formatted chapters
    this.userData = {
      isAuthenticated: this.isAuthenticated, // Store in userData object as well
      userId: this.userId, // Store in userData object as well
      openaiApiKey: apiKeys.openai || userData.openai_key || "",
      openrouterApiKey: apiKeys.openrouter || userData.openrouter_key || "",
      falaiApiKey: falaiKey,

      language: userData.language || "",
      genre: userData.genre || "",
      title: userData.title || "",
      location: userData.location || "",
      timeline: userData.timeline || "",

      characters: userData.characters || [],
      outline: userData.outline || [],
      chapters: userData.chapters || [],
    };

    this.initialized = true;
    console.log("Session initialized successfully with data from parent");
  }

  /**
   * Store user data in the session
   * @param data The user data to store
   */
  setUserData(data: UserData): void {
    this.userData = data;
    this.initialized = true;
    console.log("Session data initialized:", this.userData);

    // Save API keys for later use
    if (data.falaiApiKey) {
      this.saveFalApiKey(data.falaiApiKey);
    }
  }

  /**
   * Get the current user data
   * @returns The user data or null if not initialized
   */
  getUserData(): UserData | null {
    return this.userData;
  }

  /**
   * Check if the session has been initialized
   * @returns True if the session has been initialized
   */
  isInitialized(): boolean {
    return this.initialized;
  }

  /** // ADDED Getters for Auth Status
   * Get the authentication status received from the parent.
   * @returns True if the parent indicated the user is authenticated.
   */
  getIsAuthenticated(): boolean {
    return this.isAuthenticated;
  }

  /**
   * Get the user ID received from the parent.
   * @returns The Firebase User ID string or null.
   */
  getUserId(): string | null {
    return this.userId;
  }
  // END ADDED Getters

  /**
   * Get a specific API key
   * @param keyType The type of API key to get
   * @returns The API key or an empty string if not found
   */
  getApiKey(keyType: "openai" | "openrouter" | "falai"): string {
    if (!this.userData) return "";

    switch (keyType) {
      case "openai":
        return this.userData.openaiApiKey || "";
      case "openrouter":
        return this.userData.openrouterApiKey || "";
      case "falai":
        return this.userData.falaiApiKey || "";
      default:
        return "";
    }
  }

  /**
   * Save the fal.ai API key to the SDK's storage
   * @param apiKey The API key to save
   */
  saveFalApiKey(apiKey: string): void {
    try {
      console.log(
        "SDK: Attempting to save falai_key to localStorage:",
        apiKey.substring(0, 5) + "...",
      );

      // Save to window.localStorage using the same key name as settings.js
      if (typeof window !== "undefined" && window.localStorage) {
        window.localStorage.setItem("falai_key", apiKey);
        console.log("SDK: Fal.ai API key stored as 'falai_key'");

        // Verify the key was saved correctly
        const savedKey = window.localStorage.getItem("falai_key");
        if (savedKey) {
          console.log(
            "SDK: Verified falai_key was saved correctly:",
            savedKey.substring(0, 5) + "...",
          );
        } else {
          console.error("SDK: Failed to verify falai_key in localStorage");
        }
      } else {
        console.error("SDK: window.localStorage is not available");
      }
    } catch (error) {
      console.error("SDK: Failed to save Fal.ai API key:", error);
    }
  }

  /**
   * Get the story characters
   * @returns The characters array or an empty array if not initialized
   */
  getCharacters(): any[] {
    return this.userData?.characters || [];
  }

  /**
   * Get the story chapters
   * @returns The chapters array or an empty array if not initialized
   */
  getChapters(): any[] {
    return this.userData?.chapters || [];
  }

  /**
   * Get the story outline
   * @returns The outline array or an empty array if not initialized
   */
  getOutline(): any[] {
    return this.userData?.outline || [];
  }

  /**
   * Get the story genre
   * @returns The genre or an empty string if not initialized
   */
  getGenre(): string {
    return this.userData?.genre || "";
  }

  /**
   * Get the story title
   * @returns The title or an empty string if not initialized
   */
  getTitle(): string {
    return this.userData?.title || "";
  }
}

// Create a singleton instance
const sessionManager = new SessionManager();

export default sessionManager;

================
File: src/components/playht/voice-selector.tsx
================
import { CheckIcon, ChevronsUpDownIcon } from "lucide-react";

import { cn } from "@/lib/utils";
import { Button, ButtonProps } from "@/components/ui/button";
import {
  Command,
  CommandEmpty,
  CommandGroup,
  CommandInput,
  CommandItem,
  CommandList,
} from "@/components/ui/command";
import {
  Popover,
  PopoverContent,
  PopoverTrigger,
} from "@/components/ui/popover";
import { useState } from "react";

const voices = [
  "Jennifer (English (US)/American)",
  "Dexter (English (US)/American)",
  "Ava (English (AU)/Australian)",
  "Tilly (English (AU)/Australian)",
  "Charlotte (Advertising) (English (CA)/Canadian)",
  "Charlotte (Meditation) (English (CA)/Canadian)",
  "Cecil (English (GB)/British)",
  "Sterling (English (GB)/British)",
  "Cillian (English (IE)/Irish)",
  "Madison (English (IE)/Irish)",
  "Ada (English (ZA)/South african)",
  "Furio (English (IT)/Italian)",
  "Alessandro (English (IT)/Italian)",
  "Carmen (English (MX)/Mexican)",
  "Sumita (English (IN)/Indian)",
  "Navya (English (IN)/Indian)",
  "Baptiste (English (FR)/French)",
  "Lumi (English (FI)/Finnish)",
  "Ronel Conversational (Afrikaans/South african)",
  "Ronel Narrative (Afrikaans/South african)",
  "Abdo Conversational (Arabic/Arabic)",
  "Abdo Narrative (Arabic/Arabic)",
  "Mousmi Conversational (Bengali/Bengali)",
  "Mousmi Narrative (Bengali/Bengali)",
  "Caroline Conversational (Portuguese (BR)/Brazilian)",
  "Caroline Narrative (Portuguese (BR)/Brazilian)",
  "Ange Conversational (French/French)",
  "Ange Narrative (French/French)",
  "Anke Conversational (German/German)",
  "Anke Narrative (German/German)",
  "Bora Conversational (Greek/Greek)",
  "Bora Narrative (Greek/Greek)",
  "Anuj Conversational (Hindi/Indian)",
  "Anuj Narrative (Hindi/Indian)",
  "Alessandro Conversational (Italian/Italian)",
  "Alessandro Narrative (Italian/Italian)",
  "Kiriko Conversational (Japanese/Japanese)",
  "Kiriko Narrative (Japanese/Japanese)",
  "Dohee Conversational (Korean/Korean)",
  "Dohee Narrative (Korean/Korean)",
  "Ignatius Conversational (Malay/Malay)",
  "Ignatius Narrative (Malay/Malay)",
  "Adam Conversational (Polish/Polish)",
  "Adam Narrative (Polish/Polish)",
  "Andrei Conversational (Russian/Russian)",
  "Andrei Narrative (Russian/Russian)",
  "Aleksa Conversational (Serbian/Serbian)",
  "Aleksa Narrative (Serbian/Serbian)",
  "Carmen Conversational (Spanish/Spanish)",
  "Patricia Conversational (Spanish/Spanish)",
  "Aiken Conversational (Tagalog/Filipino)",
  "Aiken Narrative (Tagalog/Filipino)",
  "Katbundit Conversational (Thai/Thai)",
  "Katbundit Narrative (Thai/Thai)",
  "Ali Conversational (Turkish/Turkish)",
  "Ali Narrative (Turkish/Turkish)",
  "Sahil Conversational (Urdu/Pakistani)",
  "Sahil Narrative (Urdu/Pakistani)",
  "Mary Conversational (Hebrew/Israeli)",
  "Mary Narrative (Hebrew/Israeli)",
];

type VoiceSelectorProps = {
  value: string;
  onValueChange: (value: string) => void;
} & ButtonProps;

export function VoiceSelector({
  value,
  onValueChange,
  className,
  ...props
}: VoiceSelectorProps) {
  const [open, setOpen] = useState(false);

  return (
    <Popover open={open} onOpenChange={setOpen} modal>
      <PopoverTrigger asChild>
        <Button
          variant="outline"
          {...props}
          className={cn("max-w-fit justify-between", className)}
          role="combobox"
          aria-expanded={open}
        >
          {value ? voices.find((voice) => voice === value) : "Select voice..."}
          <ChevronsUpDownIcon className="opacity-50" />
        </Button>
      </PopoverTrigger>
      <PopoverContent className="w-fit p-0" align="start">
        <Command>
          <CommandInput placeholder="Search voices..." className="h-9" />
          <CommandList className="overflow-y-scroll">
            <CommandEmpty>No voice found</CommandEmpty>
            <CommandGroup>
              {voices.map((voice) => (
                <CommandItem
                  key={voice}
                  value={voice}
                  onSelect={(currentValue) => {
                    onValueChange(currentValue);
                    setOpen(false);
                  }}
                >
                  {voice}
                  <CheckIcon
                    className={cn(
                      "ml-auto",
                      value === voice ? "visible" : "invisible",
                    )}
                  />
                </CommandItem>
              ))}
            </CommandGroup>
          </CommandList>
        </Command>
      </PopoverContent>
    </Popover>
  );
}

================
File: src/components/ui/accordion.tsx
================
"use client";

import * as AccordionPrimitive from "@radix-ui/react-accordion";
import { ChevronDown } from "lucide-react";
import * as React from "react";

import { cn } from "@/lib/utils";

const Accordion = AccordionPrimitive.Root;

const AccordionItem = React.forwardRef<
  React.ElementRef<typeof AccordionPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof AccordionPrimitive.Item>
>(({ className, ...props }, ref) => (
  <AccordionPrimitive.Item
    ref={ref}
    className={cn("border-b", className)}
    {...props}
  />
));
AccordionItem.displayName = "AccordionItem";

const AccordionTrigger = React.forwardRef<
  React.ElementRef<typeof AccordionPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof AccordionPrimitive.Trigger>
>(({ className, children, ...props }, ref) => (
  <AccordionPrimitive.Header className="flex">
    <AccordionPrimitive.Trigger
      ref={ref}
      className={cn(
        "flex flex-1 items-center justify-between py-4 text-sm font-medium transition-all hover:underline text-left [&[data-state=open]>svg]:rotate-180",
        className,
      )}
      {...props}
    >
      {children}
      <ChevronDown className="h-4 w-4 shrink-0 text-muted-foreground transition-transform duration-200" />
    </AccordionPrimitive.Trigger>
  </AccordionPrimitive.Header>
));
AccordionTrigger.displayName = AccordionPrimitive.Trigger.displayName;

const AccordionContent = React.forwardRef<
  React.ElementRef<typeof AccordionPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof AccordionPrimitive.Content>
>(({ className, children, ...props }, ref) => (
  <AccordionPrimitive.Content
    ref={ref}
    className="overflow-hidden text-sm data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down"
    {...props}
  >
    <div className={cn("pb-4 pt-0", className)}>{children}</div>
  </AccordionPrimitive.Content>
));
AccordionContent.displayName = AccordionPrimitive.Content.displayName;

export { Accordion, AccordionItem, AccordionTrigger, AccordionContent };

================
File: src/components/ui/badge.tsx
================
import { type VariantProps, cva } from "class-variance-authority";
import type * as React from "react";

import { cn } from "@/lib/utils";

const badgeVariants = cva(
  "inline-flex items-center rounded-md border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2",
  {
    variants: {
      variant: {
        default:
          "border-transparent bg-primary text-primary-foreground shadow hover:bg-primary/80",
        secondary:
          "border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80",
        destructive:
          "border-transparent bg-destructive text-destructive-foreground shadow hover:bg-destructive/80",
        outline: "text-foreground",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  },
);

export interface BadgeProps
  extends React.HTMLAttributes<HTMLDivElement>,
    VariantProps<typeof badgeVariants> {}

function Badge({ className, variant, ...props }: BadgeProps) {
  return (
    <div className={cn(badgeVariants({ variant }), className)} {...props} />
  );
}

export { Badge, badgeVariants };

================
File: src/components/ui/button.tsx
================
import { Slot } from "@radix-ui/react-slot";
import { type VariantProps, cva } from "class-variance-authority";
import * as React from "react";

import { cn } from "@/lib/utils";

const buttonVariants = cva(
  "inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-all duration-200 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:transform hover:-translate-y-0.5",
  {
    variants: {
      variant: {
        default: "bg-primary text-primary-foreground hover:bg-primary/90",
        destructive:
          "bg-destructive text-destructive-foreground hover:bg-destructive/90",
        outline:
          "border border-input bg-background hover:bg-accent hover:text-accent-foreground",
        secondary:
          "bg-secondary text-secondary-foreground hover:bg-secondary/80",
        ghost: "hover:bg-accent hover:text-accent-foreground",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        default: "h-10 px-4 py-2",
        sm: "h-9 rounded-md px-3",
        lg: "h-11 rounded-md px-8",
        icon: "h-10 w-10",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  },
);

export interface ButtonProps
  extends React.ButtonHTMLAttributes<HTMLButtonElement>,
    VariantProps<typeof buttonVariants> {
  asChild?: boolean;
}

const Button = React.forwardRef<HTMLButtonElement, ButtonProps>(
  ({ className, variant, size, asChild = false, ...props }, ref) => {
    const Comp = asChild ? Slot : "button";
    return (
      <Comp
        className={cn(buttonVariants({ variant, size, className }))}
        ref={ref}
        {...props}
      />
    );
  },
);
Button.displayName = "Button";

export { Button, buttonVariants };

================
File: src/components/ui/collapsible.tsx
================
"use client";

import * as CollapsiblePrimitive from "@radix-ui/react-collapsible";

const Collapsible = CollapsiblePrimitive.Root;

const CollapsibleTrigger = CollapsiblePrimitive.CollapsibleTrigger;

const CollapsibleContent = CollapsiblePrimitive.CollapsibleContent;

export { Collapsible, CollapsibleTrigger, CollapsibleContent };

================
File: src/components/ui/command.tsx
================
"use client";

import * as React from "react";
import { type DialogProps } from "@radix-ui/react-dialog";
import { Command as CommandPrimitive } from "cmdk";
import { Search } from "lucide-react";

import { cn } from "@/lib/utils";
import { Dialog, DialogContent } from "@/components/ui/dialog";

const Command = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive>
>(({ className, ...props }, ref) => (
  <CommandPrimitive
    ref={ref}
    className={cn(
      "flex h-full w-full flex-col overflow-hidden rounded-md bg-popover text-popover-foreground",
      className,
    )}
    {...props}
  />
));
Command.displayName = CommandPrimitive.displayName;

const CommandDialog = ({ children, ...props }: DialogProps) => {
  return (
    <Dialog {...props}>
      <DialogContent className="overflow-hidden p-0">
        <Command className="[&_[cmdk-group-heading]]:px-2 [&_[cmdk-group-heading]]:font-medium [&_[cmdk-group-heading]]:text-muted-foreground [&_[cmdk-group]:not([hidden])_~[cmdk-group]]:pt-0 [&_[cmdk-group]]:px-2 [&_[cmdk-input-wrapper]_svg]:h-5 [&_[cmdk-input-wrapper]_svg]:w-5 [&_[cmdk-input]]:h-12 [&_[cmdk-item]]:px-2 [&_[cmdk-item]]:py-3 [&_[cmdk-item]_svg]:h-5 [&_[cmdk-item]_svg]:w-5">
          {children}
        </Command>
      </DialogContent>
    </Dialog>
  );
};

const CommandInput = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive.Input>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive.Input>
>(({ className, ...props }, ref) => (
  <div className="flex items-center border-b px-3" cmdk-input-wrapper="">
    <Search className="mr-2 h-4 w-4 shrink-0 opacity-50" />
    <CommandPrimitive.Input
      ref={ref}
      className={cn(
        "flex h-10 w-full rounded-md bg-transparent py-3 text-sm outline-none placeholder:text-muted-foreground disabled:cursor-not-allowed disabled:opacity-50",
        className,
      )}
      {...props}
    />
  </div>
));

CommandInput.displayName = CommandPrimitive.Input.displayName;

const CommandList = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive.List>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive.List>
>(({ className, ...props }, ref) => (
  <CommandPrimitive.List
    ref={ref}
    className={cn("max-h-[300px] overflow-y-auto overflow-x-hidden", className)}
    {...props}
  />
));

CommandList.displayName = CommandPrimitive.List.displayName;

const CommandEmpty = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive.Empty>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive.Empty>
>((props, ref) => (
  <CommandPrimitive.Empty
    ref={ref}
    className="py-6 text-center text-sm"
    {...props}
  />
));

CommandEmpty.displayName = CommandPrimitive.Empty.displayName;

const CommandGroup = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive.Group>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive.Group>
>(({ className, ...props }, ref) => (
  <CommandPrimitive.Group
    ref={ref}
    className={cn(
      "overflow-hidden p-1 text-foreground [&_[cmdk-group-heading]]:px-2 [&_[cmdk-group-heading]]:py-1.5 [&_[cmdk-group-heading]]:text-xs [&_[cmdk-group-heading]]:font-medium [&_[cmdk-group-heading]]:text-muted-foreground",
      className,
    )}
    {...props}
  />
));

CommandGroup.displayName = CommandPrimitive.Group.displayName;

const CommandSeparator = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <CommandPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 h-px bg-border", className)}
    {...props}
  />
));
CommandSeparator.displayName = CommandPrimitive.Separator.displayName;

const CommandItem = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive.Item>
>(({ className, ...props }, ref) => (
  <CommandPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex cursor-default gap-2 select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none data-[disabled=true]:pointer-events-none data-[selected=true]:bg-accent data-[selected=true]:text-accent-foreground data-[disabled=true]:opacity-50 [&_svg]:pointer-events-none [&_svg]:size-4 [&_svg]:shrink-0",
      className,
    )}
    {...props}
  />
));

CommandItem.displayName = CommandPrimitive.Item.displayName;

const CommandShortcut = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLSpanElement>) => {
  return (
    <span
      className={cn(
        "ml-auto text-xs tracking-widest text-muted-foreground",
        className,
      )}
      {...props}
    />
  );
};
CommandShortcut.displayName = "CommandShortcut";

export {
  Command,
  CommandDialog,
  CommandInput,
  CommandList,
  CommandEmpty,
  CommandGroup,
  CommandItem,
  CommandShortcut,
  CommandSeparator,
};

================
File: src/components/ui/dialog.tsx
================
"use client";

import * as DialogPrimitive from "@radix-ui/react-dialog";
import { X } from "lucide-react";
import * as React from "react";

import { cn } from "@/lib/utils";

const Dialog = DialogPrimitive.Root;

const DialogTrigger = DialogPrimitive.Trigger;

const DialogPortal = DialogPrimitive.Portal;

const DialogClose = DialogPrimitive.Close;

const DialogOverlay = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Overlay>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Overlay>
>(({ className, ...props }, ref) => (
  <DialogPrimitive.Overlay
    ref={ref}
    className={cn(
      "fixed inset-0 z-50 bg-black/80 data-[state=open]:animate-in",
      "data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0",
      "backdrop-blur-sm",
      className,
    )}
    {...props}
  />
));
DialogOverlay.displayName = DialogPrimitive.Overlay.displayName;

const DialogContent = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Content>
>(({ className, children, ...props }, ref) => (
  <DialogPortal>
    <DialogOverlay />
    <DialogPrimitive.Content
      ref={ref}
      className={cn(
        "fixed left-[50%] top-[50%] z-50 grid w-full max-w-lg translate-x-[-50%] translate-y-[-50%] gap-4 border bg-background p-6 shadow-lg duration-200 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[state=closed]:slide-out-to-left-1/2 data-[state=closed]:slide-out-to-top-[48%] data-[state=open]:slide-in-from-left-1/2 data-[state=open]:slide-in-from-top-[48%] sm:rounded-lg",
        className,
      )}
      {...props}
    >
      {children}
      <DialogPrimitive.Close className="absolute right-4 top-4 rounded-sm opacity-70 ring-offset-background transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-ring/20 focus:ring-offset-2 disabled:pointer-events-none data-[state=open]:bg-accent data-[state=open]:text-muted-foreground">
        <X className="h-4 w-4" />
        <span className="sr-only">Close</span>
      </DialogPrimitive.Close>
    </DialogPrimitive.Content>
  </DialogPortal>
));
DialogContent.displayName = DialogPrimitive.Content.displayName;

const DialogHeader = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col space-y-1.5 text-center sm:text-left",
      className,
    )}
    {...props}
  />
);
DialogHeader.displayName = "DialogHeader";

const DialogFooter = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col-reverse sm:flex-row sm:justify-end sm:space-x-2",
      className,
    )}
    {...props}
  />
);
DialogFooter.displayName = "DialogFooter";

const DialogTitle = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Title>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Title>
>(({ className, ...props }, ref) => (
  <DialogPrimitive.Title
    ref={ref}
    className={cn(
      "text-lg font-semibold leading-none tracking-tight",
      className,
    )}
    {...props}
  />
));
DialogTitle.displayName = DialogPrimitive.Title.displayName;

const DialogDescription = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Description>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Description>
>(({ className, ...props }, ref) => (
  <DialogPrimitive.Description
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
));
DialogDescription.displayName = DialogPrimitive.Description.displayName;

export {
  Dialog,
  DialogPortal,
  DialogOverlay,
  DialogTrigger,
  DialogClose,
  DialogContent,
  DialogHeader,
  DialogFooter,
  DialogTitle,
  DialogDescription,
};

================
File: src/components/ui/download-button.tsx
================
import React from "react";
import { Button } from "./button";

interface DownloadButtonProps {
  imageUrl: string;
  filename: string;
  className?: string;
  variant?:
    | "default"
    | "destructive"
    | "outline"
    | "secondary"
    | "ghost"
    | "link";
  size?: "default" | "sm" | "lg" | "icon";
  buttonText?: string;
}

export function DownloadButton({
  imageUrl,
  filename,
  className = "w-full",
  variant = "outline",
  size = "sm",
  buttonText = "Download Image",
}: DownloadButtonProps) {
  const handleDownload = async () => {
    try {
      // Fetch the image as a blob
      const response = await fetch(imageUrl);
      const blob = await response.blob();

      // Create a blob URL and use it to trigger a proper save dialog
      const blobUrl = window.URL.createObjectURL(blob);

      // Create a temporary link element
      const a = document.createElement("a");
      a.href = blobUrl;
      a.download = filename;

      // This is important to trigger the actual download dialog on the PC
      document.body.appendChild(a);
      a.click();

      // Clean up
      window.URL.revokeObjectURL(blobUrl);
      document.body.removeChild(a);
    } catch (error) {
      console.error("Error downloading image:", error);
    }
  };

  return (
    <Button
      variant={variant}
      size={size}
      className={className}
      onClick={handleDownload}
    >
      <span className="flex items-center">
        <svg
          xmlns="http://www.w3.org/2000/svg"
          width="14"
          height="14"
          viewBox="0 0 24 24"
          fill="none"
          stroke="currentColor"
          strokeWidth="2"
          strokeLinecap="round"
          strokeLinejoin="round"
          className="mr-1.5"
        >
          <path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4" />
          <polyline points="7 10 12 15 17 10" />
          <line x1="12" y1="15" x2="12" y2="3" />
        </svg>
        {buttonText}
      </span>
    </Button>
  );
}

================
File: src/components/ui/dropdown-menu.tsx
================
"use client";

import * as DropdownMenuPrimitive from "@radix-ui/react-dropdown-menu";
import { Check, ChevronRight, Circle } from "lucide-react";
import * as React from "react";

import { cn } from "@/lib/utils";

const DropdownMenu = DropdownMenuPrimitive.Root;

const DropdownMenuTrigger = DropdownMenuPrimitive.Trigger;

const DropdownMenuGroup = DropdownMenuPrimitive.Group;

const DropdownMenuPortal = DropdownMenuPrimitive.Portal;

const DropdownMenuSub = DropdownMenuPrimitive.Sub;

const DropdownMenuRadioGroup = DropdownMenuPrimitive.RadioGroup;

const DropdownMenuSubTrigger = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubTrigger>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubTrigger> & {
    inset?: boolean;
  }
>(({ className, inset, children, ...props }, ref) => (
  <DropdownMenuPrimitive.SubTrigger
    ref={ref}
    className={cn(
      "flex cursor-default gap-2 select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none focus:bg-accent data-[state=open]:bg-accent [&_svg]:pointer-events-none [&_svg]:size-4 [&_svg]:shrink-0",
      inset && "pl-8",
      className,
    )}
    {...props}
  >
    {children}
    <ChevronRight className="ml-auto" />
  </DropdownMenuPrimitive.SubTrigger>
));
DropdownMenuSubTrigger.displayName =
  DropdownMenuPrimitive.SubTrigger.displayName;

const DropdownMenuSubContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubContent>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubContent>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.SubContent
    ref={ref}
    className={cn(
      "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-lg data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
      className,
    )}
    {...props}
  />
));
DropdownMenuSubContent.displayName =
  DropdownMenuPrimitive.SubContent.displayName;

const DropdownMenuContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <DropdownMenuPrimitive.Portal>
    <DropdownMenuPrimitive.Content
      ref={ref}
      sideOffset={sideOffset}
      className={cn(
        "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-md",
        "data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        className,
      )}
      {...props}
    />
  </DropdownMenuPrimitive.Portal>
));
DropdownMenuContent.displayName = DropdownMenuPrimitive.Content.displayName;

const DropdownMenuItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Item> & {
    inset?: boolean;
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center gap-2 rounded-sm px-2 py-1.5 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&>svg]:size-4 [&>svg]:shrink-0",
      inset && "pl-8",
      className,
    )}
    {...props}
  />
));
DropdownMenuItem.displayName = DropdownMenuPrimitive.Item.displayName;

const DropdownMenuCheckboxItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.CheckboxItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.CheckboxItem>
>(({ className, children, checked, ...props }, ref) => (
  <DropdownMenuPrimitive.CheckboxItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className,
    )}
    checked={checked}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <Check className="h-4 w-4" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.CheckboxItem>
));
DropdownMenuCheckboxItem.displayName =
  DropdownMenuPrimitive.CheckboxItem.displayName;

const DropdownMenuRadioItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.RadioItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.RadioItem>
>(({ className, children, ...props }, ref) => (
  <DropdownMenuPrimitive.RadioItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className,
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <Circle className="h-2 w-2 fill-current" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.RadioItem>
));
DropdownMenuRadioItem.displayName = DropdownMenuPrimitive.RadioItem.displayName;

const DropdownMenuLabel = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Label> & {
    inset?: boolean;
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Label
    ref={ref}
    className={cn(
      "px-2 py-1.5 text-sm font-semibold",
      inset && "pl-8",
      className,
    )}
    {...props}
  />
));
DropdownMenuLabel.displayName = DropdownMenuPrimitive.Label.displayName;

const DropdownMenuSeparator = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
));
DropdownMenuSeparator.displayName = DropdownMenuPrimitive.Separator.displayName;

const DropdownMenuShortcut = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLSpanElement>) => {
  return (
    <span
      className={cn("ml-auto text-sm tracking-widest opacity-60", className)}
      {...props}
    />
  );
};
DropdownMenuShortcut.displayName = "DropdownMenuShortcut";

export {
  DropdownMenu,
  DropdownMenuTrigger,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuCheckboxItem,
  DropdownMenuRadioItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuShortcut,
  DropdownMenuGroup,
  DropdownMenuPortal,
  DropdownMenuSub,
  DropdownMenuSubContent,
  DropdownMenuSubTrigger,
  DropdownMenuRadioGroup,
};

================
File: src/components/ui/form.tsx
================
"use client";

import type * as LabelPrimitive from "@radix-ui/react-label";
import { Slot } from "@radix-ui/react-slot";
import * as React from "react";
import {
  Controller,
  type ControllerProps,
  type FieldPath,
  type FieldValues,
  FormProvider,
  useFormContext,
} from "react-hook-form";

import { Label } from "@/components/ui/label";
import { cn } from "@/lib/utils";

const Form = FormProvider;

type FormFieldContextValue<
  TFieldValues extends FieldValues = FieldValues,
  TName extends FieldPath<TFieldValues> = FieldPath<TFieldValues>,
> = {
  name: TName;
};

const FormFieldContext = React.createContext<FormFieldContextValue>(
  {} as FormFieldContextValue,
);

const FormField = <
  TFieldValues extends FieldValues = FieldValues,
  TName extends FieldPath<TFieldValues> = FieldPath<TFieldValues>,
>({
  ...props
}: ControllerProps<TFieldValues, TName>) => {
  return (
    <FormFieldContext.Provider value={{ name: props.name }}>
      <Controller {...props} />
    </FormFieldContext.Provider>
  );
};

const useFormField = () => {
  const fieldContext = React.useContext(FormFieldContext);
  const itemContext = React.useContext(FormItemContext);
  const { getFieldState, formState } = useFormContext();

  const fieldState = getFieldState(fieldContext.name, formState);

  if (!fieldContext) {
    throw new Error("useFormField should be used within <FormField>");
  }

  const { id } = itemContext;

  return {
    id,
    name: fieldContext.name,
    formItemId: `${id}-form-item`,
    formDescriptionId: `${id}-form-item-description`,
    formMessageId: `${id}-form-item-message`,
    ...fieldState,
  };
};

type FormItemContextValue = {
  id: string;
};

const FormItemContext = React.createContext<FormItemContextValue>(
  {} as FormItemContextValue,
);

const FormItem = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => {
  const id = React.useId();

  return (
    <FormItemContext.Provider value={{ id }}>
      <div ref={ref} className={cn("space-y-2", className)} {...props} />
    </FormItemContext.Provider>
  );
});
FormItem.displayName = "FormItem";

const FormLabel = React.forwardRef<
  React.ElementRef<typeof LabelPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof LabelPrimitive.Root>
>(({ className, ...props }, ref) => {
  const { error, formItemId } = useFormField();

  return (
    <Label
      ref={ref}
      className={cn(error && "text-destructive", className)}
      htmlFor={formItemId}
      {...props}
    />
  );
});
FormLabel.displayName = "FormLabel";

const FormControl = React.forwardRef<
  React.ElementRef<typeof Slot>,
  React.ComponentPropsWithoutRef<typeof Slot>
>(({ ...props }, ref) => {
  const { error, formItemId, formDescriptionId, formMessageId } =
    useFormField();

  return (
    <Slot
      ref={ref}
      id={formItemId}
      aria-describedby={
        !error
          ? `${formDescriptionId}`
          : `${formDescriptionId} ${formMessageId}`
      }
      aria-invalid={!!error}
      {...props}
    />
  );
});
FormControl.displayName = "FormControl";

const FormDescription = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLParagraphElement>
>(({ className, ...props }, ref) => {
  const { formDescriptionId } = useFormField();

  return (
    <p
      ref={ref}
      id={formDescriptionId}
      className={cn("text-[0.8rem] text-muted-foreground", className)}
      {...props}
    />
  );
});
FormDescription.displayName = "FormDescription";

const FormMessage = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLParagraphElement>
>(({ className, children, ...props }, ref) => {
  const { error, formMessageId } = useFormField();
  const body = error ? String(error?.message) : children;

  if (!body) {
    return null;
  }

  return (
    <p
      ref={ref}
      id={formMessageId}
      className={cn("text-[0.8rem] font-medium text-destructive", className)}
      {...props}
    >
      {body}
    </p>
  );
});
FormMessage.displayName = "FormMessage";

export {
  useFormField,
  Form,
  FormItem,
  FormLabel,
  FormControl,
  FormDescription,
  FormMessage,
  FormField,
};

================
File: src/components/ui/icons.tsx
================
import { cn } from "@/lib/utils";
import { LoaderCircleIcon } from "lucide-react";

type LoadingIconProps = Parameters<typeof LoaderCircleIcon>[0];

export function LoadingIcon({ className, ...props }: LoadingIconProps) {
  return (
    <LoaderCircleIcon
      className={cn("opacity-50 animate-spin", className)}
      {...props}
    />
  );
}

================
File: src/components/ui/input.tsx
================
import * as React from "react";

import { cn } from "@/lib/utils";

const Input = React.forwardRef<HTMLInputElement, React.ComponentProps<"input">>(
  ({ className, type, ...props }, ref) => {
    return (
      <input
        type={type}
        className={cn(
          "flex h-8 w-full rounded-md border border-transparent bg-muted/50 focus:bg-muted/70 px-2 py-1 text-sm",
          "transition-colors file:border-0 file:bg-transparent file:text-sm file:font-medium file:text-foreground",
          "placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring/20",
          "disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",
          className,
        )}
        ref={ref}
        {...props}
      />
    );
  },
);
Input.displayName = "Input";

export { Input };

================
File: src/components/ui/label.tsx
================
"use client";

import * as LabelPrimitive from "@radix-ui/react-label";
import { type VariantProps, cva } from "class-variance-authority";
import * as React from "react";

import { cn } from "@/lib/utils";

const labelVariants = cva(
  "text-sm font-medium leading-none peer-disabled:cursor-not-allowed peer-disabled:opacity-70",
);

const Label = React.forwardRef<
  React.ElementRef<typeof LabelPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof LabelPrimitive.Root> &
    VariantProps<typeof labelVariants>
>(({ className, ...props }, ref) => (
  <LabelPrimitive.Root
    ref={ref}
    className={cn(labelVariants(), className)}
    {...props}
  />
));
Label.displayName = LabelPrimitive.Root.displayName;

export { Label };

================
File: src/components/ui/landing-laptop-mockup.tsx
================
import type React from "react";

export function LaptopMockup({ children }: { children: React.ReactNode }) {
  return (
    <div className="relative w-full border border-border rounded-xl ">
      {/* Gradient border effect */}
      <div className="absolute top-0 -left-4 -right-4 h-[1px] bg-gradient-to-r from-transparent via-white/15 to-transparent" />
      <div className="absolute -top-4 -bottom-4 left-0 w-[1px] bg-gradient-to-b from-transparent via-white/15 to-transparent" />
      <div className="absolute -top-4 -bottom-4 right-0 w-[1px] bg-gradient-to-b from-transparent via-white/15 to-transparent" />
      <div className="absolute bottom-0 -left-4 -right-4 h-[1px] bg-gradient-to-r from-transparent via-white/15 to-transparent" />

      {/* MacBook-style frame */}
      <div className="relative rounded-xl overflow-hidden bg-gray-900 shadow-2xl">
        {/* MacBook top bar */}
        <div className="relative h-8 bg-neutral-800 backdrop-blur-sm flex items-center px-4">
          <div className="flex space-x-2">
            <div className="w-3 h-3 rounded-full bg-red-500/80" />
            <div className="w-3 h-3 rounded-full bg-yellow-500/80" />
            <div className="w-3 h-3 rounded-full bg-green-500/80" />
          </div>
        </div>
        {/* Content */}
        <div className="relative">{children}</div>
      </div>

      {/* Reflection effect */}
      <div className="absolute inset-0 rounded-xl bg-gradient-to-b from-white/5 to-transparent opacity-0 group-hover:opacity-100 transition-opacity pointer-events-none" />
    </div>
  );
}

================
File: src/components/ui/popover.tsx
================
"use client";

import * as PopoverPrimitive from "@radix-ui/react-popover";
import * as React from "react";

import { cn } from "@/lib/utils";

const Popover = PopoverPrimitive.Root;

const PopoverTrigger = PopoverPrimitive.Trigger;

const PopoverAnchor = PopoverPrimitive.Anchor;

const PopoverContent = React.forwardRef<
  React.ElementRef<typeof PopoverPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof PopoverPrimitive.Content>
>(({ className, align = "center", sideOffset = 4, ...props }, ref) => (
  <PopoverPrimitive.Portal>
    <PopoverPrimitive.Content
      ref={ref}
      align={align}
      sideOffset={sideOffset}
      className={cn(
        "z-50 w-72 rounded-md border bg-popover p-4 text-popover-foreground shadow-md outline-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        className,
      )}
      {...props}
    />
  </PopoverPrimitive.Portal>
));
PopoverContent.displayName = PopoverPrimitive.Content.displayName;

export { Popover, PopoverTrigger, PopoverContent, PopoverAnchor };

================
File: src/components/ui/select.tsx
================
"use client";

import * as SelectPrimitive from "@radix-ui/react-select";
import { Check, ChevronDown, ChevronUp } from "lucide-react";
import * as React from "react";

import { cn } from "@/lib/utils";

const Select = SelectPrimitive.Root;

const SelectGroup = SelectPrimitive.Group;

const SelectValue = SelectPrimitive.Value;

const SelectTrigger = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Trigger>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Trigger
    ref={ref}
    className={cn(
      "flex h-9 w-full items-center justify-between whitespace-nowrap rounded-md border border-input bg-transparent px-3 py-2 text-sm shadow-sm ring-offset-background placeholder:text-muted-foreground focus:outline-none focus:ring-1 focus:ring-ring/20 disabled:cursor-not-allowed disabled:opacity-50 [&>span]:line-clamp-1",
      className,
    )}
    {...props}
  >
    {children}
    <SelectPrimitive.Icon asChild>
      <ChevronDown className="h-4 w-4 opacity-50" />
    </SelectPrimitive.Icon>
  </SelectPrimitive.Trigger>
));
SelectTrigger.displayName = SelectPrimitive.Trigger.displayName;

const SelectScrollUpButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollUpButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollUpButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollUpButton
    ref={ref}
    className={cn(
      "flex cursor-default items-center justify-center py-1",
      className,
    )}
    {...props}
  >
    <ChevronUp className="h-4 w-4" />
  </SelectPrimitive.ScrollUpButton>
));
SelectScrollUpButton.displayName = SelectPrimitive.ScrollUpButton.displayName;

const SelectScrollDownButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollDownButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollDownButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollDownButton
    ref={ref}
    className={cn(
      "flex cursor-default items-center justify-center py-1",
      className,
    )}
    {...props}
  >
    <ChevronDown className="h-4 w-4" />
  </SelectPrimitive.ScrollDownButton>
));
SelectScrollDownButton.displayName =
  SelectPrimitive.ScrollDownButton.displayName;

const SelectContent = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Content>
>(({ className, children, position = "popper", ...props }, ref) => (
  <SelectPrimitive.Portal>
    <SelectPrimitive.Content
      ref={ref}
      className={cn(
        "relative z-50 max-h-96 min-w-[8rem] overflow-hidden rounded-md border bg-popover text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        position === "popper" &&
          "data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",
        className,
      )}
      position={position}
      {...props}
    >
      <SelectScrollUpButton />
      <SelectPrimitive.Viewport
        className={cn(
          "p-1",
          position === "popper" &&
            "h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)]",
        )}
      >
        {children}
      </SelectPrimitive.Viewport>
      <SelectScrollDownButton />
    </SelectPrimitive.Content>
  </SelectPrimitive.Portal>
));
SelectContent.displayName = SelectPrimitive.Content.displayName;

const SelectLabel = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Label>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Label
    ref={ref}
    className={cn("px-2 py-1.5 text-sm font-semibold", className)}
    {...props}
  />
));
SelectLabel.displayName = SelectPrimitive.Label.displayName;

const SelectItem = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Item>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex w-full cursor-default select-none items-center rounded-sm py-1.5 pl-2 pr-8 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className,
    )}
    {...props}
  >
    <span className="absolute right-2 flex h-3.5 w-3.5 items-center justify-center">
      <SelectPrimitive.ItemIndicator>
        <Check className="h-4 w-4" />
      </SelectPrimitive.ItemIndicator>
    </span>
    <SelectPrimitive.ItemText>{children}</SelectPrimitive.ItemText>
  </SelectPrimitive.Item>
));
SelectItem.displayName = SelectPrimitive.Item.displayName;

const SelectSeparator = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
));
SelectSeparator.displayName = SelectPrimitive.Separator.displayName;

export {
  Select,
  SelectGroup,
  SelectValue,
  SelectTrigger,
  SelectContent,
  SelectLabel,
  SelectItem,
  SelectSeparator,
  SelectScrollUpButton,
  SelectScrollDownButton,
};

================
File: src/components/ui/separator.tsx
================
"use client";

import * as SeparatorPrimitive from "@radix-ui/react-separator";
import * as React from "react";

import { cn } from "@/lib/utils";

const Separator = React.forwardRef<
  React.ElementRef<typeof SeparatorPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof SeparatorPrimitive.Root>
>(
  (
    { className, orientation = "horizontal", decorative = true, ...props },
    ref,
  ) => (
    <SeparatorPrimitive.Root
      ref={ref}
      decorative={decorative}
      orientation={orientation}
      className={cn(
        "shrink-0 bg-border",
        orientation === "horizontal" ? "h-[1px] w-full" : "h-full w-px",
        className,
      )}
      {...props}
    />
  ),
);
Separator.displayName = SeparatorPrimitive.Root.displayName;

export { Separator };

================
File: src/components/ui/sheet.tsx
================
"use client";

import * as React from "react";
import * as SheetPrimitive from "@radix-ui/react-dialog";
import { cva, type VariantProps } from "class-variance-authority";
import { X } from "lucide-react";

import { cn } from "@/lib/utils";

const Sheet = SheetPrimitive.Root;

const SheetTrigger = SheetPrimitive.Trigger;

const SheetClose = SheetPrimitive.Close;

const SheetPortal = SheetPrimitive.Portal;

const SheetOverlay = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Overlay>,
  React.ComponentPropsWithoutRef<typeof SheetPrimitive.Overlay>
>(({ className, ...props }, ref) => (
  <SheetPrimitive.Overlay
    className={cn(
      "fixed inset-0 z-50 bg-black/80 data-[state=open]:animate-in",
      "data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0",
      "backdrop-blur-sm",
      className,
    )}
    {...props}
    ref={ref}
  />
));
SheetOverlay.displayName = SheetPrimitive.Overlay.displayName;

const sheetVariants = cva(
  "fixed z-50 gap-4 bg-background p-6 shadow-lg transition ease-in-out data-[state=closed]:duration-200 data-[state=open]:duration-300 data-[state=open]:animate-in data-[state=closed]:animate-out",
  {
    variants: {
      side: {
        top: "inset-x-0 top-0 border-b data-[state=closed]:slide-out-to-top data-[state=open]:slide-in-from-top",
        bottom:
          "inset-x-0 bottom-0 border-t data-[state=closed]:slide-out-to-bottom data-[state=open]:slide-in-from-bottom",
        left: "inset-y-0 left-0 h-full w-3/4 border-r data-[state=closed]:slide-out-to-left data-[state=open]:slide-in-from-left sm:max-w-sm",
        right:
          "inset-y-0 right-0 h-full w-3/4 border-l data-[state=closed]:slide-out-to-right data-[state=open]:slide-in-from-right sm:max-w-sm",
      },
    },
    defaultVariants: {
      side: "right",
    },
  },
);

const SheetPanel = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Content>,
  SheetContentProps
>(({ side = "right", className, children, ...props }, ref) => (
  <SheetPrimitive.Content
    ref={ref}
    className={cn(sheetVariants({ side }), className)}
    {...props}
  >
    {children}
    <SheetPrimitive.Close className="ring-offset-background focus:ring-ring absolute right-4 top-4 rounded-sm opacity-70 transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-offset-2 disabled:pointer-events-none data-[state=open]:bg-secondary">
      <X className="h-4 w-4" />
      <span className="sr-only">Close</span>
    </SheetPrimitive.Close>
  </SheetPrimitive.Content>
));
SheetPanel.displayName = "SheetPanel";

interface SheetContentProps
  extends React.ComponentPropsWithoutRef<typeof SheetPrimitive.Content>,
    VariantProps<typeof sheetVariants> {}

const SheetContent = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Content>,
  SheetContentProps
>(({ side = "right", className, children, ...props }, ref) => (
  <SheetPortal>
    <SheetOverlay />
    <SheetPanel
      ref={ref}
      side={side}
      className={cn(sheetVariants({ side }), className)}
      {...props}
    />
  </SheetPortal>
));
SheetContent.displayName = SheetPrimitive.Content.displayName;

const SheetHeader = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col space-y-2 text-center sm:text-left",
      className,
    )}
    {...props}
  />
);
SheetHeader.displayName = "SheetHeader";

const SheetFooter = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col-reverse sm:flex-row sm:justify-end sm:space-x-2",
      className,
    )}
    {...props}
  />
);
SheetFooter.displayName = "SheetFooter";

const SheetTitle = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Title>,
  React.ComponentPropsWithoutRef<typeof SheetPrimitive.Title>
>(({ className, ...props }, ref) => (
  <SheetPrimitive.Title
    ref={ref}
    className={cn("text-lg font-semibold text-foreground", className)}
    {...props}
  />
));
SheetTitle.displayName = SheetPrimitive.Title.displayName;

const SheetDescription = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Description>,
  React.ComponentPropsWithoutRef<typeof SheetPrimitive.Description>
>(({ className, ...props }, ref) => (
  <SheetPrimitive.Description
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
));
SheetDescription.displayName = SheetPrimitive.Description.displayName;

export {
  Sheet,
  SheetPortal,
  SheetOverlay,
  SheetTrigger,
  SheetClose,
  SheetContent,
  SheetHeader,
  SheetFooter,
  SheetTitle,
  SheetDescription,
  SheetPanel,
};

================
File: src/components/ui/skeleton.tsx
================
import { cn } from "@/lib/utils";

function Skeleton({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) {
  return (
    <div
      className={cn("animate-pulse rounded-md bg-primary/10", className)}
      {...props}
    />
  );
}

export { Skeleton };

================
File: src/components/ui/slider.tsx
================
"use client";

import * as React from "react";
import * as SliderPrimitive from "@radix-ui/react-slider";

import { cn } from "@/lib/utils";

const Slider = React.forwardRef<
  React.ElementRef<typeof SliderPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof SliderPrimitive.Root>
>(({ className, ...props }, ref) => (
  <SliderPrimitive.Root
    ref={ref}
    className={cn(
      "relative flex w-full touch-none select-none items-center",
      className,
    )}
    {...props}
  >
    <SliderPrimitive.Track className="relative h-1.5 w-full grow overflow-hidden rounded-full bg-primary/20">
      <SliderPrimitive.Range className="slider-range absolute h-full bg-primary" />
    </SliderPrimitive.Track>
    <SliderPrimitive.Thumb className="block h-4 w-4 rounded-full border border-primary/50 bg-background shadow transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50" />
  </SliderPrimitive.Root>
));
Slider.displayName = SliderPrimitive.Root.displayName;

export { Slider };

================
File: src/components/ui/tabs.tsx
================
"use client";

import * as TabsPrimitive from "@radix-ui/react-tabs";
import * as React from "react";

import { cn } from "@/lib/utils";

const Tabs = TabsPrimitive.Root;

const TabsList = React.forwardRef<
  React.ElementRef<typeof TabsPrimitive.List>,
  React.ComponentPropsWithoutRef<typeof TabsPrimitive.List>
>(({ className, ...props }, ref) => (
  <TabsPrimitive.List
    ref={ref}
    className={cn(
      "inline-flex items-center justify-center rounded-md bg-muted p-0.5 text-muted-foreground",
      className,
    )}
    {...props}
  />
));
TabsList.displayName = TabsPrimitive.List.displayName;

const TabsTrigger = React.forwardRef<
  React.ElementRef<typeof TabsPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof TabsPrimitive.Trigger>
>(({ className, ...props }, ref) => (
  <TabsPrimitive.Trigger
    ref={ref}
    className={cn(
      "inline-flex items-center justify-center whitespace-nowrap rounded-md px-3 py-1",
      "text-sm font-semibold ring-offset-background transition-all",
      "focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring/20",
      "focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50",
      "data-[state=active]:bg-background data-[state=active]:text-foreground data-[state=active]:shadow",
      className,
    )}
    {...props}
  />
));
TabsTrigger.displayName = TabsPrimitive.Trigger.displayName;

const TabsContent = React.forwardRef<
  React.ElementRef<typeof TabsPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof TabsPrimitive.Content>
>(({ className, ...props }, ref) => (
  <TabsPrimitive.Content
    ref={ref}
    className={cn(
      "mt-2 ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring/20 focus-visible:ring-offset-2",
      className,
    )}
    {...props}
  />
));
TabsContent.displayName = TabsPrimitive.Content.displayName;

export { Tabs, TabsList, TabsTrigger, TabsContent };

================
File: src/components/ui/textarea.tsx
================
import * as React from "react";

import { cn } from "@/lib/utils";

const Textarea = React.forwardRef<
  HTMLTextAreaElement,
  React.ComponentProps<"textarea">
>(({ className, ...props }, ref) => {
  return (
    <textarea
      className={cn(
        "flex min-h-[60px] w-full rounded-md border border-transparent bg-muted/50 focus:bg-muted/70 px-2 py-1 text-sm",
        "placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-1",
        "focus-visible:ring-ring/20 disabled:cursor-not-allowed disabled:opacity-50",
        className,
      )}
      ref={ref}
      {...props}
    />
  );
});
Textarea.displayName = "Textarea";

export { Textarea };

================
File: src/components/ui/toast.tsx
================
"use client";

import * as ToastPrimitives from "@radix-ui/react-toast";
import { type VariantProps, cva } from "class-variance-authority";
import { X } from "lucide-react";
import * as React from "react";

import { cn } from "@/lib/utils";

const ToastProvider = ToastPrimitives.Provider;

const ToastViewport = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Viewport>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Viewport>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Viewport
    ref={ref}
    className={cn(
      "fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]",
      className,
    )}
    {...props}
  />
));
ToastViewport.displayName = ToastPrimitives.Viewport.displayName;

const toastVariants = cva(
  "group pointer-events-auto relative flex w-full items-center justify-between space-x-2 overflow-hidden rounded-md border p-4 pr-6 shadow-lg transition-all data-[swipe=cancel]:translate-x-0 data-[swipe=end]:translate-x-[var(--radix-toast-swipe-end-x)] data-[swipe=move]:translate-x-[var(--radix-toast-swipe-move-x)] data-[swipe=move]:transition-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[swipe=end]:animate-out data-[state=closed]:fade-out-80 data-[state=closed]:slide-out-to-right-full data-[state=open]:slide-in-from-top-full data-[state=open]:sm:slide-in-from-bottom-full",
  {
    variants: {
      variant: {
        default: "border bg-background text-foreground",
        destructive:
          "destructive group border-destructive bg-destructive text-destructive-foreground",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  },
);

const Toast = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Root>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Root> &
    VariantProps<typeof toastVariants>
>(({ className, variant, ...props }, ref) => {
  return (
    <ToastPrimitives.Root
      ref={ref}
      className={cn(toastVariants({ variant }), className)}
      {...props}
    />
  );
});
Toast.displayName = ToastPrimitives.Root.displayName;

const ToastAction = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Action>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Action>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Action
    ref={ref}
    className={cn(
      "inline-flex h-8 shrink-0 items-center justify-center rounded-md border bg-transparent px-3 text-sm font-medium transition-colors hover:bg-secondary focus:outline-none focus:ring-1 focus:ring-ring disabled:pointer-events-none disabled:opacity-50 group-[.destructive]:border-muted/40 group-[.destructive]:hover:border-destructive/30 group-[.destructive]:hover:bg-destructive group-[.destructive]:hover:text-destructive-foreground group-[.destructive]:focus:ring-destructive",
      className,
    )}
    {...props}
  />
));
ToastAction.displayName = ToastPrimitives.Action.displayName;

const ToastClose = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Close>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Close>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Close
    ref={ref}
    className={cn(
      "absolute right-1 top-1 rounded-md p-1 text-foreground/50 opacity-0 transition-opacity hover:text-foreground focus:opacity-100 focus:outline-none focus:ring-1 group-hover:opacity-100 group-[.destructive]:text-red-300 group-[.destructive]:hover:text-red-50 group-[.destructive]:focus:ring-red-400 group-[.destructive]:focus:ring-offset-red-600",
      className,
    )}
    toast-close=""
    {...props}
  >
    <X className="h-4 w-4" />
  </ToastPrimitives.Close>
));
ToastClose.displayName = ToastPrimitives.Close.displayName;

const ToastTitle = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Title>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Title>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Title
    ref={ref}
    className={cn("text-sm font-semibold [&+div]:text-xs", className)}
    {...props}
  />
));
ToastTitle.displayName = ToastPrimitives.Title.displayName;

const ToastDescription = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Description>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Description>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Description
    ref={ref}
    className={cn("text-sm opacity-90", className)}
    {...props}
  />
));
ToastDescription.displayName = ToastPrimitives.Description.displayName;

type ToastProps = React.ComponentPropsWithoutRef<typeof Toast>;

type ToastActionElement = React.ReactElement<typeof ToastAction>;

export {
  type ToastProps,
  type ToastActionElement,
  ToastProvider,
  ToastViewport,
  Toast,
  ToastTitle,
  ToastDescription,
  ToastClose,
  ToastAction,
};

================
File: src/components/ui/toaster.tsx
================
"use client";

import {
  Toast,
  ToastClose,
  ToastDescription,
  ToastProvider,
  ToastTitle,
  ToastViewport,
} from "@/components/ui/toast";
import { useToast } from "@/hooks/use-toast";

export function Toaster() {
  const { toasts } = useToast();

  return (
    <ToastProvider>
      {toasts.map(({ id, title, description, action, ...props }) => (
        <Toast key={id} {...props}>
          <div className="grid gap-1">
            {title && <ToastTitle>{title}</ToastTitle>}
            {description && <ToastDescription>{description}</ToastDescription>}
          </div>
          {action}
          <ToastClose />
        </Toast>
      ))}
      <ToastViewport />
    </ToastProvider>
  );
}

================
File: src/components/ui/toggle-group.tsx
================
"use client";

import * as ToggleGroupPrimitive from "@radix-ui/react-toggle-group";
import type { VariantProps } from "class-variance-authority";
import * as React from "react";

import { toggleVariants } from "@/components/ui/toggle";
import { cn } from "@/lib/utils";

const ToggleGroupContext = React.createContext<
  VariantProps<typeof toggleVariants>
>({
  size: "default",
  variant: "default",
});

const ToggleGroup = React.forwardRef<
  React.ElementRef<typeof ToggleGroupPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof ToggleGroupPrimitive.Root> &
    VariantProps<typeof toggleVariants>
>(({ className, variant, size, children, ...props }, ref) => (
  <ToggleGroupPrimitive.Root
    ref={ref}
    className={cn("flex items-center justify-center gap-1", className)}
    {...props}
  >
    <ToggleGroupContext.Provider value={{ variant, size }}>
      {children}
    </ToggleGroupContext.Provider>
  </ToggleGroupPrimitive.Root>
));

ToggleGroup.displayName = ToggleGroupPrimitive.Root.displayName;

const ToggleGroupItem = React.forwardRef<
  React.ElementRef<typeof ToggleGroupPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof ToggleGroupPrimitive.Item> &
    VariantProps<typeof toggleVariants>
>(({ className, children, variant, size, ...props }, ref) => {
  const context = React.useContext(ToggleGroupContext);

  return (
    <ToggleGroupPrimitive.Item
      ref={ref}
      className={cn(
        toggleVariants({
          variant: context.variant || variant,
          size: context.size || size,
        }),
        className,
      )}
      {...props}
    >
      {children}
    </ToggleGroupPrimitive.Item>
  );
});

ToggleGroupItem.displayName = ToggleGroupPrimitive.Item.displayName;

export { ToggleGroup, ToggleGroupItem };

================
File: src/components/ui/toggle.tsx
================
"use client";

import * as TogglePrimitive from "@radix-ui/react-toggle";
import { type VariantProps, cva } from "class-variance-authority";
import * as React from "react";

import { cn } from "@/lib/utils";

const toggleVariants = cva(
  "inline-flex items-center justify-center gap-2 rounded-md text-sm font-medium transition-colors hover:bg-muted hover:text-muted-foreground focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring/20 disabled:pointer-events-none disabled:opacity-50 data-[state=on]:bg-accent data-[state=on]:text-accent-foreground [&_svg]:pointer-events-none [&_svg]:size-4 [&_svg]:shrink-0",
  {
    variants: {
      variant: {
        default: "bg-transparent",
        outline:
          "border border-input bg-transparent shadow-sm hover:bg-accent hover:text-accent-foreground",
      },
      size: {
        default: "h-9 px-2 min-w-9",
        sm: "h-8 px-1.5 min-w-8",
        xs: "h-7 px-1 min-w-7",
        lg: "h-10 px-2.5 min-w-10",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  },
);

const Toggle = React.forwardRef<
  React.ElementRef<typeof TogglePrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof TogglePrimitive.Root> &
    VariantProps<typeof toggleVariants>
>(({ className, variant, size, ...props }, ref) => (
  <TogglePrimitive.Root
    ref={ref}
    className={cn(toggleVariants({ variant, size, className }))}
    {...props}
  />
));

Toggle.displayName = TogglePrimitive.Root.displayName;

export { Toggle, toggleVariants };

================
File: src/components/ui/tooltip.tsx
================
"use client";

import * as TooltipPrimitive from "@radix-ui/react-tooltip";
import * as React from "react";

import { cn } from "@/lib/utils";
import type { PropsWithChildren } from "react";

const TooltipProvider = TooltipPrimitive.Provider;

const Tooltip = TooltipPrimitive.Root;

const TooltipTrigger = TooltipPrimitive.Trigger;

const TooltipContent = React.forwardRef<
  React.ElementRef<typeof TooltipPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof TooltipPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <TooltipPrimitive.Portal>
    <TooltipPrimitive.Content
      ref={ref}
      sideOffset={sideOffset}
      className={cn(
        "z-50 overflow-hidden rounded-md bg-secondary px-3 py-1.5 text-sm text-foreground animate-in fade-in-0 zoom-in-95 data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=closed]:zoom-out-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        className,
      )}
      {...props}
    />
  </TooltipPrimitive.Portal>
));
TooltipContent.displayName = TooltipPrimitive.Content.displayName;

type WithTooltipProps = PropsWithChildren<
  {
    tooltip: string;
  } & TooltipPrimitive.TooltipProps
>;

const WithTooltip = ({ tooltip, children, ...props }: WithTooltipProps) => (
  <TooltipProvider>
    <Tooltip {...props}>
      <TooltipTrigger asChild>{children}</TooltipTrigger>
      <TooltipContent>
        <p>{tooltip}</p>
      </TooltipContent>
    </Tooltip>
  </TooltipProvider>
);

export {
  Tooltip,
  TooltipTrigger,
  TooltipContent,
  TooltipProvider,
  WithTooltip,
};

================
File: src/components/video/timeline.tsx
================
"use client";

import clsx from "clsx";
import type { HTMLAttributes } from "react";

type TimelineRulerProps = {
  duration?: number;
} & HTMLAttributes<HTMLDivElement>;

export function TimelineRuler({
  className,
  duration = 30,
}: TimelineRulerProps) {
  const totalTicks = duration * 10;
  return (
    <div className={clsx("w-full h-full absolute overflow-hidden", className)}>
      <div className="flex px-2 py-0.5 h-full">
        {Array.from({ length: totalTicks + 1 }).map((_, index) => {
          const isMajorTick = index % 50 === 0;
          const isMinorTick = index % 10 === 0;
          return (
            <div key={index} className="flex-grow flex flex-col">
              {isMajorTick && (
                <div className="text-muted-foreground text-sm tabular-nums h-full text-center mt-1">
                  {(index / 10).toFixed(0)}s
                  <div className="h-full max-h-full w-px bg-border/50 mx-auto mt-1 mb-4"></div>
                </div>
              )}
              {isMinorTick && !isMajorTick && (
                <div className="text-muted-foreground tabular-nums text-center">
                  &middot;
                </div>
              )}
            </div>
          );
        })}
      </div>
    </div>
  );
}

================
File: src/components/video/track.tsx
================
import { db } from "@/data/db";
import {
  queryKeys,
  refreshVideoCache,
  useProjectMediaItems,
} from "@/data/queries";
import type { MediaItem, VideoKeyFrame, VideoTrack } from "@/data/schema";
import { cn, resolveDuration, resolveMediaUrl, trackIcons } from "@/lib/utils";
import {
  keepPreviousData,
  useMutation,
  useQuery,
  useQueryClient,
} from "@tanstack/react-query";
import { TrashIcon } from "lucide-react";
import {
  type HTMLAttributes,
  type MouseEventHandler,
  createElement,
  useMemo,
  useRef,
} from "react";
import { WithTooltip } from "../ui/tooltip";
import { useProjectId, useVideoProjectStore } from "@/data/store";
import { fal } from "@/lib/fal";

type VideoTrackRowProps = {
  data: VideoTrack;
} & HTMLAttributes<HTMLDivElement>;

export function VideoTrackRow({ data, ...props }: VideoTrackRowProps) {
  const { data: keyframes = [] } = useQuery({
    queryKey: ["frames", data],
    queryFn: () => db.keyFrames.keyFramesByTrack(data.id),
  });

  const mediaType = useMemo(() => keyframes[0]?.data.type, [keyframes]);

  return (
    <div
      className={cn(
        "relative w-full timeline-container",
        "flex flex-col select-none rounded overflow-hidden shrink-0",
        {
          "min-h-[64px]": mediaType,
          "min-h-[56px]": !mediaType,
        },
      )}
      {...props}
    >
      {keyframes.map((frame) => (
        <VideoTrackView
          key={frame.id}
          className="absolute top-0 bottom-0"
          style={{
            left: `${(frame.timestamp / 10 / 30).toFixed(2)}%`,
            width: `${(frame.duration / 10 / 30).toFixed(2)}%`,
          }}
          track={data}
          frame={frame}
        />
      ))}
    </div>
  );
}

type AudioWaveformProps = {
  data: MediaItem;
};

function AudioWaveform({ data }: AudioWaveformProps) {
  const { data: waveform = [] } = useQuery({
    queryKey: ["media", "waveform", data.id],
    queryFn: async () => {
      if (data.metadata?.waveform && Array.isArray(data.metadata.waveform)) {
        return data.metadata.waveform;
      }
      const { data: waveformInfo } = await fal.subscribe(
        "fal-ai/ffmpeg-api/waveform",
        {
          input: {
            media_url: resolveMediaUrl(data),
            points_per_second: 5,
            precision: 3,
          },
        },
      );
      await db.media.update(data.id, {
        ...data,
        metadata: {
          ...data.metadata,
          waveform: waveformInfo.waveform,
        },
      });
      return waveformInfo.waveform as number[];
    },
    placeholderData: keepPreviousData,
    staleTime: Number.POSITIVE_INFINITY,
  });

  const svgWidth = waveform.length * 3;
  const svgHeight = 100;

  return (
    <div className="h-full flex items-center">
      <svg
        width="100%"
        height="80%"
        viewBox={`0 0 ${svgWidth} ${svgHeight}`}
        preserveAspectRatio="none"
      >
        <title>Audio Waveform</title>
        {waveform.map((v, index) => {
          const amplitude = Math.abs(v);
          const height = Math.max(amplitude * svgHeight, 2);
          const x = index * 3;
          const y = (svgHeight - height) / 2;

          return (
            <rect
              key={index}
              x={x}
              y={y}
              width="2"
              height={height}
              className="fill-black/40"
              rx="4"
            />
          );
        })}
      </svg>
    </div>
  );
}

type VideoTrackViewProps = {
  track: VideoTrack;
  frame: VideoKeyFrame;
} & HTMLAttributes<HTMLDivElement>;

export function VideoTrackView({
  className,
  track,
  frame,
  ...props
}: VideoTrackViewProps) {
  const queryClient = useQueryClient();
  const projectId = useProjectId();
  const { data: mediaItems = [] } = useProjectMediaItems(projectId);

  const deleteKeyframe = useMutation({
    mutationFn: () => db.keyFrames.delete(frame.id),
    onSuccess: () => refreshVideoCache(queryClient, track.projectId),
  });
  const handleOnDelete = () => {
    deleteKeyframe.mutate();
  };

  const isSelected = useVideoProjectStore((state) =>
    state.selectedKeyframes.includes(frame.id),
  );
  const selectKeyframe = useVideoProjectStore((state) => state.selectKeyframe);
  const handleOnClick: MouseEventHandler = (e) => {
    if (e.detail > 1) {
      return;
    }
    selectKeyframe(frame.id);
  };

  const media = mediaItems.find((item) => item.id === frame.data.mediaId);
  const mediaUrl = media ? resolveMediaUrl(media) : undefined;

  const imageUrl = useMemo(() => {
    if (media && media.mediaType === "image") {
      return mediaUrl;
    }
    if (media && media.mediaType === "video") {
      return (
        media.input?.image_url ||
        media.metadata?.start_frame_url ||
        media.metadata?.end_frame_url
      );
    }
    return undefined;
  }, [media, mediaUrl]);

  const trackRef = useRef<HTMLDivElement>(null);

  // If media is not found, render a placeholder
  if (!media) {
    console.warn(
      `Media not found for ID: ${frame.data.mediaId}. This keyframe may need to be removed.`,
    );
    return (
      <div
        className={cn(
          "relative flex h-full w-full items-center justify-center bg-muted/30",
          className,
        )}
        {...props}
      >
        <div className="text-xs text-muted-foreground">Media not found</div>
      </div>
    );
  }

  const label = media.mediaType ?? "unknown";

  const calculateBounds = () => {
    const timelineElement = document.querySelector(".timeline-container");
    const timelineRect = timelineElement?.getBoundingClientRect();
    const trackElement = trackRef.current;
    const trackRect = trackElement?.getBoundingClientRect();

    if (!timelineRect || !trackRect || !trackElement)
      return { left: 0, right: 0 };

    const previousTrack = trackElement?.previousElementSibling;
    const nextTrack = trackElement?.nextElementSibling;

    const leftBound = previousTrack
      ? previousTrack.getBoundingClientRect().right - (timelineRect?.left || 0)
      : 0;
    const rightBound = nextTrack
      ? nextTrack.getBoundingClientRect().left -
        (timelineRect?.left || 0) -
        trackRect.width
      : timelineRect.width - trackRect.width;

    return {
      left: leftBound,
      right: rightBound,
    };
  };

  const handleMouseDown = (e: React.MouseEvent<HTMLDivElement>) => {
    const trackElement = trackRef.current;
    if (!trackElement) return;
    const bounds = calculateBounds();
    const startX = e.clientX;
    const startLeft = trackElement.offsetLeft;

    const handleMouseMove = (moveEvent: MouseEvent) => {
      const deltaX = moveEvent.clientX - startX;
      let newLeft = startLeft + deltaX;

      if (newLeft < bounds.left) {
        newLeft = bounds.left;
      } else if (newLeft > bounds.right) {
        newLeft = bounds.right;
      }

      const timelineElement = trackElement.closest(".timeline-container");
      const parentWidth = timelineElement
        ? (timelineElement as HTMLElement).offsetWidth
        : 1;
      const newTimestamp = (newLeft / parentWidth) * 30;
      frame.timestamp = (newTimestamp < 0 ? 0 : newTimestamp) * 1000;

      trackElement.style.left = `${((frame.timestamp / 30) * 100) / 1000}%`;
      db.keyFrames.update(frame.id, { timestamp: frame.timestamp });
    };

    const handleMouseUp = () => {
      document.removeEventListener("mousemove", handleMouseMove);
      document.removeEventListener("mouseup", handleMouseUp);
      queryClient.invalidateQueries({
        queryKey: queryKeys.projectPreview(projectId),
      });
    };

    document.addEventListener("mousemove", handleMouseMove);
    document.addEventListener("mouseup", handleMouseUp);
  };

  const handleResize = (
    e: React.MouseEvent<HTMLDivElement>,
    direction: "left" | "right",
  ) => {
    e.stopPropagation();
    const trackElement = trackRef.current;
    if (!trackElement) return;
    const startX = e.clientX;
    const startWidth = trackElement.offsetWidth;

    const handleMouseMove = (moveEvent: MouseEvent) => {
      const deltaX = moveEvent.clientX - startX;
      let newWidth = startWidth + (direction === "right" ? deltaX : -deltaX);

      const minDuration = 1000;
      const maxDuration: number = resolveDuration(media) ?? 5000;

      const timelineElement = trackElement.closest(".timeline-container");
      const parentWidth = timelineElement
        ? (timelineElement as HTMLElement).offsetWidth
        : 1;
      let newDuration = (newWidth / parentWidth) * 30 * 1000;

      if (newDuration < minDuration) {
        newWidth = (minDuration / 1000 / 30) * parentWidth;
        newDuration = minDuration;
      } else if (newDuration > maxDuration) {
        newWidth = (maxDuration / 1000 / 30) * parentWidth;
        newDuration = maxDuration;
      }

      frame.duration = newDuration;
      trackElement.style.width = `${((frame.duration / 30) * 100) / 1000}%`;
    };

    const handleMouseUp = () => {
      frame.duration = Math.round(frame.duration / 100) * 100;
      trackElement.style.width = `${((frame.duration / 30) * 100) / 1000}%`;
      db.keyFrames.update(frame.id, { duration: frame.duration });
      queryClient.invalidateQueries({
        queryKey: queryKeys.projectPreview(projectId),
      });
      document.removeEventListener("mousemove", handleMouseMove);
      document.removeEventListener("mouseup", handleMouseUp);
    };

    document.addEventListener("mousemove", handleMouseMove);
    document.addEventListener("mouseup", handleMouseUp);
  };

  return (
    <div
      ref={trackRef}
      onMouseDown={handleMouseDown}
      onContextMenu={(e) => e.preventDefault()}
      aria-checked={isSelected}
      onClick={handleOnClick}
      className={cn(
        "flex flex-col border border-white/10 rounded-lg",
        className,
      )}
      {...props}
    >
      <div
        className={cn(
          "flex flex-col select-none rounded overflow-hidden group h-full",
          {
            "bg-sky-600": track.type === "video",
            "bg-teal-500": track.type === "music",
            "bg-indigo-500": track.type === "voiceover",
          },
        )}
      >
        <div className="p-0.5 pl-1 bg-black/10 flex flex-row items-center">
          <div className="flex flex-row gap-1 text-sm items-center font-semibold text-white/60 w-full">
            <div className="flex flex-row truncate gap-1 items-center">
              {createElement(trackIcons[track.type], {
                className: "w-5 h-5 text-white",
              } as React.ComponentProps<
                (typeof trackIcons)[typeof track.type]
              >)}
              <span className="line-clamp-1 truncate text-sm mb-[2px] w-full ">
                {media.input?.prompt || label}
              </span>
            </div>
            <div className="flex flex-row shrink-0 flex-1 items-center justify-end">
              <WithTooltip tooltip="Remove content">
                <button
                  type="button"
                  className="p-1 rounded hover:bg-black/5 group-hover:text-white"
                  onClick={handleOnDelete}
                >
                  <TrashIcon className="w-3 h-3 text-white" />
                </button>
              </WithTooltip>
            </div>
          </div>
        </div>
        <div
          className="p-px flex-1 items-center bg-repeat-x h-full max-h-full overflow-hidden relative"
          style={
            imageUrl
              ? {
                  background: `url(${imageUrl})`,
                  backgroundSize: "auto 100%",
                }
              : undefined
          }
        >
          {(media.mediaType === "music" || media.mediaType === "voiceover") && (
            <AudioWaveform data={media} />
          )}
          <div
            className={cn(
              "absolute right-0 z-50 top-0 bg-black/20 group-hover:bg-black/40",
              "rounded-md bottom-0 w-2 m-1 p-px cursor-ew-resize backdrop-blur-md text-white/40",
              "transition-colors flex flex-col items-center justify-center text-xs tracking-tighter",
            )}
            onMouseDown={(e) => handleResize(e, "right")}
          >
            <span className="flex gap-[1px]">
              <span className="w-px h-2 rounded bg-white/40" />
              <span className="w-px h-2 rounded bg-white/40" />
            </span>
          </div>
        </div>
      </div>
    </div>
  );
}

================
File: src/components/aspect-ratio.tsx
================
// AspectRatioSelector.tsx
import { cn } from "@/lib/utils";
import type { MouseEventHandler } from "react";
import { ToggleGroup, ToggleGroupItem } from "./ui/toggle-group";

const aspectRatioOptions = {
  "16:9": 16 / 9,
  // "4:3": 4 / 3,
  "1:1": 1,
  // "3:4": 3 / 4,
  "9:16": 9 / 16,
} as const;

export type AspectRatioOption = keyof typeof aspectRatioOptions;

interface AspectRatioSelectorProps {
  className?: string;
  onValueChange?: (ratio: AspectRatioOption | null) => void;
  value: AspectRatioOption | null;
}

export function AspectRatioSelector({
  className,
  onValueChange,
  value,
}: AspectRatioSelectorProps) {
  const handleOnClick = (ratio: AspectRatioOption) => {
    return ((e) => {
      e.preventDefault();
      if (value === ratio) {
        onValueChange?.(null);
        return;
      }
      onValueChange?.(ratio);
    }) as MouseEventHandler<HTMLButtonElement>;
  };
  const ratioValue = value ? aspectRatioOptions[value] : 0;

  return (
    <div
      className={cn(
        "mx-auto w-full flex-col items-center justify-center gap-4",
        className,
      )}
    >
      <div className="flex items-center justify-between gap-2">
        <ToggleGroup type="single" size="xs" value={value ?? ""}>
          {Object.keys(aspectRatioOptions).map((option) => (
            <ToggleGroupItem
              key={option}
              className="tabular-nums"
              onClick={handleOnClick(option as AspectRatioOption)}
              value={option}
            >
              {option}
            </ToggleGroupItem>
          ))}
        </ToggleGroup>
      </div>
      <div className="flex aspect-square w-full items-center justify-center">
        <div className="relative flex aspect-square h-full w-full items-center justify-center">
          <div className="text-sm tabular-nums">{value ?? "default"}</div>
          {!!value && (
            <div
              className={cn(
                "absolute border border-primary",
                "z-40 transition-all",
                {
                  "w-2/5": ratioValue <= 1,
                  "h-2/5": ratioValue > 1,
                },
              )}
              style={{
                aspectRatio: value.replace(":", "/"),
              }}
            />
          )}
          {Object.entries(aspectRatioOptions).map(([option, ratio]) => (
            <div
              key={option}
              className={cn(
                "absolute border border-dashed border-muted-foreground/70 transition-colors",
                {
                  "w-2/5": ratio <= 1,
                  "h-2/5": ratio > 1,
                },
              )}
              style={{
                aspectRatio: option.replace(":", "/"),
              }}
            />
          ))}
        </div>
      </div>
    </div>
  );
}

================
File: src/components/bottom-bar.tsx
================
import { db } from "@/data/db";
import {
  TRACK_TYPE_ORDER,
  type MediaItem,
  type VideoTrack,
} from "@/data/schema";
import { useProjectId, useVideoProjectStore } from "@/data/store";
import { cn, resolveDuration } from "@/lib/utils";
import { useMutation, useQuery, useQueryClient } from "@tanstack/react-query";
import { type DragEventHandler, useMemo, useState } from "react";
import { VideoControls } from "./video-controls";
import { TimelineRuler } from "./video/timeline";
import { VideoTrackRow } from "./video/track";
import { queryKeys, refreshVideoCache } from "@/data/queries";

export default function BottomBar() {
  const queryClient = useQueryClient();
  const projectId = useProjectId();
  const playerCurrentTimestamp = useVideoProjectStore(
    (s) => s.playerCurrentTimestamp,
  );
  const formattedTimestamp =
    (playerCurrentTimestamp < 10 ? "0" : "") +
    playerCurrentTimestamp.toFixed(2);
  const minTrackWidth = `${((2 / 30) * 100).toFixed(2)}%`;
  const [dragOverTracks, setDragOverTracks] = useState(false);

  const handleOnDragOver: DragEventHandler<HTMLDivElement> = (event) => {
    event.preventDefault();
    setDragOverTracks(true);
    const jobPayload = event.dataTransfer.getData("job");
    if (!jobPayload) return false;
    const job: MediaItem = JSON.parse(jobPayload);
    return job.status === "completed";
  };

  const addToTrack = useMutation({
    mutationFn: async (media: MediaItem) => {
      const tracks = await db.tracks.tracksByProject(media.projectId);
      const trackType = media.mediaType === "image" ? "video" : media.mediaType;
      let track = tracks.find((t) => t.type === trackType);
      if (!track) {
        const id = await db.tracks.create({
          projectId: media.projectId,
          type: trackType,
          label: media.mediaType,
          locked: true,
        });
        const newTrack = await db.tracks.find(id.toString());
        if (!newTrack) return;
        track = newTrack;
      }
      const keyframes = await db.keyFrames.keyFramesByTrack(track.id);

      const lastKeyframe = [...keyframes]
        .sort((a, b) => a.timestamp - b.timestamp)
        .reduce(
          (acc, frame) => {
            if (frame.timestamp + frame.duration > acc.timestamp + acc.duration)
              return frame;
            return acc;
          },
          { timestamp: 0, duration: 0 },
        );

      const duration = resolveDuration(media) ?? 5000;

      const newId = await db.keyFrames.create({
        trackId: track.id,
        data: {
          mediaId: media.id,
          type: media.input?.image_url ? "image" : "prompt",
          prompt: media.input?.prompt || "",
          url: media.input?.image_url?.url,
        },
        timestamp: lastKeyframe
          ? lastKeyframe.timestamp + 1 + lastKeyframe.duration
          : 0,
        duration,
      });
      return db.keyFrames.find(newId.toString());
    },
    onSuccess: (data) => {
      if (!data) return;
      refreshVideoCache(queryClient, projectId);
    },
  });

  const { data: tracks = [] } = useQuery({
    queryKey: queryKeys.projectTracks(projectId),
    queryFn: async () => {
      const result = await db.tracks.tracksByProject(projectId);
      return result.toSorted(
        (a, b) => TRACK_TYPE_ORDER[a.type] - TRACK_TYPE_ORDER[b.type],
      );
    },
  });

  const trackObj: Record<string, VideoTrack> = useMemo(() => {
    return {
      video:
        tracks.find((t) => t.type === "video") ||
        ({
          id: "video",
          type: "video",
          label: "Video",
          locked: true,
          keyframes: [],
          projectId: projectId,
        } as VideoTrack),
      music:
        tracks.find((t) => t.type === "music") ||
        ({
          id: "music",
          type: "music",
          label: "Music",
          locked: true,
          keyframes: [],
          projectId: projectId,
        } as VideoTrack),
      voiceover:
        tracks.find((t) => t.type === "voiceover") ||
        ({
          id: "voiceover",
          type: "voiceover",
          label: "Voiceover",
          locked: true,
          keyframes: [],
          projectId: projectId,
        } as VideoTrack),
    };
  }, [tracks, projectId]);

  const handleOnDrop: DragEventHandler<HTMLDivElement> = (event) => {
    event.preventDefault();
    setDragOverTracks(false);
    const jobPayload = event.dataTransfer.getData("job");
    if (!jobPayload) return false;
    const job: MediaItem = JSON.parse(jobPayload);
    addToTrack.mutate(job);
    return true;
  };

  return (
    <div className="border-t pb-2 border-border flex flex-col bg-background-light glassmorphism">
      <div className="border-b border-border bg-background-dark px-2 flex flex-row gap-8 py-2 justify-between items-center flex-1">
        <div className="h-full flex flex-col justify-center px-4 bg-muted/50 rounded-md font-mono cursor-default select-none shadow-inner">
          <div className="flex flex-row items-baseline font-thin tabular-nums">
            <span className="text-muted-foreground">00:</span>
            <span>{formattedTimestamp}</span>
            <span className="text-muted-foreground/50 mx-2">/</span>
            <span className="text-sm opacity-50">
              <span className="text-muted-foreground">00:</span>30.00
            </span>
          </div>
        </div>
        <VideoControls />
      </div>
      <div
        className={cn(
          "min-h-64  max-h-72 h-full flex flex-row overflow-y-scroll transition-colors",
          {
            "bg-white/5": dragOverTracks,
          },
        )}
        onDragOver={handleOnDragOver}
        onDragLeave={() => setDragOverTracks(false)}
        onDrop={handleOnDrop}
      >
        <div className="flex flex-col justify-start w-full h-full relative">
          <div
            className="absolute z-[32] top-6 bottom-0 w-[2px] bg-white/30 ms-4"
            style={{
              left: `${((playerCurrentTimestamp / 30) * 100).toFixed(2)}%`,
            }}
          />
          <TimelineRuler className="z-30 pointer-events-none" />
          <div className="flex timeline-container flex-col h-full mx-4 mt-10 gap-2 z-[31] pb-2">
            {Object.values(trackObj).map((track, index) =>
              track ? (
                <VideoTrackRow
                  key={track.id}
                  data={track}
                  style={{
                    minWidth: minTrackWidth,
                  }}
                />
              ) : (
                <div
                  key={`empty-track-${index}`}
                  className="flex flex-row relative w-full h-full timeline-container"
                />
              ),
            )}
          </div>
        </div>
      </div>
    </div>
  );
}

================
File: src/components/camera-control.tsx
================
import { useState } from "react";
import { Button } from "@/components/ui/button";
import { Slider } from "@/components/ui/slider";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "@/components/ui/select";
import { ChevronUp, ChevronDown, FocusIcon } from "lucide-react";

const CameraMovement = ({
  value: initialValue,
  onChange,
}: {
  value: { movement: string; value: number } | undefined;
  onChange: (value: { movement: string; value: number } | undefined) => void;
}) => {
  const [open, setOpen] = useState(true);
  const [movement, setMovement] = useState(initialValue?.movement || "default");
  const [value, setValue] = useState(initialValue?.value || 0);

  const getTransformStyle = () => {
    switch (movement) {
      case "roll":
        return `rotate(${value * 2}deg)`;
      case "horizontal":
        return `translateX(${value * 3}px)`;
      case "vertical":
        return `translateY(${value * 3}px)`;
      case "pan":
        return `perspective(300px) rotateX(${-value * 4}deg)`;
      case "tilt":
        return `perspective(300px) rotateY(${-value * 4}deg)`;
      case "zoom":
        return `scale(${1 + value / 25})`;
      default:
        return "rotate(0deg)";
    }
  };

  const handleChange = (type: "movement" | "value", val: string | number) => {
    console.log(type, val);
    if (type === "movement") {
      setMovement(val as string);
      setValue(0);
      if (movement === "default") {
        setValue(0);
        onChange(undefined);
      }
      onChange({ movement: val as string, value: value });
    } else {
      setValue(val as number);
      onChange({ movement: movement, value: val as number });
    }
  };

  return (
    <div className="w-full mx-auto border-y border-neutral-800 py-3">
      {/* Header */}
      <div
        className="flex justify-between items-center select-none"
        role="button"
        onClick={() => setOpen(!open)}
      >
        <div className="flex items-center gap-2">
          <span className="text-muted-foreground">Camera Movement</span>
        </div>
        <Button variant="ghost" size="icon" className="text-white">
          {open ? (
            <ChevronUp className="h-6 w-6" />
          ) : (
            <ChevronDown className="h-6 w-6" />
          )}
        </Button>
      </div>

      {open && (
        <>
          <div className="bg-neutral-800/40 px-4 py-8 flex items-center justify-center aspect-video relative rounded-md my-6">
            <div className="w-2/3 border border-dashed absolute border-neutral-800 rounded-xl aspect-video bg-neutral-950/50 flex items-center justify-center" />
            <div
              style={{
                transform: getTransformStyle(),
              }}
              className="w-2/3 border absolute rotate-0 border-green-400/20 rounded-xl aspect-video bg-green-400/30 flex items-center justify-center"
            >
              <FocusIcon size={24} />
            </div>
          </div>

          {/* Controls */}
          <div className="space-y-4">
            <div className="flex items-center gap-2">
              <div className="text-muted-foreground">Camera Control</div>
            </div>

            <div className="flex items-center gap-4">
              <Select
                value={movement}
                onValueChange={(value) => {
                  handleChange("movement", value);
                }}
              >
                <SelectTrigger className="w-full">
                  <SelectValue placeholder="Select movement" />
                </SelectTrigger>
                <SelectContent>
                  <SelectItem value="default">Default</SelectItem>
                  <SelectItem value="horizontal">Horizontal</SelectItem>
                  <SelectItem value="vertical">Vertical</SelectItem>
                  <SelectItem value="pan">Pan</SelectItem>
                  <SelectItem value="tilt">Tilt</SelectItem>
                  <SelectItem value="roll">Roll</SelectItem>
                  <SelectItem value="zoom">Zoom</SelectItem>
                </SelectContent>
              </Select>

              <Button
                variant="secondary"
                onClick={() => handleChange("movement", "default")}
              >
                Reset
              </Button>
            </div>

            {movement !== "default" && (
              <div className="space-y-2">
                <span className="text-muted-foreground capitalize">
                  {movement}:
                </span>
                <div className="flex items-center gap-4">
                  <Slider
                    defaultValue={[value]}
                    value={[value]}
                    onValueChange={(value) => handleChange("value", value[0])}
                    max={10}
                    min={-10}
                    step={0.1}
                    className="flex-1 [&_.slider-range]:bg-transparent"
                  />

                  <div className="bg-neutral-900 px-3 py-2 text-xs rounded-md text-white inline-flex tabular-nums w-10 items-center justify-center text-center">
                    {value}
                  </div>
                </div>
              </div>
            )}
          </div>
        </>
      )}
    </div>
  );
};

export default CameraMovement;

================
File: src/components/export-dialog.tsx
================
import { useMutation } from "@tanstack/react-query";
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from "./ui/dialog";
import { cn, resolveMediaUrl } from "@/lib/utils";
import {
  EMPTY_VIDEO_COMPOSITION,
  useProject,
  useVideoComposition,
} from "@/data/queries";
import { fal } from "@/lib/fal";
import { Button } from "./ui/button";
import { useProjectId, useVideoProjectStore } from "@/data/store";
import { LoadingIcon } from "./ui/icons";
import {
  CopyIcon,
  DownloadIcon,
  Share2Icon as ShareIcon,
  FilmIcon,
} from "lucide-react";
import { Input } from "./ui/input";
import type { ShareVideoParams } from "@/lib/share";
import { PROJECT_PLACEHOLDER } from "@/data/schema";
import { useRouter } from "next/navigation";

type ExportDialogProps = {} & Parameters<typeof Dialog>[0];

type ShareResult = {
  video_url: string;
  thumbnail_url: string;
};

export function ExportDialog({ onOpenChange, ...props }: ExportDialogProps) {
  const projectId = useProjectId();
  const { data: composition = EMPTY_VIDEO_COMPOSITION } =
    useVideoComposition(projectId);
  const router = useRouter();
  const exportVideo = useMutation({
    mutationFn: async () => {
      const mediaItems = composition.mediaItems;
      const videoData = composition.tracks.map((track) => ({
        id: track.id,
        type: track.type === "video" ? "video" : "audio",
        keyframes: composition.frames[track.id].map((frame) => ({
          timestamp: frame.timestamp,
          duration: frame.duration,
          url: resolveMediaUrl(mediaItems[frame.data.mediaId]),
        })),
      }));
      if (videoData.length === 0) {
        throw new Error("No tracks to export");
      }
      const { data } = await fal.subscribe("fal-ai/ffmpeg-api/compose", {
        input: {
          tracks: videoData,
        },
        mode: "polling",
        pollInterval: 3000,
      });
      return data as ShareResult;
    },
  });
  const setExportDialogOpen = useVideoProjectStore(
    (s) => s.setExportDialogOpen,
  );
  const handleOnOpenChange = (open: boolean) => {
    setExportDialogOpen(open);
    onOpenChange?.(open);
  };

  const { data: project = PROJECT_PLACEHOLDER } = useProject(projectId);
  const share = useMutation({
    mutationFn: async () => {
      if (!exportVideo.data) {
        throw new Error("No video to share");
      }
      const videoInfo = exportVideo.data;
      const response = await fetch("/api/share", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          title: project.title,
          description: project.description ?? "",
          videoUrl: videoInfo.video_url,
          thumbnailUrl: videoInfo.thumbnail_url,
          createdAt: Date.now(),
          // TODO parametrize this
          width: 1920,
          height: 1080,
        } satisfies ShareVideoParams),
      });
      if (!response.ok) {
        throw new Error("Failed to share video");
      }
      return response.json();
    },
  });

  const handleOnShare = async () => {
    const { id } = await share.mutateAsync();
    router.push(`/share/${id}`);
  };

  const actionsDisabled = exportVideo.isPending || share.isPending;

  return (
    <Dialog onOpenChange={handleOnOpenChange} {...props}>
      <DialogContent className="sm:max-w-4xl max-w-full">
        <DialogHeader>
          <DialogTitle className="flex items-center gap-2">
            <FilmIcon className="w-6 h-6 opacity-50" />
            Export video
          </DialogTitle>
          <DialogDescription />
        </DialogHeader>
        <div className="text-muted-foreground">
          <p>This may take a while, sit back and relax.</p>
        </div>
        <div
          className={cn(
            "w-full max-h-[500px] mx-auto max-w-full",
            project?.aspectRatio === "16:9" ? "aspect-[16/9]" : "aspect-[9/16]",
          )}
        >
          {exportVideo.isPending || exportVideo.data === undefined ? (
            <div
              className={cn(
                "bg-accent/30 flex flex-col items-center justify-center w-full h-full",
              )}
            >
              {exportVideo.isPending ? (
                <LoadingIcon className="w-24 h-24" />
              ) : (
                <FilmIcon className="w-24 h-24 opacity-50" />
              )}
            </div>
          ) : (
            <video
              src={exportVideo.data.video_url}
              controls
              className="w-full h-full"
            />
          )}
        </div>
        <div className="flex flex-col gap-4">
          <div className="flex flex-row gap-2 items-center">
            <Input
              value={exportVideo.data?.video_url ?? ""}
              placeholder="Video URL..."
              readOnly
              className="text-muted-foreground"
            />
            <Button
              size="icon"
              variant="ghost"
              onClick={() =>
                navigator.clipboard.writeText(exportVideo.data?.video_url ?? "")
              }
              disabled={exportVideo.data === undefined}
            >
              <CopyIcon className="w-5 h-5" />
            </Button>
          </div>
        </div>
        <DialogFooter>
          <Button
            onClick={handleOnShare}
            variant="secondary"
            disabled={actionsDisabled || !exportVideo.data}
          >
            <ShareIcon className="w-4 h-4 opacity-50" />
            Share
          </Button>
          <Button
            variant="secondary"
            disabled={actionsDisabled || !exportVideo.data}
            aria-disabled={actionsDisabled || !exportVideo.data}
            asChild
          >
            <a href={exportVideo.data?.video_url ?? "#"} download>
              <DownloadIcon className="w-4 h-4" />
              Download
            </a>
          </Button>
          <Button
            onClick={() => exportVideo.mutate()}
            disabled={actionsDisabled}
          >
            Export
          </Button>
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
}

================
File: src/components/footer.tsx
================
import React from "react";
import { cn } from "@/lib/utils";

export default function Footer() {
  return (
    <footer
      className={cn(
        "w-full py-3 px-6 mt-auto",
        "glassmorphism border-t border-gray-800/20",
        "text-center text-sm flex justify-between items-center",
      )}
    >
      <div className="flex items-center">
        <span className="font-medium">NovelVision AI</span>
        <span className="mx-2 opacity-50">|</span>
        <span className="opacity-70">© {new Date().getFullYear()}</span>
      </div>
      <div className="flex items-center gap-4">
        <a
          href="https://novelvisionai.art"
          target="_blank"
          rel="noopener noreferrer"
          className="hover:text-primary transition-colors flex items-center"
        >
          <span className="mr-1">AI Visual Studio</span>
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="14"
            height="14"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            strokeWidth="2"
            strokeLinecap="round"
            strokeLinejoin="round"
            className="opacity-50"
          >
            <path d="M7 7h10v10" />
            <path d="M7 17 17 7" />
          </svg>
        </a>
      </div>
    </footer>
  );
}

================
File: src/components/header.tsx
================
"use client";

import { Button } from "@/components/ui/button";
import { Logo } from "./logo";
import {
  SettingsIcon,
  Edit3Icon,
  Users,
  FileTextIcon,
  Home,
  DownloadIcon,
  LayoutDashboard,
  BookOpenIcon,
} from "lucide-react";
import { ThemeToggle } from "./theme-toggle";
import config from "@/lib/config";
import Link from "next/link";
import { useVideoProjectStore } from "@/data/store";
import { useRouter } from "next/navigation";
import { useCallback } from "react";
import sessionManager from "@/app/session-manager";

export default function Header({
  openKeyDialog,
}: {
  openKeyDialog?: () => void;
}) {
  const setExportDialogOpen = useVideoProjectStore(
    (s) => s.setExportDialogOpen,
  );
  const router = useRouter();

  // Handle navigation to storyboard with session data
  const navigateToStoryboard = useCallback(() => {
    // The session data is already in sessionManager
    const userData = sessionManager.getUserData();

    if (userData) {
      // Just back it up to localStorage without any complex manipulation
      try {
        localStorage.setItem(
          "videoProjectSessionData",
          JSON.stringify(userData),
        );
      } catch (error) {
        console.error("Error saving to localStorage:", error);
      }
    }

    // We just need to navigate while preserving it
    router.push("/");
  }, [router]);

  return (
    <header className="px-4 py-2 flex justify-between items-center border-b border-border glassmorphism">
      <div className="flex items-center">
        <Logo />
        <span className="mx-2 text-gray-400">|</span>
        <h2 className="text-lg font-medium">AI Visual Studio</h2>
      </div>

      <nav className="flex flex-row items-center justify-end gap-2">
        <ThemeToggle />

        <Button
          variant="ghost"
          size="sm"
          onClick={() => setExportDialogOpen(true)}
        >
          <DownloadIcon className="w-4 h-4 mr-1" />
          <span className="hidden sm:inline">Export</span>
        </Button>

        {/* Storyboard button commented out for future implementation
        <Button variant="ghost" size="sm" onClick={navigateToStoryboard}>
          <LayoutDashboard className="w-4 h-4 mr-1" />
          <span className="hidden sm:inline">Storyboard</span>
        </Button>
        */}

        {/* Home button removed as per requirements */}

        <Button variant="ghost" size="sm" asChild>
          <Link href={config.urls.writingWorkspace}>
            <Edit3Icon className="w-4 h-4 mr-1" />
            <span className="hidden sm:inline">Writing Space</span>
          </Link>
        </Button>

        {/* Characters button removed as per requirements */}

        {/* Outline button removed as per requirements */}

        <Button variant="ghost" size="sm" asChild>
          <Link href={config.urls.settings} target="_blank">
            <SettingsIcon className="w-4 h-4 mr-1" />
            <span className="hidden sm:inline">Settings</span>
          </Link>
        </Button>

        <Button variant="ghost" size="sm" asChild>
          <Link href="/docs" passHref>
            <BookOpenIcon className="w-4 h-4 mr-1" />
            <span className="hidden sm:inline">Model Guide</span>
          </Link>
        </Button>
      </nav>
    </header>
  );
}

================
File: src/components/key-dialog.tsx
================
"use client";

import { useToast } from "@/hooks/use-toast";
import { useQueryClient } from "@tanstack/react-query";

import { useState } from "react";
import { Button } from "./ui/button";
import {
  Dialog,
  DialogContent,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from "./ui/dialog";
import { Input } from "./ui/input";

type KeyDialogProps = {} & Parameters<typeof Dialog>[0];

export function KeyDialog({ onOpenChange, open, ...props }: KeyDialogProps) {
  const [falKey, setFalKey] = useState("");

  const handleOnOpenChange = (isOpen: boolean) => {
    onOpenChange?.(isOpen);
  };

  const handleSave = () => {
    localStorage.setItem("falKey", falKey);
    handleOnOpenChange(false);
    setFalKey("");
  };

  return (
    <Dialog {...props} onOpenChange={handleOnOpenChange} open={open}>
      <DialogContent className="flex flex-col max-w-lg h-fit">
        <DialogHeader>
          <DialogTitle className="sr-only">Access Key</DialogTitle>
        </DialogHeader>
        <div className="flex flex-col flex-1 gap-8">
          <h2 className="text-lg font-semibold flex flex-row gap-2">
            Save your own FAL Key
          </h2>
          <div className="flex flex-col gap-4">
            <Input
              placeholder="Your FAL Key"
              value={falKey}
              onChange={(e) => setFalKey(e.target.value)}
            />
          </div>
          <div className="flex-1 flex flex-row items-end justify-center gap-2">
            <Button onClick={handleSave}>Save</Button>
          </div>
        </div>

        <DialogFooter>
          <p className="text-muted-foreground text-sm mt-4 w-full text-center">
            You can get your FAL Key from{" "}
            <a
              className="underline underline-offset-2 decoration-foreground/50 text-foreground"
              href="https://fal.ai/dashboard/keys"
            >
              here
            </a>
            .
          </p>
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
}

================
File: src/components/landing-community.tsx
================
import { Button } from "@/components/ui/button";
import { Github, Twitter, DiscIcon as Discord } from "lucide-react";
import Link from "next/link";

export default function Community() {
  return (
    <section id="community" className="py-20 border-t border-white/10">
      <div className="container mx-auto px-4">
        <div className="max-w-2xl mx-auto text-center">
          <h2 className="text-3xl font-bold mb-4">Join our community</h2>
          <p className="text-gray-400 mb-8">
            AI Video Developer Starter Kit is built by developers, for
            developers. Join our growing community and help shape the future of
            video editing.
          </p>

          <div className="grid grid-cols-1 md:grid-cols-3 gap-4">
            <Link href="https://github.com/fal-ai-community">
              <Button variant="outline" className="w-full">
                <Github className="mr-2 h-5 w-5" />
                GitHub
              </Button>
            </Link>
            <Link href="https://discord.gg/fal-ai">
              <Button variant="outline" className="w-full">
                <Discord className="mr-2 h-5 w-5" />
                Discord
              </Button>
            </Link>
            <Link href="https://x.com/fal">
              <Button variant="outline" className="w-full">
                <Twitter className="mr-2 h-5 w-5" />
                Twitter
              </Button>
            </Link>
          </div>
        </div>
      </div>
    </section>
  );
}

================
File: src/components/landing-features.tsx
================
import { Scissors, Wand2, Share2, Zap, Users, Code } from "lucide-react";

const features = [
  {
    icon: Scissors,
    title: "Precise Editing",
    description:
      "Frame-perfect cutting and editing tools for high-quality videos.",
  },
  {
    icon: Wand2,
    title: "AI-Generated Assets",
    description:
      "Use AI to generate music, image, video and more for your videos.",
  },
  {
    icon: Share2,
    title: "Export Anywhere",
    description: "Export to any format and share directly to social platforms.",
  },
  {
    icon: Code,
    title: "Open-Source",
    description: "Built on open-source technologies and available to everyone.",
  },
];

export default function Features() {
  return (
    <section id="features" className="py-20 border-t border-white/10">
      <div className="container mx-auto px-4">
        <div className="max-w-2xl mx-auto text-center mb-16">
          <h2 className="text-3xl font-bold mb-4">
            Powerful features for modern creators
          </h2>
          <p className="text-gray-400">
            Everything you need to create professional-quality videos, available
            to everyone.
          </p>
        </div>

        <div className="max-w-screen-md mx-auto grid grid-cols-1 md:grid-cols-2 gap-8">
          {features.map((feature, index) => (
            <div
              key={index}
              className="p-6 rounded-lg border border-white/10 bg-gradient-to-b from-white/5 to-transparent hover:border-white/20 transition-colors"
            >
              <feature.icon className="w-12 h-12 mb-4 text-white/80" />
              <h3 className="text-xl font-semibold mb-2">{feature.title}</h3>
              <p className="text-gray-400">{feature.description}</p>
            </div>
          ))}
        </div>
      </div>
    </section>
  );
}

================
File: src/components/landing-footer.tsx
================
import Link from "next/link";
import { Video } from "lucide-react";

export default function Footer() {
  return (
    <footer className="border-t flex w-full border-white/10 py-12">
      <div className="container mx-auto px-4">
        <div className="grid grid-cols-2 max-w-screen-md md:grid-cols-3 gap-8 mx-auto">
          <div className="flex flex-col items-start">
            <div className="flex items-center space-x-2 mb-4">
              <Video className="w-6 h-6" />
              <span className="font-semibold">fal.ai</span>
            </div>
            <p className="text-sm text-gray-400">
              Open-source AI video developer
              <br />
              starter kit.
            </p>
          </div>

          <div className="flex flex-col items-center text-center">
            <h4 className="font-semibold mb-4">Resources</h4>
            <ul className="space-y-2 text-sm text-gray-400">
              <li>
                <Link
                  href="https://fal.ai/models"
                  className="hover:text-white transition-colors"
                >
                  AI Models
                </Link>
              </li>
              <li>
                <Link
                  href="https://docs.fal.ai"
                  className="hover:text-white transition-colors"
                >
                  API Reference
                </Link>
              </li>
              <li>
                <Link
                  href="https://blog.fal.ai"
                  className="hover:text-white transition-colors"
                >
                  fal Blog
                </Link>
              </li>
            </ul>
          </div>

          <div className="flex flex-col items-center text-center">
            <h4 className="font-semibold mb-4">Community</h4>
            <ul className="space-y-2 text-sm text-gray-400">
              <li>
                <Link
                  href="https://github.com/fal-ai-community/video-starter-kit"
                  className="hover:text-white transition-colors"
                >
                  GitHub
                </Link>
              </li>
              <li>
                <Link
                  href="https://discord.gg/fal-ai"
                  className="hover:text-white transition-colors"
                >
                  Discord
                </Link>
              </li>
              <li>
                <Link
                  href="https://x.com/fal"
                  target="_blank"
                  className="hover:text-white transition-colors"
                >
                  Twitter
                </Link>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
  );
}

================
File: src/components/landing-header.tsx
================
import Link from "next/link";
import { Button } from "@/components/ui/button";
import { Video } from "lucide-react";

export default function Header() {
  return (
    <header className="fixed top-0 w-full border-b border-white/10 bg-black/50 backdrop-blur-md z-50">
      <div className="container mx-auto px-4 h-16 flex items-center justify-between">
        <div className="flex flex-1">
          <Link href="/" className="flex items-center space-x-2">
            <Video className="w-6 h-6" />
            <span className="font-semibold">fal.ai</span>
          </Link>
        </div>

        <nav className="flex-1 hidden md:flex items-center justify-center space-x-8">
          <Link
            href="#features"
            className="text-sm text-gray-400 hover:text-white transition-colors"
          >
            Features
          </Link>
          <Link
            href="#community"
            className="text-sm text-gray-400 hover:text-white transition-colors"
          >
            Community
          </Link>
          <Link
            href="https://github.com/fal-ai-community/video-starter-kit"
            className="text-sm text-gray-400 hover:text-white transition-colors"
          >
            GitHub
          </Link>
        </nav>

        <div className="flex flex-1 justify-end items-center space-x-4">
          <Link href="/app">
            <Button className="bg-white text-black hover:bg-gray-200">
              Try it now
            </Button>
          </Link>
        </div>
      </div>
    </header>
  );
}

================
File: src/components/landing-hero.tsx
================
import { Button } from "@/components/ui/button";
import { ArrowRight, Github } from "lucide-react";
import { LaptopMockup } from "@/components/ui/landing-laptop-mockup";
import Image from "next/image";
import Link from "next/link";

export default function Hero() {
  return (
    <section className="pt-32 pb-16 md:pt-40 md:pb-24">
      <div className="container mx-auto px-4">
        <div className="text-center mb-16">
          <div className="inline-flex items-center rounded-full border border-white/10 bg-white/5 px-3 py-1 text-sm mb-8">
            <span className="text-gray-400">Now Open Source</span>
            <span className="ml-3 h-4 w-px bg-white/20" />
            <a
              href="https://github.com/fal-ai-community/video-starter-kit"
              className="ml-3 flex items-center text-white hover:text-gray-300"
            >
              Star on GitHub <ArrowRight className="ml-1 h-4 w-4" />
            </a>
          </div>

          <h1 className="text-4xl md:text-6xl font-bold tracking-tight mb-8 bg-gradient-to-r from-white to-gray-500 bg-clip-text text-transparent">
            AI Video Developer
            <br />
            Starter Kit
          </h1>

          <p className="text-gray-400 text-lg md:text-xl max-w-2xl mx-auto mb-12">
            A powerful, open-source AI video editor built for creators. Create
            stunning videos with our intuitive tools, or develop your own AI
            video product using our kit.
          </p>

          <div className="flex flex-col md:flex-row items-center justify-center gap-4 mb-16">
            <Link href="/app">
              <Button
                size="lg"
                className="bg-white text-black hover:bg-gray-200 min-w-[200px]"
              >
                Try it now
              </Button>
            </Link>
            <Link href="https://github.com/fal-ai-community/video-starter-kit">
              <Button size="lg" variant="outline" className="min-w-[200px]">
                <Github className="mr-2 h-5 w-5" />
                Star on GitHub
              </Button>
            </Link>
          </div>
        </div>

        {/* App Screenshot */}
        <div className="relative group max-w-6xl mx-auto">
          <div className="absolute inset-0 bg-gradient-to-r from-purple-500/40 to-blue-500/40 blur-3xl opacity-20" />
          <LaptopMockup>
            <Image
              src="/screenshot.webp?height=800&width=1200"
              width={1200}
              height={800}
              alt="Video Starter Kit interface"
              className="w-full h-auto"
              priority
            />
          </LaptopMockup>

          {/* Floating gradient elements */}
          <div className="absolute -top-16 -right-16 w-32 h-32 bg-purple-500/30 rounded-full blur-3xl opacity-20" />
          <div className="absolute -bottom-16 -left-16 w-32 h-32 bg-blue-500/30 rounded-full blur-3xl opacity-20" />
        </div>
      </div>
    </section>
  );
}

================
File: src/components/left-panel.tsx
================
"use client";

import { useProjectUpdater } from "@/data/mutations";
import { queryKeys, useProject, useProjectMediaItems } from "@/data/queries";
import { type MediaItem, PROJECT_PLACEHOLDER } from "@/data/schema";
import {
  type MediaType,
  useProjectId,
  useVideoProjectStore,
} from "@/data/store";
import {
  ChevronDown,
  FilmIcon,
  FolderOpenIcon,
  GalleryVerticalIcon,
  ImageIcon,
  ImagePlusIcon,
  ListPlusIcon,
  MicIcon,
  MusicIcon,
  LoaderCircleIcon,
  CloudUploadIcon,
  SparklesIcon,
} from "lucide-react";
import { MediaItemPanel } from "./media-panel";
import { Button } from "./ui/button";
import { Input } from "./ui/input";
import { Textarea } from "./ui/textarea";
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from "./ui/dropdown-menu";
import { useState } from "react";
import { useUploadThing } from "@/lib/uploadthing";
import type { ClientUploadedFileData } from "uploadthing/types";
import { db } from "@/data/db";
import { useQueryClient } from "@tanstack/react-query";
import { toast } from "@/hooks/use-toast";
import { getMediaMetadata } from "@/lib/ffmpeg";
import {
  Accordion,
  AccordionContent,
  AccordionItem,
  AccordionTrigger,
} from "./ui/accordion";

export default function LeftPanel() {
  const projectId = useProjectId();
  const { data: project = PROJECT_PLACEHOLDER } = useProject(projectId);
  const projectUpdate = useProjectUpdater(projectId);
  const [mediaType, setMediaType] = useState("all");
  const queryClient = useQueryClient();

  const { data: mediaItems = [], isLoading } = useProjectMediaItems(projectId);
  const setProjectDialogOpen = useVideoProjectStore(
    (s) => s.setProjectDialogOpen,
  );
  const openGenerateDialog = useVideoProjectStore((s) => s.openGenerateDialog);

  const { startUpload, isUploading } = useUploadThing("fileUploader");

  const handleFileUpload = async (e: React.ChangeEvent<HTMLInputElement>) => {
    const files = e.target.files;
    if (!files) return;

    try {
      const uploadedFiles = await startUpload(Array.from(files));
      if (uploadedFiles) {
        await handleUploadComplete(uploadedFiles);
      }
    } catch (err) {
      console.warn(`ERROR! ${err}`);
      toast({
        title: "Failed to upload file",
        description: "Please try again",
      });
    }
  };

  const handleUploadComplete = async (
    files: ClientUploadedFileData<{
      uploadedBy: string;
    }>[],
  ) => {
    for (let i = 0; i < files.length; i++) {
      const file = files[i];
      const mediaType = file.type.split("/")[0];
      const outputType = mediaType === "audio" ? "music" : mediaType;

      const data: Omit<MediaItem, "id"> = {
        projectId,
        kind: "uploaded",
        createdAt: Date.now(),
        mediaType: outputType as MediaType,
        status: "completed",
        url: file.url,
      };

      const mediaId = await db.media.create(data);
      const media = await db.media.find(mediaId as string);

      if (media) {
        const mediaMetadata = await getMediaMetadata(media as MediaItem);

        await db.media
          .update(media.id, {
            ...media,
            metadata: mediaMetadata?.media || {},
          })
          .finally(() => {
            queryClient.invalidateQueries({
              queryKey: queryKeys.projectMediaItems(projectId),
            });
          });
      }
    }
  };

  return (
    <div className="h-full w-full flex flex-col border-r border-border glassmorphism bg-transparent">
      <div className="p-4 flex items-center gap-4 border-b border-border">
        <div className="flex w-full">
          <Accordion type="single" collapsible className="w-full">
            <AccordionItem value="item-1" className="border-b-0">
              <AccordionTrigger className="py-4 h-10">
                <div className="flex flex-row items-center">
                  <h2 className="text-sm text-muted-foreground font-semibold flex-1">
                    {project?.title || "Project Settings"}
                  </h2>
                </div>
              </AccordionTrigger>
              <AccordionContent className="border-b-0">
                <div className="flex flex-col gap-4">
                  <Input
                    id="projectName"
                    name="name"
                    placeholder="untitled"
                    value={project.title}
                    onChange={(e) =>
                      projectUpdate.mutate({ title: e.target.value })
                    }
                    onBlur={(e) =>
                      projectUpdate.mutate({ title: e.target.value.trim() })
                    }
                  />

                  <Textarea
                    id="projectDescription"
                    name="description"
                    placeholder="Describe your video"
                    className="resize-none"
                    value={project.description}
                    rows={6}
                    onChange={(e) =>
                      projectUpdate.mutate({ description: e.target.value })
                    }
                    onBlur={(e) =>
                      projectUpdate.mutate({
                        description: e.target.value.trim(),
                      })
                    }
                  />
                </div>
              </AccordionContent>
            </AccordionItem>
          </Accordion>
        </div>
        <div className="self-start">
          <Button
            className="mt-2"
            variant="secondary"
            size="sm"
            onClick={() => setProjectDialogOpen(true)}
          >
            <FolderOpenIcon className="w-4 h-4 opacity-50" />
          </Button>
        </div>
      </div>
      <div className="flex-1 py-4 flex flex-col gap-4 border-b border-border h-full overflow-hidden relative">
        <div className="flex flex-row items-center gap-2 px-4">
          <h2 className="text-sm text-muted-foreground font-semibold flex-1">
            Gallery
          </h2>
          <div className="flex gap-2">
            <DropdownMenu>
              <DropdownMenuTrigger asChild>
                <Button variant="ghost" size="sm" className="px-2">
                  <ListPlusIcon className="w-4 h-4 opacity-50" />
                  <span className="capitalize">{mediaType}</span>
                  <ChevronDown className="w-4 h-4 opacity-50" />
                </Button>
              </DropdownMenuTrigger>
              <DropdownMenuContent side="bottom" align="start">
                <DropdownMenuItem
                  className="text-sm"
                  onClick={() => setMediaType("all")}
                >
                  <GalleryVerticalIcon className="w-4 h-4 opacity-50" />
                  All
                </DropdownMenuItem>
                <DropdownMenuItem
                  className="text-sm"
                  onClick={() => setMediaType("image")}
                >
                  <ImageIcon className="w-4 h-4 opacity-50" />
                  Image
                </DropdownMenuItem>
                <DropdownMenuItem
                  className="text-sm"
                  onClick={() => setMediaType("music")}
                >
                  <MusicIcon className="w-4 h-4 opacity-50" />
                  Music
                </DropdownMenuItem>
                <DropdownMenuItem
                  className="text-sm"
                  onClick={() => setMediaType("voiceover")}
                >
                  <MicIcon className="w-4 h-4 opacity-50" />
                  Voiceover
                </DropdownMenuItem>
                <DropdownMenuItem
                  className="text-sm"
                  onClick={() => setMediaType("video")}
                >
                  <FilmIcon className="w-4 h-4 opacity-50" />
                  Video
                </DropdownMenuItem>
              </DropdownMenuContent>
            </DropdownMenu>
            <Button
              variant="secondary"
              size="sm"
              disabled={isUploading}
              className="cursor-pointer disabled:cursor-default disabled:opacity-50"
              asChild
            >
              <label htmlFor="fileUploadButton">
                <Input
                  id="fileUploadButton"
                  type="file"
                  className="hidden"
                  onChange={handleFileUpload}
                  multiple={false}
                  disabled={isUploading}
                  accept="image/*,audio/*,video/*"
                />
                {isUploading ? (
                  <LoaderCircleIcon className="w-4 h-4 opacity-50 animate-spin" />
                ) : (
                  <CloudUploadIcon className="w-4 h-4 opacity-50" />
                )}
              </label>
            </Button>
          </div>
          {mediaItems.length > 0 && (
            <Button
              variant="secondary"
              size="sm"
              onClick={() => openGenerateDialog()}
            >
              <SparklesIcon className="w-4 h-4 opacity-50" />
              Generate...
            </Button>
          )}
        </div>
        {!isLoading && mediaItems.length === 0 && (
          <div className="h-full flex flex-col items-center justify-center gap-4 px-4">
            <p className="text-sm text-center">
              Create your image, audio and voiceover collection to compose your
              videos
            </p>
            <Button
              variant="secondary"
              size="sm"
              onClick={() => openGenerateDialog()}
            >
              <ImagePlusIcon className="w-4 h-4 opacity-50" />
              Generate...
            </Button>
          </div>
        )}

        {mediaItems.length > 0 && (
          <MediaItemPanel
            data={mediaItems}
            mediaType={mediaType}
            className="overflow-y-auto"
          />
        )}
        <div className="absolute bottom-0 left-0 right-0 bg-gradient-to-t from-background to-transparent via-background via-60% h-8 pointer-events-none" />
      </div>
    </div>
  );
}

================
File: src/components/logo.tsx
================
"use client";

import Image from "next/image";
import Link from "next/link";
import config from "@/lib/config";

export function Logo() {
  return (
    <Link href={config.urls.main} className="flex items-center space-x-2">
      <div className="relative w-10 h-10 flex items-center justify-center">
        <Image
          src="/logo.png"
          alt="NovelVision AI Logo"
          width={40}
          height={40}
          className="rounded-full"
        />
      </div>
      <span className="font-semibold text-lg text-foreground">
        NovelVision AI
      </span>
    </Link>
  );
}

================
File: src/components/main.tsx
================
"use client";

import BottomBar from "@/components/bottom-bar";
import Header from "@/components/header";
import RightPanel from "@/components/right-panel";
import VideoPreview from "@/components/video-preview";
import {
  VideoProjectStoreContext,
  createVideoProjectStore,
  useVideoProjectStore,
} from "@/data/store";
import { QueryClient, QueryClientProvider } from "@tanstack/react-query";
import { useRef, useState } from "react";
import { useStore } from "zustand";
import { ProjectDialog } from "./project-dialog";
import { MediaGallerySheet } from "./media-gallery";
import { ToastProvider } from "./ui/toast";
import { Toaster } from "./ui/toaster";
import { ExportDialog } from "./export-dialog";
import LeftPanel from "./left-panel";
import { KeyDialog } from "./key-dialog";
import { StoryboardPanel } from "./storyboard-panel";
import { fal } from "@/lib/fal";
import Footer from "@/components/footer";

type AppProps = {
  projectId: string;
};

export function App({ projectId }: AppProps) {
  const [keyDialog, setKeyDialog] = useState(false);

  const queryClient = useRef(new QueryClient()).current;
  const projectStore = useRef(
    createVideoProjectStore({
      projectId,
    }),
  ).current;
  const projectDialogOpen = useStore(projectStore, (s) => s.projectDialogOpen);
  const selectedMediaId = useStore(projectStore, (s) => s.selectedMediaId);
  const setSelectedMediaId = useStore(
    projectStore,
    (s) => s.setSelectedMediaId,
  );
  const handleOnSheetOpenChange = (open: boolean) => {
    if (!open) {
      setSelectedMediaId(null);
    }
  };
  const isExportDialogOpen = useStore(projectStore, (s) => s.exportDialogOpen);
  const setExportDialogOpen = useStore(
    projectStore,
    (s) => s.setExportDialogOpen,
  );

  const handleGenerateImage = async (
    prompt: string,
    modelId?: string,
    aspectRatio?: string,
  ) => {
    try {
      // Direct submission using the fal client
      const endpoint = modelId || "fal-ai/flux";
      console.log(
        `Generating image with endpoint: ${endpoint}, aspect ratio: ${aspectRatio || "16:9"}`,
      );

      // Determine image_size based on aspectRatio - fix portrait orientation format
      const image_size =
        aspectRatio === "9:16" ? "portrait_16_9" : "landscape_16_9";

      const result = await fal.run(endpoint, {
        input: {
          prompt,
          image_size,
          num_inference_steps: 12,
          guidance_scale: 3.5,
          enable_safety_checker: true,
        },
      });

      console.log("Image generation result:", result);

      // Based on the actual response structure, extract the image URL
      // @ts-ignore - The result structure might vary depending on the endpoint
      const imageUrl = result?.data?.images?.[0]?.url;

      if (imageUrl) {
        console.log("Generated image URL:", imageUrl);
        return imageUrl;
      } else {
        console.error("No image URL in response:", result);
        throw new Error("Image generation failed");
      }
    } catch (error) {
      console.error("Failed to generate image:", error);
      throw error;
    }
  };

  const handleSaveToMediaManager = async (imageUrl: string) => {
    // Get the store from the existing projectStore instead of using the hook again
    const setGenerateData = useStore(projectStore, (s) => s.setGenerateData);
    const openGenerateDialog = useStore(
      projectStore,
      (s) => s.openGenerateDialog,
    );

    setGenerateData({
      type: "image",
      image: imageUrl,
      metadata: {
        source: "storyboard",
      },
    });

    openGenerateDialog("image");
  };

  return (
    <ToastProvider>
      <QueryClientProvider client={queryClient}>
        <VideoProjectStoreContext.Provider value={projectStore}>
          <div className="flex flex-col relative min-h-screen overflow-auto bg-background">
            <StoryboardPanel
              onGenerateImage={handleGenerateImage}
              onSaveToMediaManager={handleSaveToMediaManager}
            />
            <Header openKeyDialog={() => setKeyDialog(true)} />
            <main className="flex overflow-x-auto flex-1 w-full">
              <div className="w-[400px] min-w-[400px]">
                <LeftPanel />
              </div>
              <div className="flex flex-col flex-1">
                <VideoPreview />
                <BottomBar />
              </div>
            </main>
            <RightPanel />
            <Footer />
          </div>
          <Toaster />
          <ProjectDialog open={projectDialogOpen} />
          <ExportDialog
            open={isExportDialogOpen}
            onOpenChange={setExportDialogOpen}
          />
          <KeyDialog
            open={keyDialog}
            onOpenChange={(open) => setKeyDialog(open)}
          />
          <MediaGallerySheet
            open={selectedMediaId !== null}
            onOpenChange={handleOnSheetOpenChange}
            selectedMediaId={selectedMediaId ?? ""}
          />
        </VideoProjectStoreContext.Provider>
      </QueryClientProvider>
    </ToastProvider>
  );
}

================
File: src/components/media-gallery.tsx
================
import {
  ComponentProps,
  HTMLAttributes,
  MouseEventHandler,
  PropsWithChildren,
  useMemo,
} from "react";
import {
  Sheet,
  SheetDescription,
  SheetHeader,
  SheetOverlay,
  SheetPanel,
  SheetPortal,
  SheetTitle,
} from "./ui/sheet";
import {
  queryKeys,
  refreshVideoCache,
  useProjectMediaItems,
} from "@/data/queries";
import { useProjectId, useVideoProjectStore } from "@/data/store";
import { cn, resolveMediaUrl } from "@/lib/utils";
import { MediaItem } from "@/data/schema";
import {
  CopyIcon,
  DownloadIcon,
  FilmIcon,
  ImagesIcon,
  MicIcon,
  MusicIcon,
  TrashIcon,
} from "lucide-react";
import { Button } from "./ui/button";
import { Separator } from "./ui/separator";
import { formatDuration } from "date-fns";
import { useMutation, useQueryClient } from "@tanstack/react-query";
import { db } from "@/data/db";
import { LoadingIcon } from "./ui/icons";
import { AVAILABLE_ENDPOINTS } from "@/lib/fal";
import { DownloadButton } from "./ui/download-button";

type MediaGallerySheetProps = ComponentProps<typeof Sheet> & {
  selectedMediaId: string;
};

type AudioPlayerProps = {
  media: MediaItem;
} & HTMLAttributes<HTMLAudioElement>;

function AudioPlayer({ media, ...props }: AudioPlayerProps) {
  const src = resolveMediaUrl(media);
  if (!src) return null;

  return (
    <div className="flex flex-col gap-4">
      <div className="aspect-square bg-accent text-muted-foreground flex flex-col items-center justify-center">
        {media.mediaType === "music" && <MusicIcon className="w-1/2 h-1/2" />}
        {media.mediaType === "voiceover" && <MicIcon className="w-1/2 h-1/2" />}
      </div>
      <div>
        <audio src={src} {...props} controls className="rounded" />
      </div>
    </div>
  );
}

type MediaPropertyItemProps = {
  className?: string;
  label: string;
  value: string;
};

function MediaPropertyItem({
  children,
  className,
  label,
  value,
}: PropsWithChildren<MediaPropertyItemProps>) {
  return (
    <div
      className={cn(
        "group relative flex flex-col gap-1 rounded bg-black/50 p-3 text-sm flex-wrap text-wrap overflow-hidden",
        className,
      )}
    >
      <div className="absolute right-2 top-2 opacity-30 transition-opacity group-hover:opacity-70">
        <Button
          variant="ghost"
          size="icon"
          onClick={() => {
            navigator.clipboard.writeText(value);
          }}
        >
          <CopyIcon className="w-4 h-4" />
        </Button>
      </div>
      <div className="font-medium text-muted-foreground">{label}</div>
      <div className="font-semibold text-foreground text-ellipsis">
        {children ?? value}
      </div>
    </div>
  );
}

const MEDIA_PLACEHOLDER: MediaItem = {
  id: "placeholder",
  kind: "generated",
  input: { prompt: "n/a" },
  mediaType: "image",
  status: "pending",
  createdAt: 0,
  endpointId: "n/a",
  projectId: "",
  requestId: "",
};

export function MediaGallerySheet({
  selectedMediaId,
  ...props
}: MediaGallerySheetProps) {
  const projectId = useProjectId();
  const { data: mediaItems = [] } = useProjectMediaItems(projectId);
  const selectedMedia =
    mediaItems.find((media) => media.id === selectedMediaId) ??
    MEDIA_PLACEHOLDER;
  const setSelectedMediaId = useVideoProjectStore((s) => s.setSelectedMediaId);
  const openGenerateDialog = useVideoProjectStore((s) => s.openGenerateDialog);
  const setGenerateData = useVideoProjectStore((s) => s.setGenerateData);
  const setEndpointId = useVideoProjectStore((s) => s.setEndpointId);
  const setGenerateMediaType = useVideoProjectStore(
    (s) => s.setGenerateMediaType,
  );
  const onGenerate = useVideoProjectStore((s) => s.onGenerate);

  const handleOpenGenerateDialog = () => {
    setGenerateMediaType("video");
    const image = selectedMedia.output?.images?.[0]?.url;

    const endpoint = AVAILABLE_ENDPOINTS.find(
      (endpoint) => endpoint.category === "video",
    );

    setEndpointId(endpoint?.endpointId ?? AVAILABLE_ENDPOINTS[0].endpointId);

    setGenerateData({
      ...(selectedMedia.input || {}),
      image,
      duration: undefined,
    });
    setSelectedMediaId(null);
    openGenerateDialog();
  };

  const handleVary = () => {
    setGenerateMediaType(selectedMedia.mediaType);
    setEndpointId(selectedMedia.endpointId as string);
    setGenerateData(selectedMedia.input || {});
    setSelectedMediaId(null);
    onGenerate();
  };

  // Event handlers
  const preventClose: MouseEventHandler = (e) => {
    e.preventDefault();
    e.stopPropagation();
  };
  const close = () => {
    setSelectedMediaId(null);
  };
  const mediaUrl = useMemo(
    () => resolveMediaUrl(selectedMedia),
    [selectedMedia],
  );
  const prompt = selectedMedia?.input?.prompt;

  const queryClient = useQueryClient();
  const deleteMedia = useMutation({
    mutationFn: async () => {
      console.log("Attempting to delete media:", selectedMediaId);

      // Check if we have a valid media ID to delete
      if (!selectedMediaId || selectedMediaId === "placeholder") {
        console.error("Invalid media ID for deletion");
        return false;
      }

      try {
        // Try to delete using the database if available
        if (db && db.media && typeof db.media.delete === "function") {
          await db.media.delete(selectedMediaId);
          console.log("Media deleted successfully via database");
          return true;
        } else {
          // If database operations aren't available, just pretend the delete was successful
          // This allows the UI to continue working even without database support
          console.log(
            "No database delete function available, simulating success",
          );
          return true;
        }
      } catch (error) {
        console.error("Error deleting media:", error);
        // Return false to indicate failure, but don't throw - this keeps the UI responsive
        return false;
      }
    },
    onSuccess: (success) => {
      if (success) {
        console.log("Delete operation succeeded, refreshing data");
        // Try to invalidate queries if the queryClient methods are available
        try {
          queryClient.invalidateQueries({
            queryKey: queryKeys.projectMediaItems(projectId),
          });
          refreshVideoCache(queryClient, projectId);
        } catch (error) {
          console.error("Error refreshing queries:", error);
        }
      } else {
        console.log("Delete operation did not complete successfully");
      }
      // Always close the dialog, even if the operation wasn't successful
      // This prevents the user from getting stuck with an unresponsive UI
      close();
    },
    onError: (error) => {
      console.error("Delete mutation error:", error);
      // Always close the dialog on error
      close();
    },
  });
  return (
    <Sheet {...props}>
      <SheetOverlay className="pointer-events-none flex flex-col" />
      <SheetPortal>
        <div
          className="pointer-events-auto fixed inset-0 z-[51] mr-[42rem] flex flex-col items-center justify-center gap-4 px-32 py-16"
          onClick={close}
        >
          {!!mediaUrl && (
            <>
              {selectedMedia.mediaType === "image" && (
                <img
                  src={mediaUrl}
                  className="animate-fade-scale-in h-auto max-h-[90%] w-auto max-w-[90%] object-contain transition-all"
                  onClick={preventClose}
                />
              )}
              {selectedMedia.mediaType === "video" && (
                <video
                  src={mediaUrl}
                  className="animate-fade-scale-in h-auto max-h-[90%] w-auto max-w-[90%] object-contain transition-all"
                  controls
                  onClick={preventClose}
                />
              )}
              {(selectedMedia.mediaType === "music" ||
                selectedMedia.mediaType === "voiceover") && (
                <AudioPlayer media={selectedMedia} />
              )}
            </>
          )}
          <style jsx>{`
            @keyframes fadeScaleIn {
              from {
                opacity: 0;
                transform: scale(0.8);
              }
              to {
                opacity: 1;
                transform: scale(1);
              }
            }
            .animate-fade-scale-in {
              animation: fadeScaleIn 0.3s ease-out forwards;
            }
          `}</style>
        </div>
        <SheetPanel
          className="flex h-screen max-h-screen min-h-screen flex-col overflow-hidden sm:max-w-2xl"
          onPointerDownOutside={preventClose as any}
        >
          <SheetHeader>
            <div className="flex flex-row justify-between items-center">
              <SheetTitle>Media Gallery</SheetTitle>
              <Button
                variant="ghost"
                size="icon"
                onClick={close}
                title="Close dialog"
              >
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  width="24"
                  height="24"
                  viewBox="0 0 24 24"
                  fill="none"
                  stroke="currentColor"
                  strokeWidth="2"
                  strokeLinecap="round"
                  strokeLinejoin="round"
                  className="w-4 h-4"
                >
                  <line x1="18" y1="6" x2="6" y2="18"></line>
                  <line x1="6" y1="6" x2="18" y2="18"></line>
                </svg>
              </Button>
            </div>
            <SheetDescription className="sr-only">
              The b-roll for your video composition
            </SheetDescription>
          </SheetHeader>
          <div className="flex h-full max-h-full flex-1 flex-col gap-8 overflow-y-hidden">
            <div className="flex flex-col gap-4">
              <p className="text-muted-foreground">
                {prompt ?? <span className="italic">No description</span>}
              </p>
              <div></div>
            </div>
            <div className="flex flex-row gap-2">
              {selectedMedia?.mediaType === "image" && (
                <Button
                  onClick={handleOpenGenerateDialog}
                  variant="secondary"
                  disabled={deleteMedia.isPending}
                >
                  <FilmIcon className="w-4 h-4 opacity-50" />
                  Make Video
                </Button>
              )}
              <Button
                onClick={handleVary}
                variant="secondary"
                disabled={deleteMedia.isPending}
              >
                <ImagesIcon className="w-4 h-4 opacity-50" />
                Re-run
              </Button>
              {mediaUrl &&
                (selectedMedia.mediaType === "image" ||
                  selectedMedia.mediaType === "video") && (
                  <Button
                    variant="secondary"
                    disabled={deleteMedia.isPending}
                    onClick={() => {
                      const handleDownload = async () => {
                        try {
                          const response = await fetch(mediaUrl);
                          const blob = await response.blob();

                          const blobUrl = window.URL.createObjectURL(blob);

                          const fileExtension =
                            selectedMedia.mediaType === "video" ? "mp4" : "png";

                          const a = document.createElement("a");
                          a.href = blobUrl;
                          a.download = `novelvision-media-${selectedMedia.id}.${fileExtension}`;

                          document.body.appendChild(a);
                          a.click();

                          window.URL.revokeObjectURL(blobUrl);
                          document.body.removeChild(a);
                        } catch (error) {
                          console.error(
                            `Error downloading ${selectedMedia.mediaType}:`,
                            error,
                          );
                        }
                      };

                      handleDownload();
                    }}
                  >
                    <DownloadIcon className="mr-1 h-4 w-4 opacity-70" />
                    Download
                  </Button>
                )}
              <Button
                variant="secondary"
                disabled={deleteMedia.isPending}
                onClick={() => {
                  console.log(
                    "Delete button clicked for media:",
                    selectedMediaId,
                  );
                  deleteMedia.mutate();
                }}
                className={cn(
                  "relative",
                  deleteMedia.isPending ? "bg-red-500/20 text-red-500" : "",
                  deleteMedia.isError ? "bg-orange-500/20 text-orange-500" : "",
                )}
              >
                {deleteMedia.isPending ? (
                  <LoadingIcon className="mr-1 h-4 w-4" />
                ) : (
                  <TrashIcon className="mr-1 h-4 w-4 opacity-70" />
                )}
                {deleteMedia.isPending ? "Deleting..." : "Delete"}

                {deleteMedia.isError && (
                  <span className="absolute -top-8 left-0 right-0 text-xs bg-red-500 text-white p-1 rounded">
                    Error deleting
                  </span>
                )}
              </Button>
            </div>
            <div className="flex-1 flex flex-col gap-2 justify-end">
              <MediaPropertyItem label="Media URL" value={mediaUrl ?? "n/a"} />
              <MediaPropertyItem
                label="Model (fal endpoint)"
                value={selectedMedia.endpointId ?? "n/a"}
              >
                <a
                  href={`https://fal.ai/models/${selectedMedia.endpointId}`}
                  target="_blank"
                  className="underline underline-offset-4 decoration-muted-foreground/70 decoration-dotted"
                >
                  <code>{selectedMedia.endpointId}</code>
                </a>
              </MediaPropertyItem>
              <MediaPropertyItem
                label="Status"
                value={selectedMedia.status ?? "n/a"}
              />
              <MediaPropertyItem
                label="Request ID"
                value={selectedMedia.requestId ?? "n/a"}
              >
                <code>{selectedMedia.requestId}</code>
              </MediaPropertyItem>
            </div>
          </div>
        </SheetPanel>
      </SheetPortal>
    </Sheet>
  );
}

================
File: src/components/media-panel.tsx
================
import { db } from "@/data/db";
import { queryKeys } from "@/data/queries";
import type { MediaItem } from "@/data/schema";
import { useProjectId, useVideoProjectStore } from "@/data/store";
import { fal } from "@/lib/fal";
import { cn, resolveMediaUrl, trackIcons } from "@/lib/utils";
import { useQuery, useQueryClient } from "@tanstack/react-query";
import { formatDistanceToNow } from "date-fns";
import {
  CircleXIcon,
  DownloadIcon,
  GripVerticalIcon,
  HourglassIcon,
  ImageIcon,
  MicIcon,
  MusicIcon,
  VideoIcon,
} from "lucide-react";
import {
  type DragEventHandler,
  Fragment,
  type HTMLAttributes,
  createElement,
} from "react";
import { Badge } from "./ui/badge";
import { Button } from "./ui/button";
import { LoadingIcon } from "./ui/icons";
import { useToast } from "@/hooks/use-toast";
import { getMediaMetadata } from "@/lib/ffmpeg";

type MediaItemRowProps = {
  data: MediaItem;
  onOpen: (data: MediaItem) => void;
  draggable?: boolean;
} & HTMLAttributes<HTMLDivElement>;

export function MediaItemRow({
  data,
  className,
  onOpen,
  draggable = true,
  ...props
}: MediaItemRowProps) {
  const isDone = data.status === "completed" || data.status === "failed";
  const queryClient = useQueryClient();
  const projectId = useProjectId();
  const { toast } = useToast();
  useQuery({
    queryKey: queryKeys.projectMedia(projectId, data.id),
    queryFn: async () => {
      if (data.kind === "uploaded") return null;
      const queueStatus = await fal.queue.status(data.endpointId, {
        requestId: data.requestId,
      });
      if (queueStatus.status === "IN_PROGRESS") {
        await db.media.update(data.id, {
          ...data,
          status: "running",
        });
        await queryClient.invalidateQueries({
          queryKey: queryKeys.projectMediaItems(data.projectId),
        });
      }
      let media: Partial<MediaItem> = {};

      if (queueStatus.status === "COMPLETED") {
        try {
          const result = await fal.queue.result(data.endpointId, {
            requestId: data.requestId,
          });
          media = {
            ...data,
            output: result.data,
            status: "completed",
          };

          await db.media.update(data.id, media);

          toast({
            title: "Generation completed",
            description: `Your ${data.mediaType} has been generated successfully.`,
          });
        } catch {
          await db.media.update(data.id, {
            ...data,
            status: "failed",
          });
          toast({
            title: "Generation failed",
            description: `Failed to generate ${data.mediaType}.`,
          });
        } finally {
          await queryClient.invalidateQueries({
            queryKey: queryKeys.projectMediaItems(data.projectId),
          });
        }

        if (media.mediaType !== "image") {
          const mediaMetadata = await getMediaMetadata(media as MediaItem);

          await db.media.update(data.id, {
            ...media,
            metadata: mediaMetadata?.media || {},
          });

          await queryClient.invalidateQueries({
            queryKey: queryKeys.projectMediaItems(data.projectId),
          });
        }
      }

      return null;
    },
    enabled: !isDone && data.kind === "generated",
    refetchInterval: data.mediaType === "video" ? 20000 : 1000,
  });
  const mediaUrl = resolveMediaUrl(data) ?? "";
  const mediaId = data.id.split("-")[0];
  const handleOnDragStart: DragEventHandler<HTMLDivElement> = (event) => {
    event.dataTransfer.setData("job", JSON.stringify(data));
    return true;
    // event.dataTransfer.dropEffect = "copy";
  };

  const coverImage =
    data.mediaType === "video"
      ? data.metadata?.start_frame_url || data?.metadata?.end_frame_url
      : resolveMediaUrl(data);

  return (
    <div
      className={cn(
        "flex items-start space-x-2 py-2 w-full px-4 hover:bg-accent transition-all",
        className,
      )}
      {...props}
      onClick={(e) => {
        e.stopPropagation();
        onOpen(data);
      }}
      draggable={draggable && data.status === "completed"}
      onDragStart={handleOnDragStart}
    >
      {!!draggable && (
        <div
          className={cn(
            "flex items-center h-full cursor-grab text-muted-foreground",
            {
              "text-muted": data.status !== "completed",
            },
          )}
        >
          <GripVerticalIcon className="w-4 h-4" />
        </div>
      )}
      <div className="w-16 h-16 aspect-square relative rounded overflow-hidden border border-transparent hover:border-accent bg-accent transition-all">
        {data.status === "completed" ? (
          <>
            {(data.mediaType === "image" || data.mediaType === "video") &&
              (coverImage ? (
                <img
                  src={coverImage}
                  alt="Generated media"
                  className="h-full w-full object-cover"
                />
              ) : (
                <div className="w-full h-full flex items-center justify-center top-0 left-0 absolute p-2 z-50">
                  {data.mediaType === "image" ? (
                    <ImageIcon className="w-7 h-7 text-muted-foreground" />
                  ) : (
                    <VideoIcon className="w-7 h-7 text-muted-foreground" />
                  )}
                </div>
              ))}
            {data.mediaType === "music" && (
              <div className="w-full h-full flex items-center justify-center top-0 left-0 absolute p-2 z-50">
                <MusicIcon className="w-7 h-7 text-muted-foreground" />
              </div>
            )}
            {data.mediaType === "voiceover" && (
              <div className="w-full h-full flex items-center justify-center top-0 left-0 absolute p-2 z-50">
                <MicIcon className="w-7 h-7 text-muted-foreground" />
              </div>
            )}
          </>
        ) : (
          <div className="w-full h-full bg-white/5 flex items-center justify-center text-muted-foreground">
            {data.status === "running" && <LoadingIcon className="w-8 h-8" />}
            {data.status === "pending" && (
              <HourglassIcon className="w-8 h-8 animate-spin ease-in-out delay-700 duration-1000" />
            )}
            {data.status === "failed" && (
              <CircleXIcon className="w-8 h-8 text-rose-700" />
            )}
          </div>
        )}
      </div>
      <div className="flex flex-col h-full gap-1 flex-1">
        <div className="flex flex-col items-start justify-center">
          <div className="flex w-full justify-between">
            <h3 className="text-sm font-medium flex flex-row gap-1 items-center">
              {createElement(trackIcons[data.mediaType], {
                className: "w-4 h-4 stroke-1",
              } as React.ComponentProps<
                (typeof trackIcons)[keyof typeof trackIcons]
              >)}
              <span>{data.kind === "generated" ? "Job" : "File"}</span>
              <code className="text-muted-foreground">#{mediaId}</code>
            </h3>
            {data.status !== "completed" && (
              <Badge
                variant="outline"
                className={cn({
                  "text-rose-700": data.status === "failed",
                  "text-sky-500": data.status === "running",
                  "text-muted-foreground": data.status === "pending",
                })}
              >
                {data.status}
              </Badge>
            )}
          </div>
          <p className="opacity-40 text-sm line-clamp-1 ">
            {data.input?.prompt}
          </p>
        </div>
        <div className="flex flex-row gap-2 justify-between">
          <span className="text-xs text-muted-foreground">
            {formatDistanceToNow(data.createdAt, { addSuffix: true })}
          </span>
          {data.status === "completed" && mediaUrl && (
            <div className="flex gap-1">
              <Button
                variant="ghost"
                size="icon"
                className="w-6 h-6"
                onClick={(e) => {
                  e.stopPropagation();
                  const handleDownload = async () => {
                    try {
                      const response = await fetch(mediaUrl);
                      const blob = await response.blob();

                      const blobUrl = window.URL.createObjectURL(blob);

                      const fileExt =
                        data.mediaType === "video"
                          ? "mp4"
                          : data.mediaType === "image"
                            ? "png"
                            : "mp3";

                      const a = document.createElement("a");
                      a.href = blobUrl;
                      a.download = `novelvision-${data.mediaType}-${mediaId}.${fileExt}`;

                      document.body.appendChild(a);
                      a.click();

                      window.URL.revokeObjectURL(blobUrl);
                      document.body.removeChild(a);
                    } catch (error) {
                      console.error(
                        `Error downloading ${data.mediaType}:`,
                        error,
                      );
                    }
                  };

                  handleDownload();
                }}
                title="Download"
              >
                <DownloadIcon className="w-3.5 h-3.5" />
              </Button>
            </div>
          )}
        </div>
      </div>
    </div>
  );
}

type MediaItemsPanelProps = {
  data: MediaItem[];
  mediaType: string;
} & HTMLAttributes<HTMLDivElement>;

export function MediaItemPanel({
  className,
  data,
  mediaType,
}: MediaItemsPanelProps) {
  const setSelectedMediaId = useVideoProjectStore((s) => s.setSelectedMediaId);
  const handleOnOpen = (item: MediaItem) => {
    setSelectedMediaId(item.id);
  };

  return (
    <div
      className={cn(
        "flex flex-col overflow-hidden divide-y divide-border w-[400px] min-w-[400px]",
        className,
      )}
    >
      {data
        .filter((media) => {
          if (mediaType === "all") return true;
          return media.mediaType === mediaType;
        })
        .map((media) => (
          <Fragment key={media.id}>
            <MediaItemRow data={media} onOpen={handleOnOpen} />
          </Fragment>
        ))}
    </div>
  );
}

================
File: src/components/model-helper.tsx
================
"use client";

import { useEffect, useState } from "react";
import { ChevronDown, ChevronUp, Copy, Check, Info } from "lucide-react";
import { Button } from "@/components/ui/button";
import {
  Tooltip,
  TooltipContent,
  TooltipProvider,
  TooltipTrigger,
} from "@/components/ui/tooltip";

interface ModelHint {
  label: string;
  category: string;
  headline: string;
  bestFor: string[];
  tips: string[];
  pricePerCreditUSD: number;
  secondsPerDollar: number;
  examplePrompt: string;
}

interface ModelHintsData {
  [key: string]: ModelHint;
}

export function ModelHelper({ modelId }: { modelId: string }) {
  const [modelHints, setModelHints] = useState<ModelHintsData | null>(null);
  const [isLoading, setIsLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);
  const [isExpanded, setIsExpanded] = useState(false);
  const [copied, setCopied] = useState(false);

  // Load model hints data
  useEffect(() => {
    const fetchModelHints = async () => {
      try {
        // Check if we already have the data in localStorage (for offline support)
        const cachedData = localStorage.getItem("modelHintsData");

        if (cachedData) {
          setModelHints(JSON.parse(cachedData));
          setIsLoading(false);
          return;
        }

        // If not cached, fetch from the public directory
        const response = await fetch("/model-hints.json");
        if (!response.ok) {
          throw new Error("Failed to load model hints data");
        }

        const data = await response.json();
        setModelHints(data);

        // Cache the data in localStorage
        localStorage.setItem("modelHintsData", JSON.stringify(data));
      } catch (err) {
        console.error("Error loading model hints:", err);
        setError("Failed to load model information");
      } finally {
        setIsLoading(false);
      }
    };

    fetchModelHints();
  }, []);

  // Check localStorage for expanded state on mount, default to collapsed
  useEffect(() => {
    const savedState = localStorage.getItem("modelHelperExpanded");
    if (savedState !== null) {
      setIsExpanded(savedState === "true");
    } else {
      // Default to collapsed
      setIsExpanded(false);
      // Save the default state to localStorage
      localStorage.setItem("modelHelperExpanded", "false");
    }
  }, []);

  // Save expanded state to localStorage when it changes
  useEffect(() => {
    localStorage.setItem("modelHelperExpanded", isExpanded.toString());
  }, [isExpanded]);

  // Reset copied state after 2 seconds
  useEffect(() => {
    if (copied) {
      const timer = setTimeout(() => {
        setCopied(false);
      }, 2000);
      return () => clearTimeout(timer);
    }
  }, [copied]);

  const toggleExpanded = () => {
    setIsExpanded(!isExpanded);
  };

  const copyToClipboard = (text: string) => {
    navigator.clipboard.writeText(text).then(
      () => {
        setCopied(true);
      },
      (err) => {
        console.error("Could not copy text: ", err);
      },
    );
  };

  if (isLoading) {
    return (
      <div className="w-full mt-3 mb-1 p-3 bg-gray-800/40 rounded-lg border border-gray-700/50 animate-pulse shadow-lg backdrop-blur-sm">
        <div className="h-5 bg-gray-700/50 rounded-full w-3/4 mb-2"></div>
        <div className="h-4 bg-gray-700/50 rounded-full w-1/2"></div>
      </div>
    );
  }

  if (error || !modelHints) {
    return (
      <div className="w-full mt-3 mb-1 p-3 bg-gray-800/40 rounded-lg border border-gray-700/50 text-gray-300 text-sm shadow-lg backdrop-blur-sm">
        <div className="flex items-center">
          <Info className="w-4 h-4 mr-2 text-blue-400/70" />
          <span>Model information unavailable</span>
        </div>
      </div>
    );
  }

  const modelHint = modelHints[modelId];

  if (!modelHint) {
    return (
      <div className="w-full mt-3 mb-1 p-3 bg-gray-800/40 rounded-lg border border-gray-700/50 text-gray-300 text-sm shadow-lg backdrop-blur-sm">
        <div className="flex items-center justify-between">
          <div className="flex items-center">
            <Info className="w-4 h-4 mr-2 text-blue-400/70" />
            <span>No tips available for this model yet</span>
          </div>
          <Button
            variant="ghost"
            size="sm"
            className="h-6 w-6 p-0 opacity-70 hover:opacity-100 transition-opacity"
            onClick={toggleExpanded}
          >
            {isExpanded ? (
              <ChevronUp className="h-4 w-4" />
            ) : (
              <ChevronDown className="h-4 w-4" />
            )}
          </Button>
        </div>
      </div>
    );
  }

  return (
    <div className="w-full h-full flex flex-col glassmorphism overflow-hidden transition-all duration-300 shadow-lg">
      {/* Header - always visible */}
      <div
        className="p-3 flex items-center justify-between cursor-pointer hover:bg-primary/20 transition-colors group"
        onClick={toggleExpanded}
      >
        <div className="flex items-center space-x-2">
          <div className="px-2 py-0.5 text-xs font-medium rounded-full bg-blue-500/20 text-blue-300 border border-blue-500/30 backdrop-blur-sm">
            {modelHint.category}
          </div>
          <h3 className="font-medium text-sm text-gray-100">
            {modelHint.label}
          </h3>
        </div>
        <Button
          variant="ghost"
          size="sm"
          className="h-6 w-6 p-0 opacity-70 group-hover:opacity-100 transition-opacity"
          onClick={(e) => {
            e.stopPropagation();
            toggleExpanded();
          }}
        >
          {isExpanded ? (
            <ChevronUp className="h-4 w-4" />
          ) : (
            <ChevronDown className="h-4 w-4" />
          )}
        </Button>
      </div>

      {/* Collapsible content */}
      <div
        className={`overflow-y-auto flex-grow transition-all duration-300 ease-in-out ${isExpanded ? "opacity-100" : "max-h-0 opacity-0"}`}
      >
        <div className="px-4 pb-4 pt-1 text-sm">
          <p className="text-gray-200 mb-3 leading-relaxed">
            {modelHint.headline}
          </p>

          {/* Best For Section */}
          <div className="mb-4">
            <h4 className="text-xs uppercase tracking-wider text-blue-300/80 mb-2 font-medium">
              Best For
            </h4>
            <div className="flex flex-wrap gap-2">
              {modelHint.bestFor.map((use, index) => (
                <span
                  key={index}
                  className="px-2.5 py-1 text-xs rounded-full bg-gray-800/70 text-gray-200 border border-gray-700/70 backdrop-blur-sm hover:bg-gray-700/50 transition-colors"
                >
                  {use}
                </span>
              ))}
            </div>
          </div>

          {/* Tips Section */}
          <div className="mb-4">
            <h4 className="text-xs uppercase tracking-wider text-blue-300/80 mb-2 font-medium">
              Prompt Tips
            </h4>
            <ul className="space-y-1.5 text-gray-200 text-xs pl-1">
              {modelHint.tips.map((tip, index) => (
                <li key={index} className="flex items-start">
                  <span className="inline-block w-1.5 h-1.5 rounded-full bg-blue-400/70 mt-1.5 mr-2 flex-shrink-0"></span>
                  <span>{tip}</span>
                </li>
              ))}
            </ul>
          </div>

          {/* Pricing Section */}
          <div className="mb-4 bg-blue-900/10 p-3 rounded-lg border border-blue-500/20 backdrop-blur-sm">
            <h4 className="text-xs uppercase tracking-wider text-blue-300/80 mb-2 font-medium">
              Pricing
            </h4>
            <div className="flex justify-between items-center">
              <div className="text-gray-200 text-sm">
                <span className="font-semibold">
                  ${modelHint.pricePerCreditUSD.toFixed(2)}
                </span>{" "}
                / 5s
              </div>
              <div className="text-gray-300 text-sm">
                ≈{" "}
                <span className="text-green-300 font-semibold">
                  {modelHint.secondsPerDollar}
                </span>{" "}
                seconds per $1
              </div>
            </div>
            <div className="mt-2 text-xs text-gray-400 italic">
              Based on fal.ai official API pricing. We use your API keys (BYOK)
              and don't charge additional fees.
            </div>
          </div>

          {/* Example Prompt */}
          <div>
            <h4 className="text-xs uppercase tracking-wider text-blue-300/80 mb-2 font-medium">
              Example Prompt
            </h4>
            <div className="relative group">
              <div className="bg-gray-900/40 p-3 rounded-lg border border-gray-700/50 text-gray-200 text-xs pr-9 max-h-24 overflow-y-auto leading-relaxed backdrop-blur-sm">
                {modelHint.examplePrompt}
              </div>
              <TooltipProvider>
                <Tooltip>
                  <TooltipTrigger asChild>
                    <Button
                      variant="ghost"
                      size="sm"
                      className="absolute top-2 right-2 h-6 w-6 p-0 opacity-70 group-hover:opacity-100 transition-opacity bg-gray-800/50 hover:bg-gray-700/70 backdrop-blur-sm"
                      onClick={() => copyToClipboard(modelHint.examplePrompt)}
                    >
                      {copied ? (
                        <Check className="h-3.5 w-3.5 text-green-400" />
                      ) : (
                        <Copy className="h-3.5 w-3.5 text-gray-300" />
                      )}
                    </Button>
                  </TooltipTrigger>
                  <TooltipContent>
                    <p>{copied ? "Copied!" : "Copy to clipboard"}</p>
                  </TooltipContent>
                </Tooltip>
              </TooltipProvider>
            </div>
          </div>
        </div>
      </div>
    </div>
  );
}

================
File: src/components/novel-vision-logo.tsx
================
"use client";

import Link from "next/link";
import config from "@/lib/config";

export function NovelVisionLogo() {
  return (
    <Link href={config.urls.main} className="flex items-center space-x-2">
      <div className="relative w-9 h-9 flex items-center justify-center">
        <svg
          width="36"
          height="36"
          viewBox="0 0 200 200"
          fill="none"
          xmlns="/public/logo.png"
          className="text-foreground"
        >
          {/* Simplified NovelVision AI logo as SVG */}
          <circle
            cx="100"
            cy="100"
            r="90"
            fill="url(#purpleGradient)"
            stroke="currentColor"
            strokeWidth="6"
          />
          <path
            d="M60,80 Q80,100 60,120"
            stroke="#60A5FA"
            strokeWidth="8"
            strokeLinecap="round"
          />
          <path
            d="M80,60 Q100,80 120,60"
            stroke="#60A5FA"
            strokeWidth="8"
            strokeLinecap="round"
          />
          <path
            d="M140,80 Q120,100 140,120"
            stroke="#60A5FA"
            strokeWidth="8"
            strokeLinecap="round"
          />
          <path
            d="M80,140 Q100,120 120,140"
            stroke="#60A5FA"
            strokeWidth="8"
            strokeLinecap="round"
          />
          <path
            d="M100,50 L100,150"
            stroke="#F9FAFB"
            strokeWidth="4"
            strokeLinecap="round"
          />
          <path
            d="M50,100 L150,100"
            stroke="#F9FAFB"
            strokeWidth="4"
            strokeLinecap="round"
          />

          {/* Feather element */}
          <path
            d="M140,70 Q160,90 140,130"
            stroke="#F9FAFB"
            strokeWidth="6"
            strokeLinecap="round"
          />
          <path
            d="M140,85 L160,65"
            stroke="#F9FAFB"
            strokeWidth="4"
            strokeLinecap="round"
          />
          <path
            d="M140,100 L170,70"
            stroke="#F9FAFB"
            strokeWidth="4"
            strokeLinecap="round"
          />
          <path
            d="M140,115 L170,85"
            stroke="#F9FAFB"
            strokeWidth="4"
            strokeLinecap="round"
          />

          {/* Glowing dots */}
          <circle cx="80" cy="80" r="6" fill="#60A5FA" />
          <circle cx="120" cy="80" r="6" fill="#60A5FA" />
          <circle cx="80" cy="120" r="6" fill="#60A5FA" />
          <circle cx="120" cy="120" r="6" fill="#60A5FA" />

          {/* Gradient definitions */}
          <defs>
            <linearGradient
              id="purpleGradient"
              x1="0"
              y1="0"
              x2="200"
              y2="200"
              gradientUnits="userSpaceOnUse"
            >
              <stop offset="0%" stopColor="#5B21B6" />
              <stop offset="100%" stopColor="#7C3AED" />
            </linearGradient>
          </defs>
        </svg>
      </div>
      <span className="font-semibold text-lg text-foreground">
        NovelVision AI
      </span>
    </Link>
  );
}

================
File: src/components/project-dialog.tsx
================
"use client";

import { useProjectCreator } from "@/data/mutations";
import { queryKeys, useProjects } from "@/data/queries";
import type { AspectRatio, VideoProject } from "@/data/schema";
import { useVideoProjectStore } from "@/data/store";
import { useToast } from "@/hooks/use-toast";
import { createProjectSuggestion } from "@/lib/project";
import { cn, rememberLastProjectId } from "@/lib/utils";
import { useMutation, useQueryClient } from "@tanstack/react-query";
import { FileVideoIcon, FolderOpenIcon, WandSparklesIcon } from "lucide-react";
import { useEffect, useState } from "react";
import { Logo } from "./logo";
import { Button } from "./ui/button";
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from "./ui/dialog";
import { LoadingIcon } from "./ui/icons";
import { Input } from "./ui/input";
import { Separator } from "./ui/separator";
import { Skeleton } from "./ui/skeleton";
import { Textarea } from "./ui/textarea";
import { WithTooltip } from "./ui/tooltip";
import { seedDatabase } from "@/data/seed";

type ProjectDialogProps = {} & Parameters<typeof Dialog>[0];

export function ProjectDialog({ onOpenChange, ...props }: ProjectDialogProps) {
  const [title, setTitle] = useState("");
  const [description, setDescription] = useState("");
  const [aspect, setAspect] = useState<AspectRatio>("16:9");
  const queryClient = useQueryClient();
  const { toast } = useToast();

  // Fetch existing projects
  const { data: projects = [], isLoading } = useProjects();

  // Seed data with template project if empty
  useEffect(() => {
    if (projects.length === 0 && !isLoading) {
      seedDatabase().then(() => {
        queryClient.invalidateQueries({ queryKey: queryKeys.projects });
      });
    }
  }, [projects, isLoading]);

  // Create project mutation
  const setProjectId = useVideoProjectStore((s) => s.setProjectId);
  const createProject = useProjectCreator();

  const suggestProject = useMutation({
    mutationFn: async () => {
      return createProjectSuggestion();
    },
    onSuccess: (suggestion) => {
      setTitle(suggestion.title);
      setDescription(suggestion.description);
    },
    onError: (error) => {
      console.warn("Failed to create suggestion", error);
      toast({
        title: "Failed to create suggestion",
        description:
          "There was an unexpected error while generating a suggestion. Try again.",
      });
    },
  });

  const setProjectDialogOpen = useVideoProjectStore(
    (s) => s.setProjectDialogOpen,
  );

  const handleSelectProject = (project: VideoProject) => {
    setProjectId(project.id);
    setProjectDialogOpen(false);
    rememberLastProjectId(project.id);
  };

  const handleOnOpenChange = (isOpen: boolean) => {
    if (!isOpen) {
      setTitle("");
      setDescription("");
    }
    onOpenChange?.(isOpen);
    setProjectDialogOpen(isOpen);
  };

  return (
    <Dialog {...props} onOpenChange={handleOnOpenChange}>
      <DialogContent className="flex flex-col max-w-4xl h-fit max-h-[520px] min-h-[380px]">
        <DialogHeader>
          <div className="flex flex-row gap-2 mb-4">
            <span className="text-lg font-medium">
              <Logo />
            </span>
          </div>
          <DialogTitle className="sr-only">New Project</DialogTitle>
          <DialogDescription className="sr-only">
            Create a new or open an existent project
          </DialogDescription>
        </DialogHeader>
        <div className="flex flex-row gap-8 h-full">
          {/* New Project Form */}
          <div className="flex flex-col flex-1 gap-8">
            <h2 className="text-lg font-semibold flex flex-row gap-2">
              <FileVideoIcon className="w-6 h-6 opacity-50 stroke-1" />
              Create New Project
            </h2>
            <div className="flex flex-col gap-4">
              <Input
                placeholder="Project Title"
                value={title}
                onChange={(e) => setTitle(e.target.value)}
              />
              <Textarea
                placeholder="Describe your project"
                value={description}
                onChange={(e) => setDescription(e.target.value)}
                rows={6}
                className="resize-none"
              />
              <div>
                <h4 className="text-xs text-muted-foreground mb-1">
                  Aspect Ratio:
                </h4>
                <div className="flex flex-row gap-2">
                  <Button
                    variant={aspect === "16:9" ? "secondary" : "outline"}
                    onClick={() => {
                      setAspect("16:9");
                    }}
                  >
                    16:9
                  </Button>
                  <Button
                    variant={aspect === "9:16" ? "secondary" : "outline"}
                    onClick={() => {
                      setAspect("9:16");
                    }}
                  >
                    9:16
                  </Button>
                </div>
              </div>
            </div>
            <div className="flex-1 flex flex-row items-end justify-start gap-2">
              <WithTooltip tooltip="Out of ideas? Generate a new random project.">
                <Button
                  variant="secondary"
                  disabled={suggestProject.isPending}
                  onClick={() => suggestProject.mutate()}
                >
                  {suggestProject.isPending ? (
                    <LoadingIcon />
                  ) : (
                    <WandSparklesIcon className="opacity-50" />
                  )}
                  Generate
                </Button>
              </WithTooltip>
              <Button
                onClick={() =>
                  createProject.mutate(
                    {
                      title,
                      description,
                      aspectRatio: aspect,
                    },
                    {
                      onSuccess: (projectId) => {
                        handleSelectProject({ id: projectId } as VideoProject);
                      },
                    },
                  )
                }
                disabled={!title.trim() || createProject.isPending}
              >
                {createProject.isPending ? "Creating..." : "Create Project"}
              </Button>
            </div>
          </div>

          <div className="flex flex-col gap-2 items-center">
            <Separator orientation="vertical" className="flex-1" />
            <span className="font-semibold">or</span>
            <Separator orientation="vertical" className="flex-1" />
          </div>

          {/* Existing Projects */}
          <div className="flex flex-col flex-1 gap-8">
            <h2 className="text-lg font-semibold flex flex-row gap-2">
              <FolderOpenIcon className="w-6 h-6 opacity-50 stroke-1" />
              Open Existing Project
            </h2>
            <div className="flex flex-col gap-2 max-h-[300px] overflow-y-auto">
              {isLoading ? (
                // Loading skeletons
                <>
                  {[1, 2, 3].map((i) => (
                    <Skeleton key={i} className="w-full h-[72px] rounded-lg" />
                  ))}
                </>
              ) : projects?.length === 0 ? (
                <div className="text-center text-sm text-muted-foreground py-8">
                  No projects found
                </div>
              ) : (
                // Project list
                projects?.map((project) => (
                  <button
                    type="button"
                    key={project.id}
                    onClick={() => handleSelectProject(project)}
                    className={cn(
                      "w-full text-left p-3 rounded",
                      "bg-card hover:bg-accent transition-colors",
                      "border border-border",
                    )}
                  >
                    <h3 className="font-medium text-sm">{project.title}</h3>
                    {project.description && (
                      <p className="text-sm text-muted-foreground line-clamp-2">
                        {project.description}
                      </p>
                    )}
                  </button>
                ))
              )}
            </div>
          </div>
        </div>
        <DialogFooter>
          <p className="text-muted-foreground text-sm mt-4 w-full text-center">
            Project management for your creative work
          </p>
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
}

================
File: src/components/right-panel.tsx
================
"use client";

import { useJobCreator } from "@/data/mutations";
import { queryKeys, useProject, useProjectMediaItems } from "@/data/queries";
import type { MediaItem } from "@/data/schema";
import {
  type GenerateData,
  type MediaType,
  useProjectId,
  useVideoProjectStore,
} from "@/data/store";
import { AVAILABLE_ENDPOINTS, type InputAsset } from "@/lib/fal";
import {
  ImageIcon,
  MicIcon,
  MusicIcon,
  LoaderCircleIcon,
  VideoIcon,
  ArrowLeft,
  TrashIcon,
  WandSparklesIcon,
  CrossIcon,
  XIcon,
} from "lucide-react";
import { MediaItemRow } from "./media-panel";
import { Button } from "./ui/button";
import { Input } from "./ui/input";
import { Textarea } from "./ui/textarea";

import { useEffect, useMemo, useState } from "react";
import { useUploadThing } from "@/lib/uploadthing";
import type { ClientUploadedFileData } from "uploadthing/types";
import { db } from "@/data/db";
import { useMutation, useQueryClient } from "@tanstack/react-query";
import { useToast } from "@/hooks/use-toast";
import {
  assetKeyMap,
  cn,
  getAssetKey,
  getAssetType,
  mapInputKey,
  resolveMediaUrl,
} from "@/lib/utils";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
  SelectGroup,
  SelectLabel,
} from "./ui/select";
import { enhancePrompt, LlmModelType } from "@/lib/prompt";
import { WithTooltip } from "./ui/tooltip";
import { Label } from "./ui/label";
import { VoiceSelector } from "./playht/voice-selector";
import { LoadingIcon } from "./ui/icons";
import { getMediaMetadata } from "@/lib/ffmpeg";
import CameraMovement from "./camera-control";
import { Sheet, SheetContent, SheetHeader, SheetTrigger } from "./ui/sheet";
import { ModelHelper } from "./model-helper";
import { PlusIcon } from "lucide-react";

type ModelEndpointPickerProps = {
  mediaType: string;
  onValueChange: (value: MediaType) => void;
} & Parameters<typeof Select>[0];

function ModelEndpointPicker({
  mediaType,
  ...props
}: ModelEndpointPickerProps) {
  const endpoints = useMemo(
    () =>
      AVAILABLE_ENDPOINTS.filter((endpoint) => endpoint.category === mediaType),
    [mediaType],
  );
  return (
    <Select {...props}>
      <SelectTrigger className="text-base w-full minw-56 font-semibold">
        <SelectValue />
      </SelectTrigger>
      <SelectContent>
        {endpoints.map((endpoint) => (
          <SelectItem key={endpoint.endpointId} value={endpoint.endpointId}>
            <div className="flex flex-row gap-2 items-center">
              <span>{endpoint.label}</span>
            </div>
          </SelectItem>
        ))}
      </SelectContent>
    </Select>
  );
}

type LlmModelPickerProps = {
  selectedModel: LlmModelType;
  onValueChange: (value: LlmModelType) => void;
} & Parameters<typeof Select>[0];

function LlmModelPicker({
  selectedModel,
  onValueChange,
  ...props
}: LlmModelPickerProps) {
  return (
    <Select
      defaultValue={selectedModel}
      onValueChange={onValueChange}
      {...props}
    >
      <SelectTrigger className="w-full">
        <SelectValue placeholder="Select LLM Model" />
      </SelectTrigger>
      <SelectContent>
        <SelectGroup>
          <SelectLabel>Meta Llama Models</SelectLabel>
          <SelectItem value="meta-llama/llama-3.2-1b-instruct">
            Llama 3.2 1B
          </SelectItem>
          <SelectItem value="meta-llama/llama-3.2-3b-instruct">
            Llama 3.2 3B
          </SelectItem>
          <SelectItem value="meta-llama/llama-3.1-8b-instruct">
            Llama 3.1 8B
          </SelectItem>
          <SelectItem value="meta-llama/llama-3.1-70b-instruct">
            Llama 3.1 70B
          </SelectItem>
        </SelectGroup>
        <SelectGroup>
          <SelectLabel>OpenAI Models</SelectLabel>
          <SelectItem value="openai/gpt-4o-mini">GPT-4o Mini</SelectItem>
          <SelectItem value="openai/gpt-4o">GPT-4o</SelectItem>
        </SelectGroup>
        <SelectGroup>
          <SelectLabel>Anthropic Models</SelectLabel>
          <SelectItem value="anthropic/claude-3.5-sonnet">
            Claude 3.5 Sonnet
          </SelectItem>
          <SelectItem value="anthropic/claude-3-5-haiku">
            Claude 3.5 Haiku
          </SelectItem>
          <SelectItem value="anthropic/claude-3-haiku">
            Claude 3 Haiku
          </SelectItem>
        </SelectGroup>
        <SelectGroup>
          <SelectLabel>Google Models</SelectLabel>
          <SelectItem value="google/gemini-pro-1.5">Gemini Pro 1.5</SelectItem>
          <SelectItem value="google/gemini-flash-1.5">
            Gemini Flash 1.5
          </SelectItem>
          <SelectItem value="google/gemini-flash-1.5-8b">
            Gemini Flash 1.5 8B
          </SelectItem>
        </SelectGroup>
      </SelectContent>
    </Select>
  );
}

export default function RightPanel({
  onOpenChange,
}: {
  onOpenChange?: (open: boolean) => void;
}) {
  const videoProjectStore = useVideoProjectStore((s) => s);
  const {
    generateData,
    setGenerateData,
    resetGenerateData,
    endpointId,
    setEndpointId,
  } = videoProjectStore;

  const [tab, setTab] = useState<string>("generation");
  const [assetMediaType, setAssetMediaType] = useState("all");
  const projectId = useProjectId();
  const openGenerateDialog = useVideoProjectStore((s) => s.openGenerateDialog);
  const generateDialogOpen = useVideoProjectStore((s) => s.generateDialogOpen);
  const closeGenerateDialog = useVideoProjectStore(
    (s) => s.closeGenerateDialog,
  );
  const queryClient = useQueryClient();

  const [open, setOpen] = useState(false);
  const [isPanelClosed, setIsPanelClosed] = useState(false);

  const handleOnOpenChange = (isOpen: boolean) => {
    setOpen(isOpen);
    if (!isOpen) {
      closeGenerateDialog();
      resetGenerateData();
      return;
    }
    if (onOpenChange) {
      onOpenChange(isOpen);
    }
    openGenerateDialog();
  };

  const togglePanel = () => {
    setIsPanelClosed(!isPanelClosed);
  };

  const { data: project } = useProject(projectId);

  const { toast } = useToast();
  const enhancePromptMutation = useMutation({
    mutationFn: async () => {
      return enhancePrompt(generateData.prompt, {
        type: mediaType as any,
        project: project,
        model: llmModel,
      });
    },
    onSuccess: (enhancedPrompt) => {
      setGenerateData({ prompt: enhancedPrompt });
    },
    onError: (error) => {
      console.warn("Failed to create suggestion", error);
      toast({
        title: "Failed to enhance prompt",
        description: "There was an unexpected error. Try again.",
      });
    },
  });

  const { data: mediaItems = [] } = useProjectMediaItems(projectId);
  const mediaType = useVideoProjectStore((s) => s.generateMediaType);
  const setMediaType = useVideoProjectStore((s) => s.setGenerateMediaType);
  const llmModel = useVideoProjectStore((s) => s.llmModel);
  const setLlmModel = useVideoProjectStore((s) => s.setLlmModel);

  const endpoint = useMemo(
    () =>
      AVAILABLE_ENDPOINTS.find(
        (endpoint) => endpoint.endpointId === endpointId,
      ),
    [endpointId],
  );
  const handleMediaTypeChange = (mediaType: string) => {
    setMediaType(mediaType as MediaType);
    const endpoint = AVAILABLE_ENDPOINTS.find(
      (endpoint) => endpoint.category === mediaType,
    );

    const initialInput = endpoint?.initialInput || {};

    if (
      (mediaType === "video" &&
        endpoint?.endpointId === "fal-ai/hunyuan-video") ||
      mediaType !== "video"
    ) {
      setGenerateData({ image: null, ...initialInput });
    } else {
      setGenerateData({ ...initialInput });
    }

    setEndpointId(endpoint?.endpointId ?? AVAILABLE_ENDPOINTS[0].endpointId);
  };
  // TODO improve model-specific parameters
  type InputType = {
    prompt: string;
    image_url?: File | string | null;
    video_url?: File | string | null;
    audio_url?: File | string | null;
    image_size?: { width: number; height: number } | string;
    aspect_ratio?: string;
    seconds_total?: number;
    voice?: string;
    input?: string;
    reference_audio_url?: File | string | null;
    advanced_camera_control?: {
      movement_value: number;
      movement_type: string;
    };
  };

  const aspectRatioMap = {
    "16:9": { image: "landscape_16_9", video: "16:9" },
    "9:16": { image: "portrait_16_9", video: "9:16" },
    "1:1": { image: "square_1_1", video: "1:1" },
  };

  let imageAspectRatio: string | { width: number; height: number } | undefined;
  let videoAspectRatio: string | undefined;

  if (project?.aspectRatio) {
    imageAspectRatio = aspectRatioMap[project.aspectRatio].image;
    videoAspectRatio = aspectRatioMap[project.aspectRatio].video;
  }

  const input: InputType = {
    prompt: generateData.prompt,
    image_url: undefined,
    image_size: imageAspectRatio,
    aspect_ratio: videoAspectRatio,
    seconds_total: generateData.duration ?? undefined,
    voice:
      endpointId === "fal-ai/playht/tts/v3" ? generateData.voice : undefined,
    input:
      endpointId === "fal-ai/playht/tts/v3" ? generateData.prompt : undefined,
  };

  if (generateData.image) {
    input.image_url = generateData.image;
  }
  if (generateData.video_url) {
    input.video_url = generateData.video_url;
  }
  if (generateData.audio_url) {
    input.audio_url = generateData.audio_url;
  }
  if (generateData.reference_audio_url) {
    input.reference_audio_url = generateData.reference_audio_url;
  }

  if (generateData.advanced_camera_control) {
    input.advanced_camera_control = generateData.advanced_camera_control;
  }

  const extraInput =
    endpointId === "fal-ai/f5-tts"
      ? {
          gen_text: generateData.prompt,
          ref_audio_url:
            "https://github.com/SWivid/F5-TTS/raw/21900ba97d5020a5a70bcc9a0575dc7dec5021cb/tests/ref_audio/test_en_1_ref_short.wav",
          ref_text: "Some call me nature, others call me mother nature.",
          model_type: "F5-TTS",
          remove_silence: true,
        }
      : {};
  const createJob = useJobCreator({
    projectId,
    endpointId:
      generateData.image && mediaType === "video"
        ? endpoint?.imageForFrame
          ? endpointId // For models like LTX Video that handle images directly
          : `${endpointId}/image-to-video` // For other models that need the /image-to-video endpoint
        : endpointId,
    mediaType,
    input: {
      ...(endpoint?.initialInput || {}),
      ...mapInputKey(input, endpoint?.inputMap || {}),
      ...extraInput,
    },
  });

  const handleOnGenerate = async () => {
    await createJob.mutateAsync({} as any, {
      onSuccess: async () => {
        if (!createJob.isError) {
          handleOnOpenChange(false);
        }
      },
    });
  };

  useEffect(() => {
    videoProjectStore.onGenerate = handleOnGenerate;
  }, [handleOnGenerate]);

  const handleSelectMedia = (media: MediaItem) => {
    const asset = endpoint?.inputAsset?.find((item) => {
      const assetType = getAssetType(item);

      if (
        assetType === "audio" &&
        (media.mediaType === "voiceover" || media.mediaType === "music")
      ) {
        return true;
      }
      return assetType === media.mediaType;
    });

    if (!asset) {
      setTab("generation");
      return;
    }

    setGenerateData({ [getAssetKey(asset)]: resolveMediaUrl(media) });
    setTab("generation");
  };

  const { startUpload, isUploading } = useUploadThing("fileUploader");

  const handleFileUpload = async (e: React.ChangeEvent<HTMLInputElement>) => {
    const files = e.target.files;
    if (!files) return;

    try {
      const uploadedFiles = await startUpload(Array.from(files));
      if (uploadedFiles) {
        await handleUploadComplete(uploadedFiles);
      }
    } catch (err) {
      console.warn(`ERROR! ${err}`);
      toast({
        title: "Failed to upload file",
        description: "Please try again",
      });
    }
  };

  const handleUploadComplete = async (
    files: ClientUploadedFileData<{
      uploadedBy: string;
    }>[],
  ) => {
    for (let i = 0; i < files.length; i++) {
      const file = files[i];
      const mediaType = file.type.split("/")[0];
      const outputType = mediaType === "audio" ? "music" : mediaType;

      const data: Omit<MediaItem, "id"> = {
        projectId,
        kind: "uploaded",
        createdAt: Date.now(),
        mediaType: outputType as MediaType,
        status: "completed",
        url: file.url,
      };

      setGenerateData({
        ...generateData,
        [assetKeyMap[outputType as keyof typeof assetKeyMap]]: file.url,
      });

      const mediaId = await db.media.create(data);
      const media = await db.media.find(mediaId as string);

      if (media && media.mediaType !== "image") {
        const mediaMetadata = await getMediaMetadata(media as MediaItem);

        await db.media
          .update(media.id, {
            ...media,
            metadata: mediaMetadata?.media || {},
          })
          .finally(() => {
            queryClient.invalidateQueries({
              queryKey: queryKeys.projectMediaItems(projectId),
            });
          });
      }
    }
  };

  const handleLlmModelChange = (model: LlmModelType) => {
    setLlmModel(model);
  };

  return (
    <>
      {isPanelClosed && generateDialogOpen && (
        <button
          onClick={togglePanel}
          className="fixed bottom-20 right-6 z-50 p-3 rounded-full bg-primary shadow-lg hover:bg-primary/90 transition-all duration-300 flex items-center justify-center floating-media-button"
          aria-label="Open Media Panel"
        >
          <span className="text-white font-medium flex items-center">
            <PlusIcon className="w-5 h-5" />
            <span className="ml-1">Generate Media</span>
          </span>
        </button>
      )}
      <div
        className={cn(
          "flex flex-col border-l border-border w-[560px] min-w-[560px] z-50 transition-all duration-300 fixed top-0 bottom-0 overflow-y-auto right-panel-container",
          generateDialogOpen
            ? isPanelClosed
              ? "right-[-560px]"
              : "right-0"
            : "-right-[560px]",
        )}
      >
        <div className="flex-1 p-4 flex flex-col gap-4 border-b border-border h-full overflow-y-auto relative">
          <div className="flex flex-row items-center justify-between">
            <h2 className="text-sm text-muted-foreground font-semibold flex-1">
              Generate Media
            </h2>
            <Button
              variant="ghost"
              size="icon"
              onClick={togglePanel}
              className="flex items-center gap-2 hover:bg-white/10 transition-colors"
            >
              <XIcon className="w-6 h-6" />
            </Button>
          </div>
          <div className="w-full flex flex-col">
            <div className="flex w-full gap-2">
              <Button
                variant="ghost"
                onClick={() => handleMediaTypeChange("image")}
                className={cn(
                  mediaType === "image" && "bg-primary/20",
                  "h-14 flex flex-col justify-center w-1/4 rounded-md gap-2 items-center",
                )}
              >
                <ImageIcon className="w-4 h-4 opacity-50" />
                <span className="text-[10px]">Image</span>
              </Button>
              <Button
                variant="ghost"
                onClick={() => handleMediaTypeChange("video")}
                className={cn(
                  mediaType === "video" && "bg-primary/20",
                  "h-14 flex flex-col justify-center w-1/4 rounded-md gap-2 items-center",
                )}
              >
                <VideoIcon className="w-4 h-4 opacity-50" />
                <span className="text-[10px]">Video</span>
              </Button>
              <Button
                variant="ghost"
                onClick={() => handleMediaTypeChange("voiceover")}
                className={cn(
                  mediaType === "voiceover" && "bg-primary/20",
                  "h-14 flex flex-col justify-center w-1/4 rounded-md gap-2 items-center",
                )}
              >
                <MicIcon className="w-4 h-4 opacity-50" />
                <span className="text-[10px]">Voiceover</span>
              </Button>
              <Button
                variant="ghost"
                onClick={() => handleMediaTypeChange("music")}
                className={cn(
                  mediaType === "music" && "bg-primary/20",
                  "h-14 flex flex-col justify-center w-1/4 rounded-md gap-2 items-center",
                )}
              >
                <MusicIcon className="w-4 h-4 opacity-50" />
                <span className="text-[10px]">Music</span>
              </Button>
            </div>
            <div className="two-column-container">
              <div className="model-selectors-column">
                <div className="flex flex-col gap-2 justify-start font-medium text-base">
                  <div className="text-muted-foreground font-medium">Using</div>
                  <ModelEndpointPicker
                    mediaType={mediaType}
                    value={endpointId}
                    onValueChange={(endpointId) => {
                      resetGenerateData();
                      setEndpointId(endpointId);

                      const endpoint = AVAILABLE_ENDPOINTS.find(
                        (endpoint) => endpoint.endpointId === endpointId,
                      );

                      const initialInput = endpoint?.initialInput || {};
                      setGenerateData({ ...initialInput });
                    }}
                  />

                  <div className="mt-2">
                    <div className="text-muted-foreground">
                      LLM Model for Prompt Enhancement
                    </div>
                    <LlmModelPicker
                      selectedModel={llmModel}
                      onValueChange={handleLlmModelChange}
                    />
                    <p className="text-xs text-muted-foreground mt-1">
                      Select the LLM model to use for enhancing prompts. More
                      powerful models may produce better results but may cost
                      more credits.
                    </p>
                  </div>
                </div>
              </div>

              <div className="helper-column">
                {/* Model Helper Panel */}
                <div className="h-full flex flex-col">
                  <ModelHelper modelId={endpointId} />
                </div>
              </div>
            </div>
          </div>
          <div className="flex flex-col gap-2 relative">
            {endpoint?.inputAsset?.map((asset, index) => (
              <div key={getAssetType(asset)} className="flex w-full">
                <div className="flex flex-col w-full" key={getAssetType(asset)}>
                  <div className="flex justify-between">
                    <h4 className="capitalize text-muted-foreground mb-1 text-sm">
                      {getAssetType(asset)} Reference
                    </h4>
                    {tab === `asset-${getAssetType(asset)}` && (
                      <Button
                        variant="ghost"
                        onClick={() => setTab("generation")}
                        size="sm"
                      >
                        <ArrowLeft /> Back
                      </Button>
                    )}
                  </div>
                  {(tab === "generation" ||
                    tab !== `asset-${getAssetType(asset)}`) && (
                    <>
                      {!generateData[getAssetKey(asset)] && (
                        <div className="flex gap-2 justify-between mb-3">
                          <Button
                            variant="ghost"
                            onClick={() => {
                              setTab(`asset-${getAssetType(asset)}`);
                              setAssetMediaType(getAssetType(asset) ?? "all");
                            }}
                            className="cursor-pointer min-h-[30px] flex-1 flex flex-col items-center justify-center border border-dashed border-border rounded-md px-2"
                          >
                            <span className="text-muted-foreground text-xs text-center text-nowrap">
                              Select
                            </span>
                          </Button>
                          <Button
                            variant="ghost"
                            size="sm"
                            disabled={isUploading}
                            className="cursor-pointer min-h-[30px] flex-1 flex flex-col items-center justify-center border border-dashed border-border rounded-md px-2"
                            asChild
                          >
                            <label htmlFor="assetUploadButton">
                              <Input
                                id="assetUploadButton"
                                type="file"
                                className="hidden"
                                onChange={handleFileUpload}
                                multiple={false}
                                disabled={isUploading}
                                accept="image/*,audio/*,video/*"
                              />
                              {isUploading ? (
                                <LoaderCircleIcon className="w-4 h-4 opacity-50 animate-spin" />
                              ) : (
                                <span className="text-muted-foreground text-xs text-center text-nowrap">
                                  Upload
                                </span>
                              )}
                            </label>
                          </Button>
                        </div>
                      )}
                      {generateData[getAssetKey(asset)] && (
                        <div className="cursor-pointer overflow-hidden relative w-full flex flex-col items-center justify-center border border-dashed border-border rounded-md bg-black/10 p-1">
                          <WithTooltip tooltip="Remove media">
                            <button
                              type="button"
                              className="p-1 rounded hover:bg-black/50 absolute top-1 z-50 bg-black/80 right-1 group-hover:text-white"
                              onClick={() =>
                                setGenerateData({
                                  [getAssetKey(asset)]: undefined,
                                })
                              }
                            >
                              <TrashIcon className="w-3 h-3 stroke-2" />
                            </button>
                          </WithTooltip>
                          {generateData[getAssetKey(asset)] && (
                            <SelectedAssetPreview
                              asset={asset}
                              data={generateData}
                            />
                          )}
                        </div>
                      )}
                    </>
                  )}
                  {tab === `asset-${getAssetType(asset)}` && (
                    <div className="flex items-center gap-2 flex-wrap overflow-y-auto max-h-80 divide-y divide-border">
                      {mediaItems
                        .filter((media) => {
                          if (assetMediaType === "all") return true;
                          if (
                            assetMediaType === "audio" &&
                            (media.mediaType === "voiceover" ||
                              media.mediaType === "music")
                          )
                            return true;
                          return media.mediaType === assetMediaType;
                        })
                        .map((job) => (
                          <MediaItemRow
                            draggable={false}
                            key={job.id}
                            data={job}
                            onOpen={handleSelectMedia}
                            className="cursor-pointer"
                          />
                        ))}
                    </div>
                  )}
                </div>
              </div>
            ))}
            <div className="relative bg-border rounded-lg pb-10 placeholder:text-base w-full resize-none">
              <Textarea
                className="text-base shadow-none focus:!ring-0 placeholder:text-base w-full h-24 resize-none"
                placeholder="Imagine..."
                value={generateData.prompt}
                rows={3}
                onChange={(e) => setGenerateData({ prompt: e.target.value })}
              />
              <WithTooltip tooltip="Enhance your prompt with AI-powered suggestions.">
                <div className="absolute bottom-2 right-2">
                  <Button
                    variant="secondary"
                    disabled={enhancePromptMutation.isPending}
                    className="bg-primary/20 text-white text-xs rounded-full h-6 px-3 hover:bg-primary/40 transition-colors"
                    onClick={() => enhancePromptMutation.mutate()}
                  >
                    {enhancePromptMutation.isPending ? (
                      <LoadingIcon />
                    ) : (
                      <WandSparklesIcon className="opacity-50" />
                    )}
                    Enhance Prompt
                  </Button>
                </div>
              </WithTooltip>
            </div>
          </div>

          {tab === "generation" && (
            <div className="flex flex-col gap-2 mb-2">
              {endpoint?.cameraControl && (
                <CameraMovement
                  value={generateData.advanced_camera_control}
                  onChange={(val) =>
                    setGenerateData({
                      advanced_camera_control: val
                        ? {
                            movement_value: val.value,
                            movement_type: val.movement,
                          }
                        : undefined,
                    })
                  }
                />
              )}
              {mediaType === "music" &&
                endpointId === "fal-ai/playht/tts/v3" && (
                  <div className="flex-1 flex flex-row gap-2">
                    {mediaType === "music" && (
                      <div className="flex flex-row items-center gap-1">
                        <Label>Duration</Label>
                        <Input
                          className="w-12 text-center tabular-nums [appearance:textfield] [&::-webkit-outer-spin-button]:appearance-none [&::-webkit-inner-spin-button]:appearance-none"
                          min={5}
                          max={30}
                          step={1}
                          type="number"
                          value={generateData.duration}
                          onChange={(e) =>
                            setGenerateData({
                              duration: Number.parseInt(e.target.value),
                            })
                          }
                        />
                        <span>s</span>
                      </div>
                    )}
                    {endpointId === "fal-ai/playht/tts/v3" && (
                      <VoiceSelector
                        value={generateData.voice}
                        onValueChange={(voice) => {
                          setGenerateData({ voice });
                        }}
                      />
                    )}
                  </div>
                )}
              <div className="flex flex-row gap-2">
                <Button
                  className="w-full bg-gradient-to-r from-blue-600 to-blue-700 hover:from-blue-700 hover:to-blue-800"
                  disabled={
                    enhancePromptMutation.isPending || createJob.isPending
                  }
                  onClick={handleOnGenerate}
                >
                  Generate
                </Button>
              </div>
            </div>
          )}
        </div>
      </div>
    </>
  );
}

const SelectedAssetPreview = ({
  data,
  asset,
}: {
  data: GenerateData;
  asset: InputAsset;
}) => {
  const assetType = getAssetType(asset);
  const assetKey = getAssetKey(asset);

  if (!data[assetKey]) return null;

  return (
    <>
      {assetType === "audio" && (
        <audio
          src={
            data[assetKey] && typeof data[assetKey] !== "string"
              ? URL.createObjectURL(data[assetKey])
              : data[assetKey] || ""
          }
          controls={true}
          className="w-full max-w-full"
        />
      )}
      {assetType === "video" && (
        <video
          src={
            data[assetKey] && typeof data[assetKey] !== "string"
              ? URL.createObjectURL(data[assetKey])
              : data[assetKey] || ""
          }
          controls={false}
          style={{ pointerEvents: "none" }}
          className="w-full max-h-[240px] object-contain"
        />
      )}
      {assetType === "image" && (
        <div className="w-full flex justify-center items-center p-2">
          <img
            id="image-preview"
            src={
              data[assetKey] && typeof data[assetKey] !== "string"
                ? URL.createObjectURL(data[assetKey])
                : data[assetKey] || ""
            }
            alt="Media Preview"
            className="max-h-[240px] object-contain rounded-md"
          />
        </div>
      )}
    </>
  );
};

================
File: src/components/storyboard-panel.tsx
================
import React, { useState, useEffect } from "react";
import { Button } from "@/components/ui/button";
import {
  Collapsible,
  CollapsibleContent,
  CollapsibleTrigger,
} from "@/components/ui/collapsible";
import { useVideoProjectStore } from "@/data/store";
import { enhancePrompt, LlmModelType } from "@/lib/prompt";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
  SelectGroup,
  SelectLabel,
} from "@/components/ui/select";
import { Textarea } from "@/components/ui/textarea";
import { Label } from "@/components/ui/label";
import { RotateCcw, Maximize2 } from "lucide-react";
import { AVAILABLE_ENDPOINTS } from "@/lib/fal";
import { Dialog, DialogContent, DialogClose } from "@/components/ui/dialog";
import { useMutation, useQueryClient } from "@tanstack/react-query";
import { db } from "@/data/db";
import { useProjectId } from "@/data/store";
import { queryKeys } from "@/data/queries";
import { useToast } from "@/hooks/use-toast";
import { DownloadButton } from "@/components/ui/download-button";

// Image style options
const IMAGE_STYLES = [
  { value: "fantasy", label: "Fantasy" },
  { value: "cyberpunk", label: "Cyberpunk" },
  { value: "gothic", label: "Gothic" },
  { value: "historical", label: "Historical" },
  { value: "surreal", label: "Surreal" },
  { value: "anime", label: "Anime" },
  { value: "scifi", label: "SciFi" },
  { value: "watercolor", label: "Watercolor" },
  { value: "custom", label: "Custom" },
];

// Add this new component for image model selection
function ImageModelPicker({
  value,
  onValueChange,
  ...props
}: {
  value: string;
  onValueChange: (value: string) => void;
} & Parameters<typeof Select>[0]) {
  // Filter only image generation endpoints from the actual configuration
  const imageEndpoints = AVAILABLE_ENDPOINTS.filter(
    (endpoint) => endpoint.category === "image",
  );

  return (
    <Select value={value} onValueChange={onValueChange} {...props}>
      <SelectTrigger className="w-full text-sm">
        <SelectValue placeholder="Select Image Model" />
      </SelectTrigger>
      <SelectContent>
        {/* Simply map all available image endpoints from the configuration */}
        {imageEndpoints.map((endpoint) => (
          <SelectItem key={endpoint.endpointId} value={endpoint.endpointId}>
            {endpoint.label}
          </SelectItem>
        ))}
      </SelectContent>
    </Select>
  );
}

interface StoryboardPanelProps {
  onGenerateImage: (
    prompt: string,
    modelId?: string,
    aspectRatio?: string,
  ) => Promise<string | undefined>;
  onSaveToMediaManager: (imageUrl: string) => void;
}

export function StoryboardPanel({
  onGenerateImage,
  onSaveToMediaManager,
}: StoryboardPanelProps) {
  const [isExpanded, setIsExpanded] = useState(true);
  const [slides, setSlides] = useState<
    Array<{
      chapterNumber: string;
      prompt: string;
      promptPreview?: string;
      imageUrl?: string;
    }>
  >([]);
  const [isVisible, setIsVisible] = useState(false);
  const [isLoading, setIsLoading] = useState<Record<number, boolean>>({});
  const [isGeneratingPrompts, setIsGeneratingPrompts] = useState(false);
  const [selectedLlmModel, setSelectedLlmModel] = useState<LlmModelType>(
    "anthropic/claude-3-haiku",
  );
  const [selectedImageStyle, setSelectedImageStyle] = useState("fantasy");
  const [customStyle, setCustomStyle] = useState("");
  const [selectedImageModel, setSelectedImageModel] = useState<string>(() => {
    // Get the default image model from available endpoints
    const imageEndpoints = AVAILABLE_ENDPOINTS.filter(
      (endpoint) => endpoint.category === "image",
    );
    // Use the first available image model, or fallback to a common default
    return imageEndpoints.length > 0
      ? imageEndpoints[0].endpointId
      : "fal-ai/fast-sdxl";
  });
  // Add state for expanded image
  const [expandedImage, setExpandedImage] = useState<string | null>(null);
  // Add state for storyboard metadata
  const [storyboardSource, setStoryboardSource] = useState<
    "chapter" | "custom"
  >("chapter");
  const [storyboardMetadata, setStoryboardMetadata] = useState<any>({});
  const [storyboardTitle, setStoryboardTitle] = useState("Storyboard Editor");
  // Add state for aspect ratio
  const [aspectRatio, setAspectRatio] = useState<"16:9" | "9:16">("16:9");

  const sessionData = useVideoProjectStore((state) => state.generateData);
  const projectId = useProjectId();
  const queryClient = useQueryClient();
  const { toast } = useToast();

  // Immediate check on mount for console debugging
  console.log("StoryboardPanel initial render", {
    hasLocalStorage: !!localStorage.getItem("storyboardData"),
    hasSession: !!sessionData,
  });

  useEffect(() => {
    console.log("StoryboardPanel mounted - checking for storyboard data");

    // Use a slight delay to ensure localStorage is available
    setTimeout(() => {
      checkForStoryboardData();
    }, 200);

    // Also listen for postMessage events as a fallback
    const handleMessage = (event: MessageEvent) => {
      console.log("Received message event:", event.data);
      if (event.data.type === "STORYBOARD_DATA") {
        const { slides, metadata } = event.data;
        console.log("Received storyboard data via postMessage:", slides);
        setSlides(slides);

        if (metadata) {
          processStoryboardMetadata(metadata);
        }

        setTimeout(() => {
          console.log("Setting isVisible to true from postMessage");
          setIsVisible(true);
        }, 500);
      }
    };

    window.addEventListener("message", handleMessage);
    return () => window.removeEventListener("message", handleMessage);
  }, []);

  // Function to process storyboard metadata
  const processStoryboardMetadata = (metadata: any) => {
    console.log("Processing storyboard metadata:", metadata);
    setStoryboardMetadata(metadata);

    // Set source
    if (metadata.source) {
      setStoryboardSource(metadata.source);
    }

    // Set image style from metadata if available
    if (metadata.style) {
      console.log("Setting image style from metadata:", metadata.style);
      setSelectedImageStyle(metadata.style);
    }

    // Set title based on source
    if (metadata.source === "chapter" && metadata.chapterNumber) {
      setStoryboardTitle(
        `Storyboard: Chapter ${metadata.chapterNumber}${metadata.chapterTitle ? ` - ${metadata.chapterTitle}` : ""}`,
      );
    } else if (metadata.source === "custom") {
      setStoryboardTitle("Custom Storyboard");
    }
  };

  // Function to check for storyboard data
  const checkForStoryboardData = () => {
    try {
      const storedData = localStorage.getItem("storyboardData");
      if (storedData) {
        console.log("Found storyboard data in localStorage:", storedData);
        const parsedData = JSON.parse(storedData);
        if (parsedData.slides && Array.isArray(parsedData.slides)) {
          console.log(
            "Valid slides array found, setting slides:",
            parsedData.slides.length,
          );
          setSlides(parsedData.slides);

          // Process metadata if available
          if (parsedData.metadata) {
            processStoryboardMetadata(parsedData.metadata);
          }

          // Use a staggered timing for better animation
          setTimeout(() => {
            console.log("Setting isVisible to true");
            setIsVisible(true);
          }, 300);

          // Clear localStorage to avoid loading it again on refresh
          // Use a delay to ensure it's processed first
          setTimeout(() => {
            localStorage.removeItem("storyboardData");
            console.log("Cleared storyboardData from localStorage");
          }, 1000);
        } else {
          console.log("Invalid slides data structure:", parsedData);
        }
      } else {
        console.log("No storyboard data found in localStorage");
      }
    } catch (error) {
      console.error("Error loading storyboard data from localStorage:", error);
    }
  };

  // Function to generate AI prompts for all slides
  const generateAIPrompts = async () => {
    if (slides.length === 0) return;

    try {
      setIsGeneratingPrompts(true);

      // Generate new prompts for each slide using the selected LLM and style
      const updatedSlides = [...slides];

      // Create a comprehensive context for the entire storyboard
      const styleText =
        selectedImageStyle === "custom"
          ? customStyle
          : IMAGE_STYLES.find((style) => style.value === selectedImageStyle)
              ?.label || "Fantasy";

      // Get the full story or chapter content
      const fullStory = storyboardMetadata?.fullStory || "";
      const location = storyboardMetadata?.location || "";
      const timeline = storyboardMetadata?.timeline || "";

      // Different prompt formats based on source
      let promptContext = "";

      if (storyboardSource === "chapter") {
        // Format for chapter-based storyboards
        promptContext = `
          You are an expert storyboard generator for AI-based image creation. Your task is to enhance the provided prompts to make them more detailed and visually compelling. Follow these instructions internally, and output only the final image prompts.

          1. Analyze the current slide prompt:
             "${slides.map((slide) => slide.prompt).join("\n")}"
          
          2. Since this is from a chapter, integrate location: "${location}" and timeline: "${timeline}" from memory to maintain continuity and setting accuracy.

          3. For each prompt, create a comprehensive image generation prompt. Each prompt must be a complete sentence that vividly describes the scene with details such as setting, characters, actions, and cinematic elements. Ensure you integrate the selected art style: "${styleText}".

          4. Output only the enhanced prompts. Do not include numbering, explanations, or any other text.
        `;
      } else {
        // Format for custom story input
        promptContext = `
          You are an expert storyboard generator for AI-based image creation. Your task is to enhance the provided prompts to make them more detailed and visually compelling. Follow these instructions internally, and output only the final image prompts.

          1. Analyze the following narrative and current prompts:
             Full Story: "${fullStory}"
             Current Prompts:
             "${slides.map((slide) => slide.prompt).join("\n")}"
          
          2. Since this is a "Start from Scratch" mode, do NOT use location or timeline unless explicitly mentioned in the provided text. Instead, infer the necessary scene details from the given input.

          3. For each prompt, create a comprehensive image generation prompt. Each prompt must be a complete sentence that vividly describes the scene with details such as setting, characters, actions, and cinematic elements. Ensure you integrate the selected art style: "${styleText}".

          4. Output only the enhanced prompts. Do not include numbering, explanations, or any other text.
        `;
      }

      // Process each slide individually to enhance its prompt
      for (let i = 0; i < updatedSlides.length; i++) {
        const slide = updatedSlides[i];

        // Create a slide-specific prompt for the LLM
        const slidePromptContext = `
          ${promptContext}
          
          Current slide to enhance:
          "${slide.prompt}"
          
          Output only the enhanced prompt for this specific slide. Make it rich in visual details, cinematic quality, and incorporate the ${styleText} style. Do not include any explanations or numbering.
        `;

        try {
          // Use the enhancePrompt function with the selected LLM model
          const enhancedPrompt = await enhancePrompt(slidePromptContext, {
            type: "image",
            model: selectedLlmModel,
          });

          // Generate a preview version for display
          const promptPreview =
            enhancedPrompt.length > 200
              ? enhancedPrompt.substring(0, 200) + "..."
              : enhancedPrompt;

          // Update the slide with the new AI-generated prompt
          updatedSlides[i] = {
            ...slide,
            prompt: enhancedPrompt,
            promptPreview,
          };
        } catch (error) {
          console.error(`Failed to generate prompt for slide ${i}:`, error);
        }
      }

      // Update all slides with their new prompts
      setSlides(updatedSlides);
    } catch (error) {
      console.error("Failed to generate AI prompts:", error);
    } finally {
      setIsGeneratingPrompts(false);
    }
  };

  // Function to get the StoryboardPanel title based on source
  const getStoryboardTitle = () => {
    if (storyboardSource === "chapter") {
      return `Storyboard: Chapter ${storyboardMetadata.chapterNumber || ""}`;
    } else {
      return "Custom Storyboard";
    }
  };

  const handleGenerateImage = async (index: number) => {
    try {
      // Set loading state for this slide
      setIsLoading((prev) => ({ ...prev, [index]: true }));

      const slide = slides[index];
      console.log(
        "Generating image with model:",
        selectedImageModel,
        "aspect ratio:",
        aspectRatio,
      );

      // Pass the prompt, model ID, and now the aspect ratio
      const imageUrl = await onGenerateImage(
        slide.prompt,
        selectedImageModel,
        aspectRatio,
      );

      if (imageUrl) {
        console.log("Image generated successfully:", imageUrl);
        setSlides(slides.map((s, i) => (i === index ? { ...s, imageUrl } : s)));
      }
    } catch (error) {
      console.error(`Failed to generate image for slide ${index}:`, error);
    } finally {
      // Clear loading state
      setIsLoading((prev) => ({ ...prev, [index]: false }));
    }
  };

  // Function to regenerate a single prompt
  const handleRegeneratePrompt = async (index: number) => {
    try {
      setIsLoading((prev) => ({ ...prev, [index]: true }));

      const slide = slides[index];

      // Get the style text to embed
      const styleText =
        selectedImageStyle === "custom"
          ? customStyle
          : IMAGE_STYLES.find((style) => style.value === selectedImageStyle)
              ?.label || "Fantasy";

      // Get relevant metadata
      const fullStory = storyboardMetadata?.fullStory || "";
      const location = storyboardMetadata?.location || "";
      const timeline = storyboardMetadata?.timeline || "";

      // Create prompt context based on source
      let promptContext = "";

      if (storyboardSource === "chapter") {
        promptContext = `
          You are an expert storyboard generator for AI-based image creation. Your task is to create a detailed image prompt based on a chapter segment.

          1. Analyze the following chapter content:
             "Chapter ${slide.chapterNumber}, with location "${location}" and timeline "${timeline}""
             
          2. The current prompt is: "${slide.prompt}"
          
          3. Create a comprehensive, visually rich image generation prompt that captures this scene. Include details about setting, characters, actions, mood, lighting, and cinematic elements. The image should be in ${styleText} style.
          
          4. Output ONLY the enhanced prompt, with no additional text, explanations, or numbering.
        `;
      } else {
        // For custom story input
        promptContext = `
          You are an expert storyboard generator for AI-based image creation. Your task is to create a detailed image prompt based on a story segment.

          1. Analyze the following story segment:
             "${slide.prompt}"
             
          2. If helpful, here's more context from the full story: "${fullStory.substring(0, 300)}..."
          
          3. Create a comprehensive, visually rich image generation prompt that captures this scene. Include details about setting, characters, actions, mood, lighting, and cinematic elements. The image should be in ${styleText} style.
          
          4. Output ONLY the enhanced prompt, with no additional text, explanations, or numbering.
        `;
      }

      try {
        // Use the enhancePrompt function with the selected LLM model
        const enhancedPrompt = await enhancePrompt(promptContext, {
          type: "image",
          model: selectedLlmModel,
        });

        // Generate a preview version for display
        const promptPreview =
          enhancedPrompt.length > 200
            ? enhancedPrompt.substring(0, 200) + "..."
            : enhancedPrompt;

        // Update just this slide with the new prompt
        setSlides(
          slides.map((s, i) =>
            i === index ? { ...s, prompt: enhancedPrompt, promptPreview } : s,
          ),
        );
      } catch (error) {
        console.error(`Failed to regenerate prompt for slide ${index}:`, error);
      }
    } catch (error) {
      console.error(`Failed to regenerate prompt for slide ${index}:`, error);
    } finally {
      setIsLoading((prev) => ({ ...prev, [index]: false }));
    }
  };

  // Update the handlePromptEdit function to handle full prompt text
  const handlePromptEdit = (index: number, newPrompt: string) => {
    setSlides(
      slides.map((s, i) => {
        if (i === index) {
          return {
            ...s,
            prompt: newPrompt,
            // If we had a preview, update it with a truncated version of the new prompt
            promptPreview:
              newPrompt.length > 200
                ? newPrompt.substring(0, 200) + "..."
                : newPrompt,
          };
        }
        return s;
      }),
    );
  };

  // Add this mutation for saving media
  const saveMediaMutation = useMutation({
    mutationFn: async (imageUrl: string) => {
      // Find the slide with this image
      const slide = slides.find((s) => s.imageUrl === imageUrl);

      // Create properly structured media data
      const mediaData = {
        projectId,
        kind: "generated" as "generated" | "uploaded",
        mediaType: "image" as "image" | "video" | "music" | "voiceover",
        status: "completed" as "pending" | "running" | "completed" | "failed",
        createdAt: Date.now(),
        endpointId: selectedImageModel,
        input: {
          prompt: slide?.prompt || "",
        },
        // Add the output structure with images array - this is critical
        output: {
          images: [
            {
              url: imageUrl,
              // Include any other metadata available
              width: 1024,
              height: 1024,
            },
          ],
        },
      };

      try {
        const mediaId = await db.media.create(mediaData);
        console.log("Created media with ID:", mediaId);
        return mediaId;
      } catch (error) {
        console.error("Failed to save to media gallery:", error);
        throw error;
      }
    },
    onSuccess: () => {
      queryClient.invalidateQueries({
        queryKey: queryKeys.projectMediaItems(projectId),
      });
      toast({
        title: "Image saved to media gallery",
        description: "The image has been added to your media library",
      });
    },
    onError: (error) => {
      console.error("Error saving media:", error);
      toast({
        title: "Failed to save image",
        description:
          "There was an error adding the image to your media library",
      });
    },
  });

  // Update the onSaveToMediaManager usage
  const handleSaveToMediaManager = (imageUrl: string) => {
    saveMediaMutation.mutate(imageUrl);
  };

  // Only show the panel if we have slides
  if (slides.length === 0) {
    return null;
  }

  return (
    <div
      className={`w-full transition-all duration-500 ease-in-out ${
        isVisible ? "opacity-100 translate-y-0" : "opacity-0 -translate-y-full"
      }`}
    >
      <Collapsible
        open={isExpanded}
        onOpenChange={setIsExpanded}
        className="w-full bg-background border-b"
      >
        <CollapsibleTrigger className="flex items-center justify-between w-full p-4 hover:bg-accent">
          <div className="flex items-center">
            <svg
              xmlns="http://www.w3.org/2000/svg"
              className="h-5 w-5 mr-2"
              viewBox="0 0 20 20"
              fill="currentColor"
            >
              <path d="M5 3a2 2 0 00-2 2v2a2 2 0 002 2h2a2 2 0 002-2V5a2 2 0 00-2-2H5zM5 11a2 2 0 00-2 2v2a2 2 0 002 2h2a2 2 0 002-2v-2a2 2 0 00-2-2H5zM11 5a2 2 0 012-2h2a2 2 0 012 2v2a2 2 0 01-2 2h-2a2 2 0 01-2-2V5zM11 13a2 2 0 012-2h2a2 2 0 012 2v2a2 2 0 01-2 2h-2a2 2 0 01-2-2v-2z" />
            </svg>
            <h2 className="text-lg font-semibold">{storyboardTitle}</h2>
            <span className="ml-2 bg-blue-600/70 text-xs px-2 py-0.5 rounded-full">
              {slides.length} Slides
            </span>
          </div>
          <span>{isExpanded ? "▼" : "▲"}</span>
        </CollapsibleTrigger>

        <CollapsibleContent className="p-4">
          {/* AI Prompt Generation Controls */}
          <div className="mb-6 p-5 border rounded-lg bg-card/50 shadow-sm">
            <h3 className="text-md font-medium mb-4 flex items-center">
              <svg
                xmlns="http://www.w3.org/2000/svg"
                width="18"
                height="18"
                viewBox="0 0 24 24"
                fill="none"
                stroke="currentColor"
                strokeWidth="2"
                strokeLinecap="round"
                strokeLinejoin="round"
                className="mr-2"
              >
                <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z" />
                <path d="M9 11h6" />
                <path d="M12 8v6" />
              </svg>
              AI Prompt Generation
            </h3>

            <div className="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
              {/* LLM Model Selector */}
              <div>
                <Label
                  htmlFor="llm-model"
                  className="text-sm font-medium mb-1.5 block"
                >
                  LLM Model
                </Label>
                <Select
                  value={selectedLlmModel}
                  onValueChange={(value) =>
                    setSelectedLlmModel(value as LlmModelType)
                  }
                >
                  <SelectTrigger className="w-full bg-background/80">
                    <SelectValue placeholder="Select LLM Model" />
                  </SelectTrigger>
                  <SelectContent>
                    <SelectGroup>
                      <SelectLabel>Meta Llama Models</SelectLabel>
                      <SelectItem value="meta-llama/llama-3.1-70b-instruct">
                        Llama 3.1 70B
                      </SelectItem>
                    </SelectGroup>
                    <SelectGroup>
                      <SelectLabel>OpenAI Models</SelectLabel>
                      <SelectItem value="openai/gpt-4o-mini">
                        GPT-4o Mini
                      </SelectItem>
                      <SelectItem value="openai/gpt-4o">GPT-4o</SelectItem>
                    </SelectGroup>
                    <SelectGroup>
                      <SelectLabel>Anthropic Models</SelectLabel>
                      <SelectItem value="anthropic/claude-3.5-sonnet">
                        Claude 3.5 Sonnet
                      </SelectItem>
                      <SelectItem value="anthropic/claude-3-5-haiku">
                        Claude 3.5 Haiku
                      </SelectItem>
                      <SelectItem value="anthropic/claude-3-haiku">
                        Claude 3 Haiku
                      </SelectItem>
                    </SelectGroup>
                    <SelectGroup>
                      <SelectLabel>Google Models</SelectLabel>
                      <SelectItem value="google/gemini-pro-1.5">
                        Gemini Pro 1.5
                      </SelectItem>
                      <SelectItem value="google/gemini-flash-1.5">
                        Gemini Flash 1.5
                      </SelectItem>
                    </SelectGroup>
                  </SelectContent>
                </Select>
              </div>

              {/* Image Style Selector */}
              <div>
                <Label
                  htmlFor="image-style"
                  className="text-sm font-medium mb-1.5 block"
                >
                  Image Style
                </Label>
                <Select
                  value={selectedImageStyle}
                  onValueChange={setSelectedImageStyle}
                >
                  <SelectTrigger className="w-full bg-background/80">
                    <SelectValue placeholder="Select Image Style" />
                  </SelectTrigger>
                  <SelectContent>
                    {IMAGE_STYLES.map((style) => (
                      <SelectItem key={style.value} value={style.value}>
                        {style.label}
                      </SelectItem>
                    ))}
                  </SelectContent>
                </Select>
              </div>
            </div>

            {/* Custom Style Input (shown only when "Custom" is selected) */}
            {selectedImageStyle === "custom" && (
              <div className="mb-4">
                <Label
                  htmlFor="custom-style"
                  className="text-sm font-medium mb-1.5 block"
                >
                  Custom Style Description
                </Label>
                <Textarea
                  id="custom-style"
                  placeholder="Describe your custom style (e.g., 'Dark noir with neon accents')"
                  value={customStyle}
                  onChange={(e) => setCustomStyle(e.target.value)}
                  className="resize-none h-20 bg-background/80"
                />
              </div>
            )}

            {/* Generate Button */}
            <Button
              onClick={generateAIPrompts}
              disabled={isGeneratingPrompts}
              className="w-full bg-blue-600 hover:bg-blue-700 text-white font-medium"
            >
              {isGeneratingPrompts ? (
                <span className="flex items-center justify-center">
                  <svg
                    className="animate-spin -ml-1 mr-2 h-4 w-4 text-white"
                    xmlns="http://www.w3.org/2000/svg"
                    fill="none"
                    viewBox="0 0 24 24"
                  >
                    <circle
                      className="opacity-25"
                      cx="12"
                      cy="12"
                      r="10"
                      stroke="currentColor"
                      strokeWidth="4"
                    ></circle>
                    <path
                      className="opacity-75"
                      fill="currentColor"
                      d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"
                    ></path>
                  </svg>
                  Generating AI Prompts...
                </span>
              ) : (
                <span className="flex items-center justify-center">
                  <svg
                    xmlns="http://www.w3.org/2000/svg"
                    width="18"
                    height="18"
                    viewBox="0 0 24 24"
                    fill="none"
                    stroke="currentColor"
                    strokeWidth="2"
                    strokeLinecap="round"
                    strokeLinejoin="round"
                    className="mr-2"
                  >
                    <path d="M12 22c5.523 0 10-4.477 10-10S17.523 2 12 2 2 6.477 2 12s4.477 10 10 10z" />
                    <path d="m9 12 2 2 4-4" />
                  </svg>
                  Generate AI Prompts for All Slides
                </span>
              )}
            </Button>
          </div>

          <div className="flex gap-4 overflow-x-auto pb-6">
            {slides.map((slide, index) => (
              <div
                key={index}
                className="min-w-[300px] p-5 space-y-4 bg-card rounded-lg border border-gray-700/20 shadow-sm hover:shadow-md transition-shadow duration-200"
              >
                <h3 className="font-medium flex items-center justify-between">
                  <span className="flex items-center">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      width="16"
                      height="16"
                      viewBox="0 0 24 24"
                      fill="none"
                      stroke="currentColor"
                      strokeWidth="2"
                      strokeLinecap="round"
                      strokeLinejoin="round"
                      className="mr-1.5"
                    >
                      <path d="M4 19.5v-15A2.5 2.5 0 0 1 6.5 2H20v20H6.5a2.5 2.5 0 0 1-2.5-2.5Z" />
                      <path d="M10 2v20" />
                    </svg>
                    {storyboardSource === "chapter"
                      ? `Chapter ${slide.chapterNumber}`
                      : "Scene"}
                  </span>
                  <span className="bg-blue-600/20 text-blue-500 text-xs px-2 py-0.5 rounded-full font-medium">
                    Slide {index + 1}
                  </span>
                </h3>

                {/* Editable Prompt Text Area */}
                <div>
                  <Label
                    htmlFor={`prompt-${index}`}
                    className="text-sm font-medium mb-1.5 block flex items-center"
                  >
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      width="14"
                      height="14"
                      viewBox="0 0 24 24"
                      fill="none"
                      stroke="currentColor"
                      strokeWidth="2"
                      strokeLinecap="round"
                      strokeLinejoin="round"
                      className="mr-1.5"
                    >
                      <polyline points="9 17 4 12 9 7" />
                      <path d="M20 18v-2a4 4 0 0 0-4-4H4" />
                    </svg>
                    Prompt
                  </Label>
                  <Textarea
                    id={`prompt-${index}`}
                    className="h-28 mb-3 bg-background/50 text-sm shadow-inner border-gray-700/30"
                    placeholder="Edit this prompt to customize the image generation"
                    value={slide.prompt}
                    onChange={(e) => handlePromptEdit(index, e.target.value)}
                  />
                </div>

                <div className="mb-3">
                  <Label
                    htmlFor={`image-model-${index}`}
                    className="text-sm font-medium mb-1.5 block"
                  >
                    Image Model
                  </Label>
                  <ImageModelPicker
                    value={selectedImageModel}
                    onValueChange={setSelectedImageModel}
                  />
                </div>

                {/* Add Aspect Ratio selector */}
                <div className="mb-3">
                  <Label
                    htmlFor={`aspect-ratio-${index}`}
                    className="text-sm font-medium mb-1.5 block"
                  >
                    Aspect Ratio
                  </Label>
                  <div className="flex flex-row gap-2">
                    <Button
                      id={`aspect-ratio-16-9-${index}`}
                      variant={aspectRatio === "16:9" ? "secondary" : "outline"}
                      size="sm"
                      className="flex-1"
                      onClick={() => setAspectRatio("16:9")}
                    >
                      16:9
                    </Button>
                    <Button
                      id={`aspect-ratio-9-16-${index}`}
                      variant={aspectRatio === "9:16" ? "secondary" : "outline"}
                      size="sm"
                      className="flex-1"
                      onClick={() => setAspectRatio("9:16")}
                    >
                      9:16
                    </Button>
                  </div>
                </div>

                <Button
                  variant="outline"
                  size="sm"
                  className="w-full mb-3 group"
                  onClick={() => handleRegeneratePrompt(index)}
                  disabled={isGeneratingPrompts}
                >
                  <span className="flex items-center">
                    <RotateCcw className="mr-1.5 w-3.5 h-3.5 group-hover:rotate-180 transition-transform duration-300" />
                    Regenerate Prompt
                  </span>
                </Button>

                {slide.imageUrl ? (
                  <div className="relative">
                    <img
                      src={slide.imageUrl}
                      alt={`Chapter ${slide.chapterNumber} visualization`}
                      className={`w-full ${aspectRatio === "16:9" ? "h-40" : "h-48"} object-cover rounded-md shadow-sm cursor-pointer hover:opacity-95 transition-opacity`}
                      onClick={() => {
                        if (slide.imageUrl) {
                          setExpandedImage(slide.imageUrl);
                        }
                      }}
                    />
                    <div className="absolute top-2 right-2 flex gap-1">
                      <Button
                        variant="outline"
                        size="icon"
                        className="w-7 h-7 rounded-full bg-background/70 backdrop-blur-sm hover:bg-background/90"
                        onClick={(e) => {
                          e.stopPropagation();
                          if (slide.imageUrl) {
                            setExpandedImage(slide.imageUrl);
                          }
                        }}
                      >
                        <Maximize2 className="h-3.5 w-3.5" />
                      </Button>
                    </div>
                  </div>
                ) : (
                  <div
                    className={`w-full ${aspectRatio === "16:9" ? "h-40" : "h-48"} bg-gradient-to-br from-gray-800/10 to-gray-800/25 rounded-md flex flex-col items-center justify-center border border-gray-700/20 shadow-inner`}
                  >
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      width="24"
                      height="24"
                      viewBox="0 0 24 24"
                      fill="none"
                      stroke="currentColor"
                      strokeWidth="1.5"
                      strokeLinecap="round"
                      strokeLinejoin="round"
                      className="text-gray-400 mb-2"
                    >
                      <rect width="18" height="18" x="3" y="3" rx="2" ry="2" />
                      <circle cx="9" cy="9" r="2" />
                      <path d="m21 15-3.086-3.086a2 2 0 0 0-2.828 0L6 21" />
                    </svg>
                    <span className="text-gray-500 text-sm font-medium">
                      Image will appear here
                    </span>
                    <span className="text-gray-400 text-xs mt-1">
                      {aspectRatio === "16:9" ? "Landscape" : "Portrait"} format
                    </span>
                  </div>
                )}

                <div className="space-y-2">
                  <Button
                    variant="default"
                    size="sm"
                    className="w-full bg-blue-600 hover:bg-blue-700 text-white flex items-center justify-center"
                    onClick={() => handleGenerateImage(index)}
                    disabled={isLoading[index]}
                  >
                    {isLoading[index] ? (
                      <span className="flex items-center">
                        <svg
                          className="animate-spin -ml-1 mr-2 h-4 w-4 text-white"
                          xmlns="http://www.w3.org/2000/svg"
                          fill="none"
                          viewBox="0 0 24 24"
                        >
                          <circle
                            className="opacity-25"
                            cx="12"
                            cy="12"
                            r="10"
                            stroke="currentColor"
                            strokeWidth="4"
                          ></circle>
                          <path
                            className="opacity-75"
                            fill="currentColor"
                            d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"
                          ></path>
                        </svg>
                        Generating...
                      </span>
                    ) : (
                      <span className="flex items-center">
                        <svg
                          xmlns="http://www.w3.org/2000/svg"
                          width="14"
                          height="14"
                          viewBox="0 0 24 24"
                          fill="none"
                          stroke="currentColor"
                          strokeWidth="2"
                          strokeLinecap="round"
                          strokeLinejoin="round"
                          className="mr-1.5"
                        >
                          <path d="M21 8v-.65A2.15 2.15 0 0 0 19 5.2M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
                          <path d="M12 7v9" />
                          <path d="m15 13-3 3-3-3" />
                        </svg>
                        Generate Image
                      </span>
                    )}
                  </Button>

                  {slide.imageUrl && (
                    <Button
                      variant="secondary"
                      size="sm"
                      className="w-full flex items-center justify-center mb-2"
                      onClick={() => handleSaveToMediaManager(slide.imageUrl!)}
                      disabled={saveMediaMutation.isPending}
                    >
                      {saveMediaMutation.isPending ? (
                        <span className="flex items-center">
                          <svg
                            className="animate-spin -ml-1 mr-2 h-4 w-4 text-white"
                            xmlns="http://www.w3.org/2000/svg"
                            fill="none"
                            viewBox="0 0 24 24"
                          >
                            <circle
                              className="opacity-25"
                              cx="12"
                              cy="12"
                              r="10"
                              stroke="currentColor"
                              strokeWidth="4"
                            ></circle>
                            <path
                              className="opacity-75"
                              fill="currentColor"
                              d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"
                            ></path>
                          </svg>
                          Saving...
                        </span>
                      ) : (
                        <span className="flex items-center">
                          <svg
                            xmlns="http://www.w3.org/2000/svg"
                            width="14"
                            height="14"
                            viewBox="0 0 24 24"
                            fill="none"
                            stroke="currentColor"
                            strokeWidth="2"
                            strokeLinecap="round"
                            strokeLinejoin="round"
                            className="mr-1.5"
                          >
                            <path d="M5 12h14" />
                            <path d="M12 5v14" />
                          </svg>
                          Save to Media Manager
                        </span>
                      )}
                    </Button>
                  )}

                  {slide.imageUrl && (
                    <DownloadButton
                      imageUrl={slide.imageUrl}
                      filename={`storyboard-image-${index + 1}.png`}
                    />
                  )}
                </div>
              </div>
            ))}
          </div>
        </CollapsibleContent>
      </Collapsible>

      {/* Image Expansion Dialog */}
      <Dialog
        open={!!expandedImage}
        onOpenChange={(open) => !open && setExpandedImage(null)}
      >
        <DialogContent className="max-w-4xl">
          {expandedImage && (
            <img
              src={expandedImage}
              alt="Expanded view"
              className="w-full object-contain"
            />
          )}
          <DialogClose asChild>
            <Button className="mt-4">Close</Button>
          </DialogClose>
        </DialogContent>
      </Dialog>
    </div>
  );
}

================
File: src/components/theme-provider.tsx
================
"use client";

import * as React from "react";
import { ThemeProvider as NextThemesProvider } from "next-themes";
import type { ThemeProviderProps } from "next-themes";

export function ThemeProvider({ children, ...props }: ThemeProviderProps) {
  return <NextThemesProvider {...props}>{children}</NextThemesProvider>;
}

================
File: src/components/theme-toggle.tsx
================
"use client";

import * as React from "react";
import { Moon, Sun } from "lucide-react";
import { useTheme } from "next-themes";

import { Button } from "@/components/ui/button";

export function ThemeToggle() {
  const { theme, setTheme } = useTheme();

  return (
    <Button
      variant="ghost"
      size="icon"
      onClick={() => setTheme(theme === "light" ? "dark" : "light")}
      className="transition-all duration-300 hover:bg-primary/20 glassmorphism"
      title={theme === "light" ? "Switch to dark mode" : "Switch to light mode"}
      aria-label="Toggle theme"
    >
      <Sun className="h-5 w-5 rotate-0 scale-100 transition-all dark:-rotate-90 dark:scale-0" />
      <Moon className="absolute h-5 w-5 rotate-90 scale-0 transition-all dark:rotate-0 dark:scale-100" />
      <span className="sr-only">Toggle theme</span>
    </Button>
  );
}

================
File: src/components/video-controls.tsx
================
import { useVideoProjectStore } from "@/data/store";
import { useHotkeys } from "react-hotkeys-hook";

import {
  ChevronFirstIcon,
  ChevronLastIcon,
  ChevronLeftIcon,
  ChevronRightIcon,
  PauseIcon,
  PlayIcon,
} from "lucide-react";
import { Button } from "./ui/button";

const FPS = 30;

export function VideoControls() {
  const player = useVideoProjectStore((s) => s.player);
  const playerState = useVideoProjectStore((s) => s.playerState);
  const handleTogglePlay = () => {
    if (!player) return;
    if (player.isPlaying()) {
      player.pause();
    } else {
      player.play();
    }
  };
  const onSeekToStart = () => {
    if (!player) return;
    player.seekTo(0);
  };
  const onSeekToEnd = () => {
    if (!player) return;
    // player.seekTo(player.getDuration());
  };
  const onSeekBackward = () => {
    if (!player) return;
    player.pause();
    player.seekTo(player.getCurrentFrame() - FPS);
  };
  const onSeekForward = () => {
    if (!player) return;
    player.seekTo(player.getCurrentFrame() + FPS);
  };

  useHotkeys("space", handleTogglePlay, [player]);
  useHotkeys("left", onSeekBackward, [player]);
  useHotkeys("right", onSeekForward, [player]);
  useHotkeys("home", onSeekToStart, [player]);
  useHotkeys("end", onSeekToEnd, [player]);

  return (
    <div className="flex flex-row justify-center items-center">
      <Button variant="ghost" size="icon" onClick={onSeekToStart}>
        <ChevronFirstIcon />
      </Button>
      <Button variant="ghost" size="icon" onClick={onSeekBackward}>
        <ChevronLeftIcon />
      </Button>
      <Button variant="ghost" size="icon" onClick={handleTogglePlay}>
        {playerState === "paused" && <PlayIcon className="fill-current" />}
        {playerState === "playing" && <PauseIcon className="fill-current" />}
      </Button>
      <Button variant="ghost" size="icon" onClick={onSeekForward}>
        <ChevronRightIcon />
      </Button>
      <Button variant="ghost" size="icon" onClick={onSeekToEnd}>
        <ChevronLastIcon />
      </Button>
    </div>
  );
}

================
File: src/components/video-preview.tsx
================
import { db } from "@/data/db";
import {
  EMPTY_VIDEO_COMPOSITION,
  useProject,
  useVideoComposition,
} from "@/data/queries";
import {
  type MediaItem,
  PROJECT_PLACEHOLDER,
  TRACK_TYPE_ORDER,
  type VideoKeyFrame,
  type VideoProject,
  type VideoTrack,
} from "@/data/schema";
import { useProjectId, useVideoProjectStore } from "@/data/store";
import { cn, resolveDuration, resolveMediaUrl } from "@/lib/utils";
import { Player, type PlayerRef } from "@remotion/player";
import { preloadVideo, preloadAudio } from "@remotion/preload";
import { useCallback, useEffect } from "react";
import {
  AbsoluteFill,
  Audio,
  Composition,
  Img,
  Sequence,
  Video,
} from "remotion";
import { throttle } from "throttle-debounce";
import { Button } from "./ui/button";
import { DownloadIcon } from "lucide-react";

interface VideoCompositionProps {
  project: VideoProject;
  tracks: VideoTrack[];
  frames: Record<string, VideoKeyFrame[]>;
  mediaItems: Record<string, MediaItem>;
}

const FPS = 30;
const DEFAULT_DURATION = 5;
const VIDEO_WIDTH = 1024;
const VIDEO_HEIGHT = 720;

const videoSizeMap = {
  "16:9": { width: 1024, height: 576 },
  "9:16": { width: 576, height: 1024 },
  "1:1": { width: 1024, height: 1024 },
};

export const VideoComposition: React.FC<VideoCompositionProps> = ({
  project,
  tracks,
  frames,
  mediaItems,
}) => {
  const sortedTracks = [...tracks].sort((a, b) => {
    return TRACK_TYPE_ORDER[a.type] - TRACK_TYPE_ORDER[b.type];
  });

  let width = VIDEO_WIDTH;
  let height = VIDEO_HEIGHT;

  if (project.aspectRatio) {
    const size = videoSizeMap[project.aspectRatio];
    if (size) {
      width = size.width;
      height = size.height;
    }
  }

  return (
    <Composition
      id={project.id}
      component={MainComposition as any}
      durationInFrames={DEFAULT_DURATION * FPS}
      fps={FPS}
      width={width}
      height={height}
      defaultProps={{
        project,
        tracks: sortedTracks,
        frames,
        mediaItems,
      }}
    />
  );
};

const MainComposition: React.FC<VideoCompositionProps> = ({
  tracks,
  frames,
  mediaItems,
}) => {
  return (
    <AbsoluteFill>
      {tracks.map((track) => (
        <Sequence key={track.id}>
          {track.type === "video" && (
            <VideoTrackSequence
              track={track}
              frames={frames[track.id] || []}
              mediaItems={mediaItems}
            />
          )}
          {(track.type === "music" || track.type === "voiceover") && (
            <AudioTrackSequence
              track={track}
              frames={frames[track.id] || []}
              mediaItems={mediaItems}
            />
          )}
        </Sequence>
      ))}
    </AbsoluteFill>
  );
};

interface TrackSequenceProps {
  track: VideoTrack;
  frames: VideoKeyFrame[];
  mediaItems: Record<string, MediaItem>;
}

const VideoTrackSequence: React.FC<TrackSequenceProps> = ({
  frames,
  mediaItems,
}) => {
  return (
    <AbsoluteFill>
      {frames.map((frame) => {
        const media = mediaItems[frame.data.mediaId];
        if (!media || media.status !== "completed") return null;

        const mediaUrl = resolveMediaUrl(media);
        if (!mediaUrl) return null;

        const duration = frame.duration || resolveDuration(media) || 5000;
        const durationInFrames = Math.floor(duration / (1000 / FPS));

        return (
          <Sequence
            key={frame.id}
            from={Math.floor(frame.timestamp / (1000 / FPS))}
            durationInFrames={durationInFrames}
            premountFor={3000}
          >
            {media.mediaType === "video" && <Video src={mediaUrl} />}
            {media.mediaType === "image" && (
              <Img src={mediaUrl} style={{ objectFit: "cover" }} />
            )}
          </Sequence>
        );
      })}
    </AbsoluteFill>
  );
};

const AudioTrackSequence: React.FC<TrackSequenceProps> = ({
  frames,
  mediaItems,
}) => {
  return (
    <>
      {frames.map((frame) => {
        const media = mediaItems[frame.data.mediaId];
        if (!media || media.status !== "completed") return null;

        const audioUrl = resolveMediaUrl(media);
        if (!audioUrl) return null;

        const duration = frame.duration || resolveDuration(media) || 5000;
        const durationInFrames = Math.floor(duration / (1000 / FPS));

        return (
          <Sequence
            key={frame.id}
            from={Math.floor(frame.timestamp / (1000 / FPS))}
            durationInFrames={durationInFrames}
            premountFor={3000}
          >
            <Audio src={audioUrl} />
          </Sequence>
        );
      })}
    </>
  );
};

export default function VideoPreview() {
  const projectId = useProjectId();
  const setPlayer = useVideoProjectStore((s) => s.setPlayer);

  const { data: project = PROJECT_PLACEHOLDER } = useProject(projectId);
  const {
    data: composition = EMPTY_VIDEO_COMPOSITION,
    isLoading: isCompositionLoading,
  } = useVideoComposition(projectId);
  const { tracks = [], frames = {}, mediaItems = {} } = composition;

  useEffect(() => {
    const mediaIds = Object.values(frames)
      .flat()
      .flatMap((f) => f.data.mediaId);
    for (const media of Object.values(mediaItems)) {
      if (media.status === "completed" && mediaIds.includes(media.id)) {
        const mediaUrl = resolveMediaUrl(media);
        if (!mediaUrl) continue;
        if (media.mediaType === "video") {
          preloadVideo(mediaUrl);
        }
        if (
          mediaUrl.indexOf("v2.") === -1 &&
          (media.mediaType === "music" || media.mediaType === "voiceover")
        ) {
          preloadAudio(mediaUrl);
        }
      }
    }
  }, [frames, mediaItems]);

  // Calculate the effective duration based on the latest keyframe
  const calculateDuration = useCallback(() => {
    let maxTimestamp = 0;
    for (const trackFrames of Object.values(frames)) {
      for (const frame of trackFrames) {
        maxTimestamp = Math.max(maxTimestamp, frame.timestamp);
      }
    }
    // Add 5 seconds padding after the last frame
    return Math.max(DEFAULT_DURATION, Math.ceil((maxTimestamp + 5000) / 1000));
  }, [frames]);

  const duration = calculateDuration();

  const setPlayerCurrentTimestamp = useVideoProjectStore(
    (s) => s.setPlayerCurrentTimestamp,
  );

  const setPlayerState = useVideoProjectStore((s) => s.setPlayerState);
  // Frame updates are super frequent, so we throttle the updates to the timestamp
  const updatePlayerCurrentTimestamp = useCallback(
    throttle(64, setPlayerCurrentTimestamp),
    [],
  );

  // Register events on the player
  const playerRef = useCallback(
    (player: PlayerRef) => {
      if (!player) return;
      setPlayer(player);
      player.addEventListener("play", (e) => {
        setPlayerState("playing");
      });
      player.addEventListener("pause", (e) => {
        setPlayerState("paused");
      });
      player.addEventListener("seeked", (e) => {
        const currentFrame = e.detail.frame;
        updatePlayerCurrentTimestamp(currentFrame / FPS);
      });
      player.addEventListener("frameupdate", (e) => {
        const currentFrame = e.detail.frame;
        updatePlayerCurrentTimestamp(currentFrame / FPS);
      });
    },
    [setPlayer, setPlayerState, updatePlayerCurrentTimestamp],
  );

  const setExportDialogOpen = useVideoProjectStore(
    (s) => s.setExportDialogOpen,
  );

  let width = VIDEO_WIDTH;
  let height = VIDEO_HEIGHT;

  if (project.aspectRatio) {
    const size = videoSizeMap[project.aspectRatio];
    if (size) {
      width = size.width;
      height = size.height;
    }
  }

  return (
    <div className="flex-grow flex-1 h-full flex items-center justify-center bg-background-dark dark:bg-background-light relative">
      <div className="w-full h-full flex items-center justify-center mx-6  max-h-[calc(100vh-25rem)]">
        <Player
          className={cn(
            "[&_video]:shadow-2xl inline-flex items-center justify-center mx-auto w-full h-full max-h-[500px] 3xl:max-h-[800px]",
            {
              "aspect-[16/9]": project.aspectRatio === "16:9",
              "aspect-[9/16]": project.aspectRatio === "9:16",
              "aspect-[1/1]": project.aspectRatio === "1:1",
            },
          )}
          ref={playerRef}
          component={MainComposition}
          inputProps={{
            project,
            tracks,
            frames,
            mediaItems,
          }}
          durationInFrames={duration * FPS}
          fps={FPS}
          compositionWidth={width}
          compositionHeight={height}
          style={{
            width: "100%",
            height: "100%",
          }}
          clickToPlay={true}
          showPosterWhenPaused={false}
          autoPlay={false}
          loop={false}
          controls={false}
        />
      </div>
    </div>
  );
}

================
File: src/data/db.ts
================
import { openDB } from "idb";
import type {
  MediaItem,
  VideoKeyFrame,
  VideoProject,
  VideoTrack,
} from "./schema";

function open() {
  return openDB("ai-vstudio-db-v2", 1, {
    upgrade(db) {
      db.createObjectStore("projects", { keyPath: "id" });

      const trackStore = db.createObjectStore("tracks", { keyPath: "id" });
      trackStore.createIndex("by_projectId", "projectId");

      const keyFrameStore = db.createObjectStore("keyFrames", {
        keyPath: "id",
      });
      keyFrameStore.createIndex("by_trackId", "trackId");

      const mediaStore = db.createObjectStore("media_items", {
        keyPath: "id",
      });
      mediaStore.createIndex("by_projectId", "projectId");
    },
  });
}

export const db = {
  projects: {
    async find(id: string): Promise<VideoProject | null> {
      const db = await open();
      return db.get("projects", id);
    },
    async list(): Promise<VideoProject[]> {
      const db = await open();
      return db.getAll("projects");
    },
    async create(project: Omit<VideoProject, "id">) {
      const db = await open();
      const tx = db.transaction("projects", "readwrite");
      const result = await tx.store.put({
        id: crypto.randomUUID(),
        ...project,
      });
      await tx.done;
      return result;
    },
    async update(id: string, project: Partial<VideoProject>) {
      const db = await open();
      const existing = await db.get("projects", id);
      if (!existing) return;
      return db.put("projects", {
        ...existing,
        ...project,
        id,
      });
    },
  },

  tracks: {
    async find(id: string): Promise<VideoTrack | null> {
      const db = await open();
      return db.get("tracks", id);
    },
    async tracksByProject(projectId: string): Promise<VideoTrack[]> {
      const db = await open();
      return db.getAllFromIndex("tracks", "by_projectId", projectId);
    },
    async create(track: Omit<VideoTrack, "id">) {
      const db = await open();
      return db.put("tracks", {
        id: crypto.randomUUID(),
        ...track,
      });
    },
  },

  keyFrames: {
    async find(id: string): Promise<VideoKeyFrame | null> {
      const db = await open();
      return db.get("keyFrames", id);
    },
    async keyFramesByTrack(trackId: string): Promise<VideoKeyFrame[]> {
      const db = await open();
      const result = await db.getAllFromIndex(
        "keyFrames",
        "by_trackId",
        trackId,
      );
      return result.toSorted((a, b) => a.timestamp - b.timestamp);
    },
    async create(keyFrame: Omit<VideoKeyFrame, "id">) {
      const db = await open();
      return db.put("keyFrames", {
        id: crypto.randomUUID(),
        ...keyFrame,
      });
    },
    async update(id: string, keyFrame: Partial<VideoKeyFrame>) {
      const db = await open();
      const existing = await db.get("keyFrames", id);
      if (!existing) return;

      return db.put("keyFrames", {
        ...existing,
        ...keyFrame,
        id,
      });
    },
    async delete(id: string) {
      const db = await open();
      return db.delete("keyFrames", id);
    },
  },

  media: {
    async find(id: string): Promise<MediaItem | null> {
      const db = await open();
      return db.get("media_items", id);
    },
    async mediaByProject(projectId: string): Promise<MediaItem[]> {
      const db = await open();
      const results = await db.getAllFromIndex(
        "media_items",
        "by_projectId",
        projectId,
      );

      return results.toSorted((a, b) => b.createdAt - a.createdAt);
    },
    async create(media: Omit<MediaItem, "id">) {
      const db = await open();
      const tx = db.transaction("media_items", "readwrite");
      const id = crypto.randomUUID().toString();
      const result = await tx.store.put({
        id,
        ...media,
      });
      await tx.done;
      return result;
    },
    async update(id: string, media: Partial<MediaItem>) {
      const db = await open();
      const existing = await db.get("media_items", id);
      if (!existing) return;
      const tx = db.transaction("media_items", "readwrite");
      const result = await tx.store.put({
        ...existing,
        ...media,
        id,
      });
      await tx.done;
      return result;
    },
    async delete(id: string) {
      const db = await open();
      const media: MediaItem | null = await db.get("media_items", id);
      if (!media) return;
      // Delete associated keyframes
      const tracks = await db.getAllFromIndex(
        "tracks",
        "by_projectId",
        media.projectId,
      );
      const trackIds = tracks.map((track) => track.id);
      const frames = (
        await Promise.all(
          trackIds.map(
            (trackId) =>
              db.getAllFromIndex("keyFrames", "by_trackId", trackId) as Promise<
                VideoKeyFrame[]
              >,
          ),
        )
      )
        .flatMap((f) => f)
        .filter((f) => f.data.mediaId === id)
        .map((f) => f.id);
      const tx = db.transaction(["media_items", "keyFrames"], "readwrite");
      await Promise.all(
        frames.map((id) => tx.objectStore("keyFrames").delete(id)),
      );
      await tx.objectStore("media_items").delete(id);
      await tx.done;
    },
  },
} as const;

================
File: src/data/mutations.ts
================
import { fal } from "@/lib/fal";
import { useMutation, useQueryClient } from "@tanstack/react-query";
import { db } from "./db";
import { queryKeys } from "./queries";
import type { VideoProject } from "./schema";

export const useProjectUpdater = (projectId: string) => {
  const queryClient = useQueryClient();
  return useMutation({
    mutationFn: (project: Partial<VideoProject>) =>
      db.projects.update(projectId, project),
    onSettled: () => {
      queryClient.invalidateQueries({ queryKey: queryKeys.project(projectId) });
    },
  });
};

export const useProjectCreator = () => {
  const queryClient = useQueryClient();
  return useMutation({
    mutationFn: (project: Omit<VideoProject, "id">) =>
      db.projects.create(project),
    onSettled: () => {
      queryClient.invalidateQueries({ queryKey: queryKeys.projects });
    },
  });
};

type JobCreatorParams = {
  projectId: string;
  endpointId: string;
  mediaType: "video" | "image" | "voiceover" | "music";
  input: Record<string, any>;
};

export const useJobCreator = ({
  projectId,
  endpointId,
  mediaType,
  input,
}: JobCreatorParams) => {
  const queryClient = useQueryClient();
  return useMutation({
    mutationFn: () =>
      fal.queue.submit(endpointId, {
        input,
      }),
    onSuccess: async (data) => {
      await db.media.create({
        projectId,
        createdAt: Date.now(),
        mediaType,
        kind: "generated",
        endpointId,
        requestId: data.request_id,
        status: "pending",
        input,
      });

      await queryClient.invalidateQueries({
        queryKey: queryKeys.projectMediaItems(projectId),
      });
    },
  });
};

================
File: src/data/queries.ts
================
import {
  keepPreviousData,
  type QueryClient,
  useQuery,
} from "@tanstack/react-query";
import { db } from "./db";
import {
  MediaItem,
  PROJECT_PLACEHOLDER,
  VideoKeyFrame,
  VideoTrack,
} from "./schema";

export const queryKeys = {
  projects: ["projects"],
  project: (projectId: string) => ["project", projectId],
  projectMediaItems: (projectId: string) => ["mediaItems", projectId],
  projectMedia: (projectId: string, jobId: string) => [
    "media",
    projectId,
    jobId,
  ],
  projectTracks: (projectId: string) => ["tracks", projectId],
  projectPreview: (projectId: string) => ["preview", projectId],
};

export const refreshVideoCache = async (
  queryClient: QueryClient,
  projectId: string,
) =>
  Promise.all([
    queryClient.invalidateQueries({
      queryKey: queryKeys.projectTracks(projectId),
    }),
    queryClient.invalidateQueries({
      queryKey: queryKeys.projectPreview(projectId),
    }),
    queryClient.invalidateQueries({
      queryKey: ["frames"],
    }),
  ]);

export const useProject = (projectId: string) => {
  return useQuery({
    queryKey: queryKeys.project(projectId),
    queryFn: async () =>
      (await db.projects.find(projectId)) ?? PROJECT_PLACEHOLDER,
  });
};

export const useProjects = () => {
  return useQuery({
    queryKey: queryKeys.projects,
    queryFn: db.projects.list,
  });
};

export const useProjectMediaItems = (projectId: string) => {
  return useQuery({
    queryKey: queryKeys.projectMediaItems(projectId),
    queryFn: () => db.media.mediaByProject(projectId),
    refetchOnMount: true,
    refetchOnWindowFocus: true,
    placeholderData: keepPreviousData,
  });
};

export type VideoCompositionData = {
  tracks: VideoTrack[];
  frames: Record<string, VideoKeyFrame[]>;
  mediaItems: Record<string, MediaItem>;
};

export const EMPTY_VIDEO_COMPOSITION: VideoCompositionData = {
  tracks: [],
  frames: {},
  mediaItems: {},
};

export const useVideoComposition = (projectId: string) =>
  useQuery({
    queryKey: queryKeys.projectPreview(projectId),
    queryFn: async () => {
      const tracks = await db.tracks.tracksByProject(projectId);
      const frames = (
        await Promise.all(
          tracks.map((track) => db.keyFrames.keyFramesByTrack(track.id)),
        )
      ).flatMap((f) => f);
      const mediaItems = await db.media.mediaByProject(projectId);
      return {
        tracks,
        frames: Object.fromEntries(
          tracks.map((track) => [
            track.id,
            frames.filter((f) => f.trackId === track.id),
          ]),
        ),
        mediaItems: Object.fromEntries(
          mediaItems.map((item) => [item.id, item]),
        ),
      } satisfies VideoCompositionData;
    },
  });

================
File: src/data/schema.ts
================
export type AspectRatio = "16:9" | "9:16" | "1:1";

export type VideoProject = {
  id: string;
  title: string;
  description: string;
  aspectRatio: AspectRatio;
};

export const PROJECT_PLACEHOLDER: VideoProject = {
  id: "",
  title: "",
  description: "",
  aspectRatio: "16:9",
};

export type VideoTrackType = "video" | "music" | "voiceover";

export const TRACK_TYPE_ORDER: Record<VideoTrackType, number> = {
  video: 1,
  music: 2,
  voiceover: 3,
};

export type VideoTrack = {
  id: string;
  locked: boolean;
  label: string;
  type: VideoTrackType;
  projectId: string;
};

export const MAIN_VIDEO_TRACK: VideoTrack = {
  id: "main",
  locked: true,
  label: "Main",
  type: "video",
  projectId: PROJECT_PLACEHOLDER.id,
};

export type VideoKeyFrame = {
  id: string;
  timestamp: number;
  duration: number;
  trackId: string;
  data: KeyFrameData;
};

export type KeyFrameData = {
  type: "prompt" | "image" | "video" | "voiceover" | "music";
  mediaId: string;
} & (
  | {
      type: "prompt";
      prompt: string;
    }
  | {
      type: "image";
      prompt: string;
      url: string;
    }
  | {
      type: "video";
      prompt: string;
      url: string;
    }
);

export type MediaItem = {
  id: string;
  kind: "generated" | "uploaded";
  endpointId?: string;
  requestId?: string;
  projectId: string;
  mediaType: "image" | "video" | "music" | "voiceover";
  status: "pending" | "running" | "completed" | "failed";
  createdAt: number;
  input?: Record<string, any>;
  output?: Record<string, any>;
  url?: string;
  metadata?: Record<string, any>; // TODO: Define the metadata schema
} & (
  | {
      kind: "generated";
      endpointId: string;
      requestId: string;
      input: Record<string, any>;
      output?: Record<string, any>;
    }
  | {
      kind: "uploaded";
      url: string;
    }
);

================
File: src/data/seed.ts
================
import { db } from "@/data/db";
import { VideoProject, MediaItem, VideoTrack, VideoKeyFrame } from "./schema";

type ProjectSeed = {
  project: VideoProject;
  media: MediaItem[];
  tracks: VideoTrack[];
  keyframes: VideoKeyFrame[];
};

const TEMPLATE_PROJECT_SEED: ProjectSeed = {
  project: {
    title: "The morning brew",
    description:
      "A starter project that shows off the features of the video editor.",
    aspectRatio: "16:9",
    id: "433685b7-3494-4a56-9657-c1522686139d",
  },
  media: [
    {
      projectId: "433685b7-3494-4a56-9657-c1522686139d",
      createdAt: 1737486876353,
      mediaType: "music",
      kind: "generated",
      endpointId: "fal-ai/stable-audio",
      requestId: "7d2d12f8-41d8-4e20-87a1-d4fdf8a7a28b",
      status: "completed",
      input: {
        prompt: "lofi beats, chill morning song in a bossa nova style",
        seconds_total: 17,
      },
      id: "c41cbde0-680b-4dd3-a94e-04d66726a1ed",
      output: {
        audio_file: {
          url: "https://v2.fal.media/files/c8c6eb5859584e9382ac7c6202a98ac2_tmpla47ie4n.wav",
          content_type: "application/octet-stream",
          file_name: "tmpla47ie4n.wav",
          file_size: 2998878,
        },
      },
      metadata: {
        media_type: "audio",
        url: "https://v2.fal.media/files/c8c6eb5859584e9382ac7c6202a98ac2_tmpla47ie4n.wav",
        content_type: "audio/wav",
        file_name: "c8c6eb5859584e9382ac7c6202a98ac2_tmpla47ie4n.wav",
        file_size: 2998878,
        duration: 17,
        bitrate: 1411236,
        codec: "pcm_s16le",
        container: "wav",
        channels: 2,
        sample_rate: 44100,
      },
    },
    {
      projectId: "433685b7-3494-4a56-9657-c1522686139d",
      createdAt: 1737486790214,
      mediaType: "voiceover",
      kind: "generated",
      endpointId: "fal-ai/playht/tts/v3",
      requestId: "991f473d-7e15-48d1-bc01-fe033d7f7773",
      status: "completed",
      input: {
        prompt: "There's nothing like a fresh cup of coffee in a sunny morning",
        seconds_total: 30,
        voice: "Cecil (English (GB)/British)",
        input: "There's nothing like a fresh cup of coffee in a sunny morning",
      },
      id: "1b8473a5-7a50-4354-be7a-3d3d5c1edc58",
      output: {
        audio: {
          url: "https://v3.fal.media/files/rabbit/cZzZVGe4ugQEAMBruZvgh_01a71259-c5f1-4fc2-940e-98f1ddc0d34a.mp3",
          content_type: "audio/mpeg",
          file_name: "01a71259-c5f1-4fc2-940e-98f1ddc0d34a.mp3",
          file_size: 71469,
          duration: 2.946,
        },
      },
      metadata: {
        media_type: "audio",
        url: "https://v3.fal.media/files/rabbit/cZzZVGe4ugQEAMBruZvgh_01a71259-c5f1-4fc2-940e-98f1ddc0d34a.mp3",
        content_type: "audio/mp3",
        file_name:
          "cZzZVGe4ugQEAMBruZvgh_01a71259-c5f1-4fc2-940e-98f1ddc0d34a.mp3",
        file_size: 71469,
        duration: 2.976,
        bitrate: 192120,
        codec: "mp3",
        container: "mp3",
        channels: 1,
        sample_rate: 48000,
      },
    },
    {
      projectId: "433685b7-3494-4a56-9657-c1522686139d",
      createdAt: 1737486533032,
      mediaType: "video",
      kind: "generated",
      endpointId: "fal-ai/minimax/video-01-live/image-to-video",
      requestId: "994b9ca4-a710-4c50-ab1e-ebd5fb2b3648",
      status: "completed",
      input: {
        prompt: "Coffee beans grinding on a morning setting",
        image_url: "https://fal.media/files/rabbit/lqoM2SZ6yArJG4dc5i6Eu.png",
        aspect_ratio: "16:9",
      },
      id: "89e9a2f8-a6a7-445d-bf45-f489395db200",
      output: {
        video: {
          url: "https://fal.media/files/koala/3xD7JOTCNYwUwiOaV047W_output.mp4",
          content_type: "video/mp4",
          file_name: "output.mp4",
          file_size: 372192,
        },
      },
      metadata: {
        media_type: "video",
        url: "https://fal.media/files/koala/3xD7JOTCNYwUwiOaV047W_output.mp4",
        content_type: "video/mov",
        file_name: "3xD7JOTCNYwUwiOaV047W_output.mp4",
        file_size: 372192,
        duration: 5.64,
        bitrate: 527931,
        codec: "h264",
        container: "mov",
        fps: 25,
        frame_count: 141,
        timebase: "1/12800",
        resolution: {
          aspect_ratio: "16:9",
          width: 1280,
          height: 720,
        },
        format: {
          container: "mov",
          video_codec: "h264",
          profile: "High",
          level: 31,
          pixel_format: "yuv420p",
          bitrate: 527931,
        },
        audio: null,
        start_frame_url:
          "https://v3.fal.media/files/lion/lNDR8wzGJmAgPtSdroZso_start_frame.png",
        end_frame_url:
          "https://v3.fal.media/files/monkey/IrmTo3WlXwU_xt2e7LWZR_end_frame.png",
      },
    },
    {
      projectId: "433685b7-3494-4a56-9657-c1522686139d",
      createdAt: 1737486526606,
      mediaType: "video",
      kind: "generated",
      endpointId: "fal-ai/minimax/video-01-live/image-to-video",
      requestId: "830a26dc-e7ec-4509-b12d-15deeb619c37",
      status: "completed",
      input: {
        prompt:
          "a steaming cup of coffee on a cozy morning scene, with the warm sunlight peeking through the blinds, as the sound of gentle brewing fills the air.",
        image_url: "https://fal.media/files/koala/zyEDUZ9j-AsNCatPaFmYO.png",
        aspect_ratio: "16:9",
      },
      id: "4fc9a50b-6a3c-47ef-93e8-48341b4f050e",
      output: {
        video: {
          url: "https://fal.media/files/kangaroo/tusxIei9BCsgat_K0H_Dr_output.mp4",
          content_type: "video/mp4",
          file_name: "output.mp4",
          file_size: 389484,
        },
      },
      metadata: {
        media_type: "video",
        url: "https://fal.media/files/kangaroo/tusxIei9BCsgat_K0H_Dr_output.mp4",
        content_type: "video/mov",
        file_name: "tusxIei9BCsgat_K0H_Dr_output.mp4",
        file_size: 389484,
        duration: 5.64,
        bitrate: 552459,
        codec: "h264",
        container: "mov",
        fps: 25,
        frame_count: 141,
        timebase: "1/12800",
        resolution: {
          aspect_ratio: "16:9",
          width: 1280,
          height: 720,
        },
        format: {
          container: "mov",
          video_codec: "h264",
          profile: "High",
          level: 31,
          pixel_format: "yuv420p",
          bitrate: 552459,
        },
        audio: null,
        start_frame_url:
          "https://v3.fal.media/files/tiger/vcYlw7KVM_GYJ27Rbwe5k_start_frame.png",
        end_frame_url:
          "https://v3.fal.media/files/koala/oslkG0Zf4xKBWk1oY95GD_end_frame.png",
      },
    },
    {
      projectId: "433685b7-3494-4a56-9657-c1522686139d",
      createdAt: 1737486511674,
      mediaType: "image",
      kind: "generated",
      endpointId: "fal-ai/flux/dev",
      requestId: "69ee44e4-b55f-475c-8691-fd88ed8af293",
      status: "completed",
      input: {
        prompt: "Coffee beans grinding on a morning setting",
        image_size: "landscape_16_9",
        seconds_total: 30,
      },
      id: "92f32f84-c11e-4e26-afd9-c73d4ac88c25",
      output: {
        images: [
          {
            url: "https://fal.media/files/rabbit/lqoM2SZ6yArJG4dc5i6Eu.png",
            width: 1024,
            height: 576,
            content_type: "image/jpeg",
          },
        ],
        timings: {
          inference: 1.5897783394902945,
        },
        seed: 2349012095,
        has_nsfw_concepts: [false],
        prompt: "Coffee beans grinding on a morning setting",
      },
    },
    {
      projectId: "433685b7-3494-4a56-9657-c1522686139d",
      createdAt: 1737486437745,
      mediaType: "image",
      kind: "generated",
      endpointId: "fal-ai/flux/dev",
      requestId: "e62ee21f-270f-4260-a1d0-139494a8e3f3",
      status: "completed",
      input: {
        prompt:
          "a steaming cup of coffee on a cozy morning scene, with the warm sunlight peeking through the blinds, as the sound of gentle brewing fills the air.",
        image_size: "landscape_16_9",
        seconds_total: 30,
      },
      id: "25a45955-5274-433b-8cdb-8387ecff5157",
      output: {
        images: [
          {
            url: "https://fal.media/files/koala/zyEDUZ9j-AsNCatPaFmYO.png",
            width: 1024,
            height: 576,
            content_type: "image/jpeg",
          },
        ],
        timings: {
          inference: 1.599838787689805,
        },
        seed: 3670790609,
        has_nsfw_concepts: [false],
        prompt:
          "a steaming cup of coffee on a cozy morning scene, with the warm sunlight peeking through the blinds, as the sound of gentle brewing fills the air.",
      },
    },
    {
      projectId: "433685b7-3494-4a56-9657-c1522686139d",
      createdAt: 1737486370974,
      mediaType: "video",
      kind: "generated",
      endpointId: "fal-ai/minimax/video-01-live/image-to-video",
      requestId: "278d33fc-2eb9-4aaa-9f61-2851c5a82fbb",
      status: "completed",
      input: {
        prompt:
          "Image of a steaming cup of coffee in a quiet morning setting, showcasing the morning brew process, using the Coffee machine as a central feature",
        image_url: "https://fal.media/files/kangaroo/w0qQbeIhqmZfEttswt-HD.png",
        aspect_ratio: "16:9",
      },
      id: "40cec777-9f45-465a-ace4-4fdc4000fa9e",
      output: {
        video: {
          url: "https://fal.media/files/tiger/3MaLDMcELHRLdI_wpaeIB_output.mp4",
          content_type: "video/mp4",
          file_name: "output.mp4",
          file_size: 398174,
        },
      },
      metadata: {
        media_type: "video",
        url: "https://fal.media/files/tiger/3MaLDMcELHRLdI_wpaeIB_output.mp4",
        content_type: "video/mov",
        file_name: "3MaLDMcELHRLdI_wpaeIB_output.mp4",
        file_size: 398174,
        duration: 5.64,
        bitrate: 564785,
        codec: "h264",
        container: "mov",
        fps: 25,
        frame_count: 141,
        timebase: "1/12800",
        resolution: {
          aspect_ratio: "16:9",
          width: 1280,
          height: 720,
        },
        format: {
          container: "mov",
          video_codec: "h264",
          profile: "High",
          level: 31,
          pixel_format: "yuv420p",
          bitrate: 564785,
        },
        audio: null,
        start_frame_url:
          "https://v3.fal.media/files/monkey/WuO2_lfG3R6uggUWBA6Qv_start_frame.png",
        end_frame_url:
          "https://v3.fal.media/files/elephant/Ti8XbHVdc78bgONN4waSL_end_frame.png",
      },
    },
    {
      projectId: "433685b7-3494-4a56-9657-c1522686139d",
      createdAt: 1737486358044,
      mediaType: "image",
      kind: "generated",
      endpointId: "fal-ai/flux/dev",
      requestId: "a1adf628-cc0a-4700-ba06-0aa8a96cb5e8",
      status: "completed",
      input: {
        prompt:
          "Image of a steaming cup of coffee in a quiet morning setting, showcasing the morning brew process, using the Coffee machine as a central feature",
        image_size: "landscape_16_9",
        seconds_total: 30,
      },
      id: "8e1fc138-a967-4d05-8cbf-aeb6e8e643e4",
      output: {
        images: [
          {
            url: "https://fal.media/files/kangaroo/w0qQbeIhqmZfEttswt-HD.png",
            width: 1024,
            height: 576,
            content_type: "image/jpeg",
          },
        ],
        timings: {
          inference: 1.603529468877241,
        },
        seed: 20684661,
        has_nsfw_concepts: [false],
        prompt:
          "Image of a steaming cup of coffee in a quiet morning setting, showcasing the morning brew process, using the Coffee machine as a central feature",
      },
    },
  ],
  tracks: [
    {
      projectId: "433685b7-3494-4a56-9657-c1522686139d",
      type: "voiceover",
      label: "voiceover",
      locked: true,
      id: "04bd485f-0856-485c-96cf-5b698c690c39",
    },
    {
      projectId: "433685b7-3494-4a56-9657-c1522686139d",
      type: "video",
      label: "video",
      locked: true,
      id: "700049df-22c3-4419-b886-c9b200851ac8",
    },
    {
      projectId: "433685b7-3494-4a56-9657-c1522686139d",
      type: "music",
      label: "music",
      locked: true,
      id: "dc875d02-e09c-4d57-9d71-1c152e1a5798",
    },
  ],
  keyframes: [
    {
      trackId: "04bd485f-0856-485c-96cf-5b698c690c39",
      data: {
        mediaId: "1b8473a5-7a50-4354-be7a-3d3d5c1edc58",
        type: "prompt",
        prompt: "There's nothing like a fresh cup of coffee in a sunny morning",
      },
      timestamp: 9207.920792079207,
      duration: 2976,
      id: "5682fa05-4f24-40e1-9de9-c4db1ed8aeff",
    },
    {
      trackId: "700049df-22c3-4419-b886-c9b200851ac8",
      data: {
        mediaId: "89e9a2f8-a6a7-445d-bf45-f489395db200",
        type: "image",
        prompt: "Coffee beans grinding on a morning setting",
      },
      timestamp: 1,
      duration: 5640,
      id: "e0ad9534-267d-47fb-b512-a723e7ac04c6",
    } as any,
    {
      trackId: "700049df-22c3-4419-b886-c9b200851ac8",
      data: {
        mediaId: "40cec777-9f45-465a-ace4-4fdc4000fa9e",
        type: "image",
        prompt:
          "Image of a steaming cup of coffee in a quiet morning setting, showcasing the morning brew process, using the Coffee machine as a central feature",
      },
      timestamp: 5642,
      duration: 5640,
      id: "2596b76f-a198-49a9-bcd8-6f8263fd45b1",
    },
    {
      trackId: "700049df-22c3-4419-b886-c9b200851ac8",
      data: {
        mediaId: "4fc9a50b-6a3c-47ef-93e8-48341b4f050e",
        type: "image",
        prompt:
          "a steaming cup of coffee on a cozy morning scene, with the warm sunlight peeking through the blinds, as the sound of gentle brewing fills the air.",
      },
      timestamp: 11283,
      duration: 5640,
      id: "7e846b84-934b-49a2-a9ac-6b21ca7edb98",
    },
    {
      trackId: "dc875d02-e09c-4d57-9d71-1c152e1a5798",
      data: {
        mediaId: "c41cbde0-680b-4dd3-a94e-04d66726a1ed",
        type: "prompt",
        prompt: "lofi beats, chill morning song in a bossa nova style",
      },
      timestamp: 1,
      duration: 17000,
      id: "2c2fc7a7-76d8-48a6-b53d-b89cf09e249b",
    },
  ],
};

export const seedDatabase = async () => {
  await db.projects.create(TEMPLATE_PROJECT_SEED.project);
  await Promise.all(TEMPLATE_PROJECT_SEED.media.map(db.media.create));
  await Promise.all(TEMPLATE_PROJECT_SEED.tracks.map(db.tracks.create));
  await Promise.all(TEMPLATE_PROJECT_SEED.keyframes.map(db.keyFrames.create));
};

================
File: src/data/store.ts
================
"use client";

import { AVAILABLE_ENDPOINTS } from "@/lib/fal";
import { LlmModelType } from "@/lib/prompt";
import type { PlayerRef } from "@remotion/player";
import { createContext, useContext } from "react";
import { createStore } from "zustand";
import { useStore } from "zustand/react";

export const LAST_PROJECT_ID_KEY = "__aivs_lastProjectId";

export type MediaType = "image" | "video" | "voiceover" | "music";

export type GenerateData = {
  prompt: string;
  image?: File | string | null;
  video_url?: File | string | null;
  audio_url?: File | string | null;
  duration: number;
  voice: string;
  [key: string]: any;
};

interface VideoProjectProps {
  projectId: string;
  projectDialogOpen: boolean;
  player: PlayerRef | null;
  playerCurrentTimestamp: number;
  playerState: "playing" | "paused";
  generateDialogOpen: boolean;
  generateMediaType: MediaType;
  selectedMediaId: string | null;
  selectedKeyframes: string[];
  generateData: GenerateData;
  exportDialogOpen: boolean;
  endpointId: string;
  llmModel: LlmModelType;
}

interface VideoProjectState extends VideoProjectProps {
  setProjectId: (projectId: string) => void;
  setProjectDialogOpen: (open: boolean) => void;
  resetGenerateData: () => void;
  setPlayer: (player: PlayerRef) => void;
  setPlayerCurrentTimestamp: (timestamp: number) => void;
  setPlayerState: (state: "playing" | "paused") => void;
  setGenerateMediaType: (mediaType: MediaType) => void;
  openGenerateDialog: (mediaType?: MediaType) => void;
  closeGenerateDialog: () => void;
  setSelectedMediaId: (mediaId: string | null) => void;
  selectKeyframe: (frameId: string) => void;
  setGenerateData: (generateData: Partial<GenerateData>) => void;
  setExportDialogOpen: (open: boolean) => void;
  setEndpointId: (endpointId: string) => void;
  setLlmModel: (model: LlmModelType) => void;
  onGenerate: () => void;
}

const DEFAULT_PROPS: VideoProjectProps = {
  projectId: "",
  endpointId: AVAILABLE_ENDPOINTS[0].endpointId,
  projectDialogOpen: false,
  player: null,
  playerCurrentTimestamp: 0,
  playerState: "paused",
  generateDialogOpen: false,
  generateMediaType: "image",
  selectedMediaId: null,
  selectedKeyframes: [],
  generateData: {
    prompt: "",
    image: null,
    duration: 30,
    voice: "",
    video_url: null,
    audio_url: null,
  },
  exportDialogOpen: false,
  llmModel: "meta-llama/llama-3.2-1b-instruct",
};

type VideoProjectStore = ReturnType<typeof createVideoProjectStore>;

export const createVideoProjectStore = (
  initProps?: Partial<VideoProjectProps>,
) => {
  return createStore<VideoProjectState>()((set, state) => ({
    ...DEFAULT_PROPS,
    ...initProps,
    projectDialogOpen: initProps?.projectId ? false : true,
    setEndpointId: (endpointId: string) => set({ endpointId }),
    setProjectId: (projectId: string) => set({ projectId }),
    setProjectDialogOpen: (projectDialogOpen: boolean) =>
      set({ projectDialogOpen }),
    setGenerateData: (generateData: Partial<GenerateData>) =>
      set({
        generateData: Object.assign({}, state().generateData, generateData),
      }),
    resetGenerateData: () =>
      set({
        generateData: {
          ...state().generateData,
          prompt: "",
          duration: 30,
          image: null,
          video_url: null,
          audio_url: null,
          voice: "",
        },
      }),
    // [NOTE]: This is a placeholder function
    onGenerate: () => {},
    setPlayer: (player: PlayerRef) => set({ player }),
    setPlayerCurrentTimestamp: (playerCurrentTimestamp: number) =>
      set({ playerCurrentTimestamp }),
    setPlayerState: (playerState: "playing" | "paused") => set({ playerState }),
    setGenerateMediaType: (generateMediaType: MediaType) =>
      set({ generateMediaType }),
    openGenerateDialog: (mediaType) =>
      set({
        generateDialogOpen: true,
        generateMediaType: mediaType ?? state().generateMediaType,
      }),
    closeGenerateDialog: () => set({ generateDialogOpen: false }),
    setSelectedMediaId: (selectedMediaId: string | null) =>
      set({ selectedMediaId }),
    selectKeyframe: (frameId: string) => {
      const selected = state().selectedKeyframes;
      if (selected.includes(frameId)) {
        set({
          selectedKeyframes: selected.filter((id) => id !== frameId),
        });
      } else {
        set({ selectedKeyframes: [...selected, frameId] });
      }
    },
    setExportDialogOpen: (exportDialogOpen: boolean) =>
      set({ exportDialogOpen }),
    setLlmModel: (model: LlmModelType) => set({ llmModel: model }),
  }));
};

export const VideoProjectStoreContext = createContext<VideoProjectStore>(
  createVideoProjectStore(),
);

export function useVideoProjectStore<T>(
  selector: (state: VideoProjectState) => T,
): T {
  const store = useContext(VideoProjectStoreContext);
  return useStore(store, selector);
}

export function useProjectId() {
  return useVideoProjectStore((s) => s.projectId);
}

================
File: src/docs/model-guide.md
================
# Video Starter Kit Model Guide

This comprehensive guide provides detailed information about all AI models available in the Video Starter Kit, including usage tips, pricing, and best practices for optimal results.

## Table of Contents

- [Video Models](#video-models) 🎬
  - [Veo 2](#veo-2)
  - [LTX Video v0.95](#ltx-video-v095)
  - [Minimax Video 01 Live](#minimax-video-01-live)
  - [Hunyuan](#hunyuan)
  - [Kling 1.5 Pro](#kling-15-pro)
  - [Kling 1.0 Standard](#kling-10-standard)
  - [Luma Dream Machine 1.5](#luma-dream-machine-15)
  - [MMAudio V2](#mmaudio-v2)
  - [sync.so -- lipsync 1.8.0](#syncso----lipsync-180)
- [Image Models](#image-models) 🖼️
  - [Flux Dev](#flux-dev)
  - [Flux Schnell](#flux-schnell)
  - [Flux Pro 1.1 Ultra](#flux-pro-11-ultra)
  - [Stable Diffusion 3.5 Large](#stable-diffusion-35-large)
- [Music Models](#music-models) 🎵
  - [Minimax Music](#minimax-music)
  - [Stable Audio](#stable-audio)
- [Voiceover Models](#voiceover-models) 🎙️
  - [PlayHT TTS v3](#playht-tts-v3)
  - [PlayAI Text-to-Speech Dialog](#playai-text-to-speech-dialog)
  - [F5 TTS](#f5-tts)

---

## Video Models 🎬

<div style="background-color: rgba(0, 123, 255, 0.1); border-left: 4px solid #007bff; padding: 1rem; margin-bottom: 1.5rem;">
<strong>💡 Pro Tip:</strong> Video models transform text prompts or images into dynamic video content. They vary in style, quality, and motion capabilities.
</div>

### **Veo 2**

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">PROFESSIONAL</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">HIGH QUALITY</span>
</div>

![Veo 2 Example](/images/models/veo2-placeholder.jpg)

**Cost:**
- Base price: $1.25 for 5-second video
- Additional seconds: $0.25 per second (limited time offer)
- Regular pricing: $2.50 for 5-second video, $0.50 per additional second

**Best Use Cases:**
- Text-to-video generation
- High-quality 4K resolution output
- Realistic motion and physics
- Detailed textures and lighting

**Prompt Techniques:**
- Include camera movement terms like "panning," "zooming," "dolly shot"
- Specify film stock for consistent aesthetics (e.g., "Kodak Portra 400")
- Mention specific lens types for different looks (e.g., "35mm lens," "85mm portrait lens")
- Describe lighting conditions in detail

---

### **LTX Video v0.95**
# Video Starter Kit Model Guide

This comprehensive guide provides detailed information about all AI models available in the Video Starter Kit, including usage tips, pricing, and best practices for optimal results.

## Table of Contents

- [Video Models](#video-models) 🎬
  - [Veo 2](#veo-2)
  - [LTX Video v0.95](#ltx-video-v095)
  - [Minimax Video 01 Live](#minimax-video-01-live)
  - [Hunyuan](#hunyuan)
  - [Kling 1.5 Pro](#kling-15-pro)
  - [Kling 1.0 Standard](#kling-10-standard)
  - [Luma Dream Machine 1.5](#luma-dream-machine-15)
  - [MMAudio V2](#mmaudio-v2)
  - [sync.so -- lipsync 1.8.0](#syncso----lipsync-180)
- [Image Models](#image-models) 🖼️
  - [Flux Dev](#flux-dev)
  - [Flux Schnell](#flux-schnell)
  - [Flux Pro 1.1 Ultra](#flux-pro-11-ultra)
  - [Stable Diffusion 3.5 Large](#stable-diffusion-35-large)
- [Music Models](#music-models) 🎵
  - [Minimax Music](#minimax-music)
  - [Stable Audio](#stable-audio)
- [Voiceover Models](#voiceover-models) 🎙️
  - [PlayHT TTS v3](#playht-tts-v3)
  - [PlayAI Text-to-Speech Dialog](#playai-text-to-speech-dialog)
  - [F5 TTS](#f5-tts)

---

### LTX Video v0.95

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">PROFESSIONAL</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">HIGH QUALITY</span>
</div>

![Veo 2 Example](/images/models/veo2-placeholder.jpg)

Veo 2 creates videos with realistic motion and high-quality output, up to 4K resolution.

**Endpoint ID:** `fal-ai/veo2`

**✨ Capabilities:**
- Text-to-video generation
- High-quality 4K resolution output
- Realistic motion and physics
- Detailed textures and lighting

**💰 Pricing:**
- Base price: $1.25 for 5-second video
- Additional seconds: $0.25 per second (limited time offer)
- Regular pricing: $2.50 for 5-second video, $0.50 per additional second

**🔍 Best Prompts Include:**
- Detailed scene descriptions
- Lighting information
- Camera movement
- Stylistic preferences

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">The camera floats gently through rows of pastel-painted wooden beehives, buzzing honeybees gliding in and out of frame. The motion settles on a beekeeper standing at the center, wearing a pristine white beekeeping suit gleaming in the golden afternoon light. Behind him, tall sunflowers sway rhythmically in the breeze. Shot with a 35mm lens on Kodak Portra 400 film, the golden light creates rich textures.</pre>
</div>

**📝 Tips:**
- Include camera movement terms like "panning," "zooming," "dolly shot"
- Specify film stock for consistent aesthetics (e.g., "Kodak Portra 400")
- Mention specific lens types for different looks (e.g., "35mm lens," "85mm portrait lens")
- Describe lighting conditions in detail

---

### **LTX Video v0.95**

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">NEW</span>
<span style="background-color: #e6f7ee; color: #00843d; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">IMAGE-GUIDED</span>
</div>

![LTX Video Example](/images/models/ltx-video-placeholder.jpg)

**Cost:** Not specified

**Best Use Cases:**
- Generate videos from text prompts
- Use reference images as visual guides
- Direct image-to-video conversion

**Prompt Techniques:**
- For image-guided generation, use clear, high-quality reference images
- Balance text prompt specificity with visual guidance from images
- Use descriptive motion terms when you want specific movements
- Upload a reference image along with your prompt for better results
- The model will use both your text and image to guide the video generation

---

### **Minimax Video 01 Live**

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">VERSATILE</span>
<span style="background-color: #e6f7ee; color: #00843d; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">IMAGE-GUIDED</span>
</div>

![Minimax Video Example](/images/models/minimax-video-placeholder.jpg)

Generates high-quality videos with realistic motion and accurate physics simulation.

**Endpoint ID:** `fal-ai/minimax/video-01-live`

**✨ Capabilities:**
- Text-to-video generation
- Image-to-video transformation
- Realistic physics and motion

**🖼️ Input Assets:** Supports image input for video generation

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.</pre>
</div>

**📝 Tips:**
- Include specific character actions for better motion
- Describe detailed environments for richer scenes
- Specify clothing and items for better character rendering

---

### Hunyuan

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">HIGH QUALITY</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">DIVERSE MOTION</span>
</div>

![Hunyuan Example](/images/models/hunyuan-placeholder.jpg)

Produces videos with high visual quality, diverse motion patterns, and strong alignment with text prompts.

**Endpoint ID:** `fal-ai/hunyuan-video`

**✨ Capabilities:**
- High visual fidelity
- Diverse motion patterns
- Strong text-to-video alignment

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A serene lakeside at dawn, with mist rising from the water's surface, gentle ripples spreading across the lake as a fish jumps, mountains reflected in the clear water, golden sunlight gradually illuminating the scene</pre>
</div>

**📝 Tips:**
- Use detailed descriptions of lighting and atmosphere
- Include a mix of static and dynamic elements
- Specify the mood or emotional tone of the scene

---

### Kling 1.5 Pro

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">PROFESSIONAL</span>
<span style="background-color: #e6f7ee; color: #00843d; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">IMAGE-GUIDED</span>
</div>

![Kling Pro Example](/images/models/kling-pro-placeholder.jpg)

Generates high-quality videos with an emphasis on visual quality and smooth motion.

**Endpoint ID:** `fal-ai/kling-video/v1.5/pro`

**✨ Capabilities:**
- High-quality video generation
- Image-to-video transformation

**🖼️ Input Assets:** Supports image input for video generation


**🖼️ Input Assets:** Supports image input for video generation

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A futuristic cityscape at night, glowing holographic advertisements reflect in puddles on the street, flying cars zoom between towering skyscrapers, neon lights cast colorful shadows</pre>
</div>

**📝 Tips:**
- Include both static and dynamic elements
- Specify lighting details for better atmosphere
- When using image input, ensure it matches your text description

---

### MMAudio V2

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">AUDIO</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">SYNCED SOUND</span>
</div>

![MMAudio Example](/images/models/mmaudio-placeholder.jpg)

Generates synchronized audio for videos using text and video inputs.

**Endpoint ID:** `fal-ai/mmaudio-v2`

**✨ Capabilities:**
- Generate synchronized audio for videos
- Create sound effects based on text descriptions
- Combine with video models for complete audio-visual experiences

**🎞️ Input Assets:** Requires video input

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">Background city ambient noise with distant traffic, occasional car horns, people chatting, and footsteps on pavement</pre>
</div>

**📝 Tips:**
- Be specific about the type of sounds you want
- Describe layered audio elements for richer soundscapes
- Mention timing details if specific audio cues are needed

---

### sync.so -- lipsync 1.8.0

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">AUDIO</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">LIP SYNC</span>
</div>

![Lip Sync Example](/images/models/lipsync-placeholder.jpg)

Generates realistic lip-syncing animations from audio inputs.

**Endpoint ID:** `fal-ai/sync-lipsync`

**✨ Capabilities:**
- Generate lip-sync animations from audio
- Create realistic mouth movements for characters
- Synchronize speech with video content

**🎞️ Input Assets:** Requires both video and audio inputs

**📝 Tips:**
- Use clear, high-quality audio for best results
- Close-up videos of faces work best for lip-syncing
- Ensure the face in the video is well-lit and clearly visible

---

## Image Models

<div style="background-color: rgba(25, 135, 84, 0.1); border-left: 4px solid #198754; padding: 1rem; margin-bottom: 1.5rem;">
<strong>💡 Pro Tip:</strong> Image models convert text prompts into static images with various styles and qualities. They're perfect for creating reference images or standalone visual content.
</div>

### Flux Dev

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">VERSATILE</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">GENERAL PURPOSE</span>
</div>

![Flux Dev Example](/images/models/flux-dev-placeholder.jpg)

General-purpose text-to-image model for generating images from text prompts.

**Endpoint ID:** `fal-ai/flux/dev`

**✨ Capabilities:**
- Text-to-image generation
- General-purpose image creation

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A photorealistic image of a coastal city at sunset, golden light reflecting off skyscraper windows, palm trees lining the boulevard, small boats in the harbor, wispy clouds in the orange and purple sky</pre>
</div>

**📝 Tips:**
- Include detailed descriptions of visual elements
- Specify lighting and atmosphere
- Mention style preferences if you have any

---

### Flux Schnell

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">FAST</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">QUICK ITERATIONS</span>
</div>

![Flux Schnell Example](/images/models/flux-schnell-placeholder.jpg)

Fast variant of the Flux text-to-image model for quicker generations.

**Endpoint ID:** `fal-ai/flux/schnell`

**✨ Capabilities:**
- Faster text-to-image generation
- Similar quality to Flux Dev with faster processing

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A detailed watercolor painting of a medieval castle on a hilltop, autumn trees with red and orange leaves surrounding it, a winding path leading to the gate, small figures of knights and horses in the foreground</pre>
</div>

**📝 Tips:**
- Similar to Flux Dev but optimized for speed
- Great for rapid iterations and testing

---

### Flux Pro 1.1 Ultra

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">PROFESSIONAL</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">HIGH QUALITY</span>
</div>

![Flux Pro Example](/images/models/flux-pro-placeholder.jpg)

Professional-grade text-to-image model with enhanced visual quality.

**Endpoint ID:** `fal-ai/flux-pro/v1.1-ultra`

**✨ Capabilities:**
- High-quality text-to-image generation
- Enhanced detail and composition

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A hyper-realistic portrait of an elderly fisherman with weathered skin and deep wrinkles, wearing a faded blue cap, warm golden hour lighting, detailed texture of beard stubble and salt-crusted skin, shot with a Canon EOS R5 85mm f/1.2 lens</pre>
</div>

**📝 Tips:**
- Include camera and lens details for photorealistic outputs
- Specify lighting conditions in detail
- Use technical photography terms for better results

---

### Stable Diffusion 3.5 Large

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">ADVANCED</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">TYPOGRAPHY</span>
</div>

![Stable Diffusion Example](/images/models/sd35-placeholder.jpg)

Advanced text-to-image model with improved typography and complex prompt understanding.

**Endpoint ID:** `fal-ai/stable-diffusion-v35-large`

**✨ Capabilities:**
- High-quality image generation
- Excellent typography rendering
- Complex prompt understanding

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">A detailed cyberpunk street scene with Japanese and English neon signs saying "DIGITAL DREAMS" and "未来の都市", holographic advertisements, people with cybernetic implants walking under umbrella drones in the rain, reflective puddles on the ground</pre>
</div>

**📝 Tips:**
- Great for scenes requiring text or typography
- Can handle multi-language text prompts
- Excels at complex, detailed scenes

---

## Music Models

<div style="background-color: rgba(108, 117, 125, 0.1); border-left: 4px solid #6c757d; padding: 1rem; margin-bottom: 1.5rem;">
<strong>💡 Pro Tip:</strong> Music models generate original audio compositions based on text descriptions. They can create everything from simple beats to complex arrangements.
</div>

### Minimax Music

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">VERSATILE</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">REFERENCE AUDIO</span>
</div>

![Minimax Music Example](/images/models/minimax-music-placeholder.jpg)

Creates high-quality, diverse musical compositions with advanced AI techniques.

**Endpoint ID:** `fal-ai/minimax-music`

**✨ Capabilities:**
- Generate diverse musical compositions
- Create various music styles and genres
- Option to use reference audio for style matching

**🎵 Input Assets:** Optionally supports audio reference input

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompt:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">Upbeat electronic dance music with energetic synth leads, deep bass, and a driving drum beat at 128 BPM, building to a euphoric drop with arpeggiated melodies</pre>
</div>

**📝 Tips:**
- Specify BPM (beats per minute) for rhythm control
- Include instrument details
- Describe mood and energy level
- Mention genre or style references

---

### Stable Audio

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f8e5ff; color: #6f42c1; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">VERSATILE</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">SOUND EFFECTS</span>
</div>

![Stable Audio Example](/images/models/stable-audio-placeholder.jpg)

Generate high-quality music tracks and sound effects using Stability AI's audio model.

**Endpoint ID:** `fal-ai/stable-audio`

**✨ Capabilities:**
- Generate music in various styles
- Create sound effects and ambient sounds
- Produce variable-length audio outputs

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Prompts:</strong>

For Music:
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">128 BPM tech house drum loop with deep bass, crisp hi-hats, and a driving kick drum</pre>

For Sound Effects:
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">Door slam. High-quality, stereo.</pre>
</div>

**📝 Tips:**
- For music, specify BPM, genre, and instruments
- For sound effects, add "High-quality, stereo" to your prompt
- Keep prompts concise but descriptive
- Describe the mood or feeling you want to convey

---

## Voiceover Models

<div style="background-color: rgba(220, 53, 69, 0.1); border-left: 4px solid #dc3545; padding: 1rem; margin-bottom: 1.5rem;">
<strong>💡 Pro Tip:</strong> Voiceover models convert text into natural-sounding speech. They're ideal for narration, dialogue, and character voices in your projects.
</div>

### PlayHT TTS v3

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">NATURAL</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">EMOTIONAL</span>
</div>

![PlayHT Example](/images/models/playht-placeholder.jpg)

Generates fluent and natural speech with improved emotional tones.

**Endpoint ID:** `fal-ai/playht/tts/v3`

**✨ Capabilities:**
- High-quality text-to-speech
- Natural-sounding voice synthesis
- Fast processing for efficient workflows

**🗣️ Default Voice:** Dexter (English (US)/American)

**💰 Pricing:** $0.03 per minute per audio minute generated
Generates fluent and natural speech with improved emotional tones.

**Endpoint ID:** `fal-ai/playht/tts/v3`

**✨ Capabilities:**
- High-quality text-to-speech
- Natural-sounding voice synthesis
- Fast processing for efficient workflows

**🗣️ Default Voice:** Dexter (English (US)/American)

**💰 Pricing:** $0.03 per minute per audio minute generated

**🎯 Example Usage:**
- Narration for videos
- Voiceovers for presentations
- Audio content creation

**📝 Tips:**
- Use natural language patterns for best results
- Add punctuation to control pacing and pauses
- For emphasis, use italics or capitalize words

---

### PlayAI Text-to-Speech Dialog

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">DIALOGUE</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">MULTI-SPEAKER</span>
</div>

![PlayAI Dialog Example](/images/models/playai-dialog-placeholder.jpg)

Generates natural-sounding multi-speaker dialogues for storytelling and interactive media.

**Endpoint ID:** `fal-ai/playai/tts/dialog`

**✨ Capabilities:**
- Multi-speaker dialogue generation
- Natural-sounding conversations
- Enhanced expressiveness for storytelling

**🗣️ Default Voices:**
- Speaker 1: Jennifer (English (US)/American)
- Speaker 2: Furio (English (IT)/Italian)

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 1rem; margin: 1rem 0;">
<strong>Example Input:</strong>
<pre style="background-color: #f1f3f5; padding: 0.5rem; border-radius: 4px; overflow-x: auto;">Speaker 1: Have you heard about the new AI technologies?
Speaker 2: Yes, they're fascinating! I've been reading about them extensively.
Speaker 1: What interests you the most about them?
Speaker 2: The way they can create content that seems so human-like.</pre>
</div>

**📝 Tips:**
- Use speaker prefixes consistently
- Include natural conversational elements
- Vary sentence length and structure for realism
- Include emotional cues in parentheses for better expression

---

### F5 TTS

<div style="display: flex; align-items: center; margin-bottom: 1rem;">
<span style="background-color: #f2f7ff; color: #0057b8; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold; margin-right: 0.5rem;">VOICE CLONING</span>
<span style="background-color: #fff8e6; color: #bb6a00; padding: 0.25rem 0.5rem; border-radius: 4px; font-weight: bold;">REFERENCE AUDIO</span>
</div>

![F5 TTS Example](/images/models/f5-tts-placeholder.jpg)

Fluent and faithful speech synthesis with flow matching technology.

**Endpoint ID:** `fal-ai/f5-tts`

**✨ Capabilities:**
- High-quality text-to-speech
- Reference audio matching
- Natural speech patterns

**⚙️ Default Settings:**
- Uses a reference audio and text
- Removes silence by default

**🎯 Example Usage:**
- Voice cloning with reference audio
- Consistent voice styling across projects
- Voice preservation for legacy content

**📝 Tips:**
- Provide clear reference audio for better voice matching
- Ensure reference text matches the audio for accurate voice learning
- Keep generated text in a similar style to the reference for best results

---

## General Tips for All Models

<div style="background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 8px; padding: 1.5rem; margin: 2rem 0;">

### 🧠 Prompt Engineering

1. **Be Specific:** The more details you provide, the better the AI can match your vision
2. **Use Visual Language:** Describe what you want to see, hear, or experience
3. **Include Technical Details:** For appropriate models, include camera specs, music terminology, or voice characteristics
4. **Experiment:** Try different prompts and variations to find what works best

### 🔄 Optimal Workflow

1. **Start Simple:** Begin with basic prompts and add complexity
2. **Iterate:** Use generated results to inform your next prompt
3. **Combine Models:** Use multiple models together for comprehensive projects
4. **Save Successful Prompts:** Keep a library of prompts that work well

### 💼 Resource Usage

1. **Consider Costs:** Be aware of the pricing for each model
2. **Batch Processing:** Plan your generations to maximize efficiency
3. **Test First:** Use shorter generations initially to test concepts

</div>

---

*This guide will be regularly updated as new models are added or existing models are updated.*

================
File: src/hooks/use-toast.ts
================
"use client";

// Inspired by react-hot-toast library
import * as React from "react";

import type { ToastActionElement, ToastProps } from "@/components/ui/toast";

const TOAST_LIMIT = 1;
const TOAST_REMOVE_DELAY = 1000000;

type ToasterToast = ToastProps & {
  id: string;
  title?: React.ReactNode;
  description?: React.ReactNode;
  action?: ToastActionElement;
};

const actionTypes = {
  ADD_TOAST: "ADD_TOAST",
  UPDATE_TOAST: "UPDATE_TOAST",
  DISMISS_TOAST: "DISMISS_TOAST",
  REMOVE_TOAST: "REMOVE_TOAST",
} as const;

let count = 0;

function genId() {
  count = (count + 1) % Number.MAX_SAFE_INTEGER;
  return count.toString();
}

type ActionType = typeof actionTypes;

type Action =
  | {
      type: ActionType["ADD_TOAST"];
      toast: ToasterToast;
    }
  | {
      type: ActionType["UPDATE_TOAST"];
      toast: Partial<ToasterToast>;
    }
  | {
      type: ActionType["DISMISS_TOAST"];
      toastId?: ToasterToast["id"];
    }
  | {
      type: ActionType["REMOVE_TOAST"];
      toastId?: ToasterToast["id"];
    };

interface State {
  toasts: ToasterToast[];
}

const toastTimeouts = new Map<string, ReturnType<typeof setTimeout>>();

const addToRemoveQueue = (toastId: string) => {
  if (toastTimeouts.has(toastId)) {
    return;
  }

  const timeout = setTimeout(() => {
    toastTimeouts.delete(toastId);
    dispatch({
      type: "REMOVE_TOAST",
      toastId: toastId,
    });
  }, TOAST_REMOVE_DELAY);

  toastTimeouts.set(toastId, timeout);
};

export const reducer = (state: State, action: Action): State => {
  switch (action.type) {
    case "ADD_TOAST":
      return {
        ...state,
        toasts: [action.toast, ...state.toasts].slice(0, TOAST_LIMIT),
      };

    case "UPDATE_TOAST":
      return {
        ...state,
        toasts: state.toasts.map((t) =>
          t.id === action.toast.id ? { ...t, ...action.toast } : t,
        ),
      };

    case "DISMISS_TOAST": {
      const { toastId } = action;

      // ! Side effects ! - This could be extracted into a dismissToast() action,
      // but I'll keep it here for simplicity
      if (toastId) {
        addToRemoveQueue(toastId);
      } else {
        state.toasts.forEach((toast) => {
          addToRemoveQueue(toast.id);
        });
      }

      return {
        ...state,
        toasts: state.toasts.map((t) =>
          t.id === toastId || toastId === undefined
            ? {
                ...t,
                open: false,
              }
            : t,
        ),
      };
    }
    case "REMOVE_TOAST":
      if (action.toastId === undefined) {
        return {
          ...state,
          toasts: [],
        };
      }
      return {
        ...state,
        toasts: state.toasts.filter((t) => t.id !== action.toastId),
      };
  }
};

const listeners: Array<(state: State) => void> = [];

let memoryState: State = { toasts: [] };

function dispatch(action: Action) {
  memoryState = reducer(memoryState, action);
  listeners.forEach((listener) => {
    listener(memoryState);
  });
}

type Toast = Omit<ToasterToast, "id">;

function toast({ ...props }: Toast) {
  const id = genId();

  const update = (props: ToasterToast) =>
    dispatch({
      type: "UPDATE_TOAST",
      toast: { ...props, id },
    });
  const dismiss = () => dispatch({ type: "DISMISS_TOAST", toastId: id });

  dispatch({
    type: "ADD_TOAST",
    toast: {
      ...props,
      id,
      open: true,
      onOpenChange: (open) => {
        if (!open) dismiss();
      },
    },
  });

  return {
    id: id,
    dismiss,
    update,
  };
}

function useToast() {
  const [state, setState] = React.useState<State>(memoryState);

  React.useEffect(() => {
    listeners.push(setState);
    return () => {
      const index = listeners.indexOf(setState);
      if (index > -1) {
        listeners.splice(index, 1);
      }
    };
  }, [state]);

  return {
    ...state,
    toast,
    dismiss: (toastId?: string) => dispatch({ type: "DISMISS_TOAST", toastId }),
  };
}

export { useToast, toast };

================
File: src/lib/config.ts
================
/**
 * Application configuration settings
 * Centralized configuration for easy updates between development and production
 */

interface AppConfig {
  // Main application URLs
  urls: {
    // NovelVision main website
    main: string;

    // Writing workspace page
    writingWorkspace: string;

    // Settings page
    settings: string;

    // Character setup page
    characterSetup: string;

    // Story outline page
    storyOutline: string;
  };

  // Environment configuration
  environment: {
    // Is this a production build
    isProduction: boolean;
  };
}

// Development configuration
const devConfig: AppConfig = {
  urls: {
    main: "http://localhost:8000",
    writingWorkspace:
      "http://localhost:8000/writer_workspace/writing_working_space.html",
    settings: "http://localhost:8000/pages/settings.html",
    characterSetup: "http://localhost:8000/pages/character_setup.html",
    storyOutline: "http://localhost:8000/pages/story-outline.html",
  },
  environment: {
    isProduction: false,
  },
};

// Production configuration
const prodConfig: AppConfig = {
  urls: {
    main: "https://novelvisionai.art",
    writingWorkspace:
      "https://novelvisionai.art/writer_workspace/writing_working_space.html",
    settings: "https://novelvisionai.art/pages/settings.html",
    characterSetup: "https://novelvisionai.art/pages/character_setup.html",
    storyOutline: "https://novelvisionai.art/pages/story-outline.html",
  },
  environment: {
    isProduction: true,
  },
};

// Determine which config to use
// In a real environment, you might use environment variables like NEXT_PUBLIC_APP_ENV
const isProduction = process.env.NODE_ENV === "production";
const config: AppConfig = isProduction ? prodConfig : devConfig;

export default config;

================
File: src/lib/fal.ts
================
"use client";

import { createFalClient } from "@fal-ai/client";

// Add logging for debugging
console.log("🔍 Initializing Fal.ai client...");

// Function to wait for the falai_key to be set in localStorage
async function waitForFalaiKey(maxWaitMs = 5000, checkIntervalMs = 100) {
  console.log("🔍 Waiting for falai_key to be set in localStorage...");

  return new Promise<string>((resolve) => {
    // Check if the key is already in localStorage
    const initialCheck = localStorage?.getItem("falai_key");
    if (initialCheck) {
      console.log(
        "🔍 falai_key already in localStorage:",
        initialCheck.substring(0, 5) + "...",
      );
      return resolve(initialCheck);
    }

    // Set up a timeout to resolve with empty string after maxWaitMs
    const timeoutId = setTimeout(() => {
      console.warn("❌ Timed out waiting for falai_key");
      clearInterval(intervalId);
      resolve("");
    }, maxWaitMs);

    // Check localStorage at regular intervals
    const intervalId = setInterval(() => {
      const key = localStorage?.getItem("falai_key");
      if (key) {
        console.log(
          "🔍 falai_key found in localStorage:",
          key.substring(0, 5) + "...",
        );
        clearTimeout(timeoutId);
        clearInterval(intervalId);
        resolve(key);
      }
    }, checkIntervalMs);

    // Also listen for the FALAI_KEY_RESPONSE message
    const messageHandler = (event: MessageEvent) => {
      if (
        event.data &&
        event.data.type === "FALAI_KEY_RESPONSE" &&
        event.data.falai_key
      ) {
        console.log(
          "🔍 Received FALAI_KEY_RESPONSE:",
          event.data.falai_key.substring(0, 5) + "...",
        );
        localStorage.setItem("falai_key", event.data.falai_key);
        clearTimeout(timeoutId);
        clearInterval(intervalId);
        window.removeEventListener("message", messageHandler);
        resolve(event.data.falai_key);
      }
    };

    window.addEventListener("message", messageHandler);

    // Request the key from the parent
    try {
      window.parent.postMessage({ type: "REQUEST_FALAI_KEY" }, "*");
      console.log("🔍 Sent REQUEST_FALAI_KEY to parent");
    } catch (e) {
      console.error("🔍 Error requesting key from parent:", e);
    }
  });
}

export const fal = createFalClient({
  // Use the official fal.ai configuration pattern
  proxyUrl: "/api/fal", // This is our custom proxy endpoint

  // The credentials function is called when the client needs to authenticate
  credentials: () => {
    // Safe check for browser environment
    if (typeof window === "undefined") {
      console.log(
        "🔍 FAL CLIENT: Running on server, returning empty credentials",
      );
      return ""; // Empty string on server-side
    }

    // Try to get the key from localStorage
    const apiKey = localStorage?.getItem("falai_key") || "";

    // Log the attempt
    console.log(
      "🔍 FAL CLIENT: API key from localStorage:",
      apiKey
        ? "Found (starts with " + apiKey.substring(0, 5) + "...)"
        : "NOT FOUND",
    );

    // If no key found, request it from the parent
    if (!apiKey) {
      console.log(
        "🔍 FAL CLIENT: No key found in localStorage, requesting from parent",
      );
      try {
        // Send a message to the parent window requesting the key
        window.parent.postMessage({ type: "REQUEST_FALAI_KEY" }, "*");
        console.log("🔍 FAL CLIENT: Sent REQUEST_FALAI_KEY to parent");
      } catch (e) {
        console.error("🔍 FAL CLIENT: Error requesting key from parent:", e);
      }

      // Warning if key is missing
      console.warn(
        "❌ FAL CLIENT credentials: No Fal.ai API key found in localStorage.",
      );
    } else {
      console.log("🔍 FAL CLIENT: Using key from localStorage");
    }

    return apiKey;
  },

  // This middleware is called before each request to allow customizing the request
  requestMiddleware: async (request) => {
    console.log("🔍 FAL CLIENT: Request middleware executed");

    // Log request details for debugging
    const targetUrl = request.url;
    console.log("🔍 FAL CLIENT middleware: Target URL:", targetUrl);
    console.log("🔍 FAL CLIENT middleware: Request method:", request.method);

    // Try to get the key from localStorage
    let apiKey = localStorage?.getItem("falai_key") || "";
    console.log(
      "🔍 FAL CLIENT middleware: API key from localStorage (initial attempt):",
      apiKey
        ? "Found (starts with " + apiKey.substring(0, 5) + "...)"
        : "NOT FOUND",
    );

    // If no key found, wait for it to be set
    if (!apiKey) {
      console.log(
        "🔍 FAL CLIENT middleware: No key found in localStorage, waiting for it to be set",
      );
      try {
        // Wait for the key to be set in localStorage
        apiKey = await waitForFalaiKey(3000); // Wait up to 3 seconds

        if (apiKey) {
          console.log(
            "🔍 FAL CLIENT middleware: Key found after waiting:",
            apiKey.substring(0, 5) + "...",
          );
        } else {
          console.warn("🔍 FAL CLIENT middleware: No key found after waiting");
        }
      } catch (e) {
        console.error("🔍 FAL CLIENT middleware: Error waiting for key:", e);
      }
    }

    // Add the Authorization header with the API key
    if (apiKey) {
      request.headers = {
        ...request.headers,
        Authorization: `Key ${apiKey}`,
        // IMPORTANT: Add the x-fal-target-url header for the proxy to know where to forward the request
        "x-fal-target-url": targetUrl,
      };
      console.log(
        "🔍 FAL CLIENT middleware: Added Authorization header and x-fal-target-url header",
      );
      console.log(
        "🔍 FAL CLIENT middleware: Final request headers:",
        JSON.stringify(request.headers, null, 2),
      );
    } else {
      console.error(
        "❌ FAL CLIENT middleware: No API key available to add to request",
      );
      // Try one last desperate attempt to get the key directly from localStorage
      const emergencyKey = localStorage?.getItem("falai_key");
      if (emergencyKey) {
        console.log(
          "🔍 FAL CLIENT middleware: EMERGENCY RECOVERY - Found key in localStorage",
        );
        request.headers = {
          ...request.headers,
          Authorization: `Key ${emergencyKey}`,
          "x-fal-target-url": targetUrl,
        };
        console.log(
          "🔍 FAL CLIENT middleware: EMERGENCY RECOVERY - Added Authorization header",
        );
      }
    }

    return request;
  },
});

export type InputAsset =
  | "video"
  | "image"
  | "audio"
  | {
      type: "video" | "image" | "audio";
      key: string;
    };

export type ApiInfo = {
  endpointId: string;
  label: string;
  description: string;
  cost: string;
  inferenceTime?: string;
  inputMap?: Record<string, string>;
  inputAsset?: InputAsset[];
  initialInput?: Record<string, unknown>;
  cameraControl?: boolean;
  imageForFrame?: boolean;
  category: "image" | "video" | "music" | "voiceover";
};

export const AVAILABLE_ENDPOINTS: ApiInfo[] = [
  {
    endpointId: "fal-ai/flux/dev",
    label: "Flux Dev",
    description: "Generate a video from a text prompt",
    cost: "",
    category: "image",
  },
  {
    endpointId: "fal-ai/flux/schnell",
    label: "Flux Schnell",
    description: "Generate a video from a text prompt",
    cost: "",
    category: "image",
  },
  {
    endpointId: "fal-ai/flux-pro/v1.1-ultra",
    label: "Flux Pro 1.1 Ultra",
    description: "Generate a video from a text prompt",
    cost: "",
    category: "image",
  },
  {
    endpointId: "fal-ai/stable-diffusion-v35-large",
    label: "Stable Diffusion 3.5 Large",
    description: "Image quality, typography, complex prompt understanding",
    cost: "",
    category: "image",
  },
  {
    endpointId: "fal-ai/minimax/video-01-live",
    label: "Minimax Video 01 Live",
    description: "High quality video, realistic motion and physics",
    cost: "",
    category: "video",
    inputAsset: ["image"],
  },
  {
    endpointId: "fal-ai/hunyuan-video",
    label: "Hunyuan",
    description: "High visual quality, motion diversity and text alignment",
    cost: "",
    category: "video",
  },
  {
    endpointId: "fal-ai/kling-video/v1.5/pro",
    label: "Kling 1.5 Pro",
    description: "High quality video",
    cost: "",
    category: "video",
    inputAsset: ["image"],
  },
  {
    endpointId: "fal-ai/kling-video/v1/standard/text-to-video",
    label: "Kling 1.0 Standard",
    description: "High quality video",
    cost: "",
    category: "video",
    inputAsset: [],
    cameraControl: true,
  },
  {
    endpointId: "fal-ai/luma-dream-machine",
    label: "Luma Dream Machine 1.5",
    description: "High quality video",
    cost: "",
    category: "video",
    inputAsset: ["image"],
  },
  {
    endpointId: "fal-ai/minimax-music",
    label: "Minimax Music",
    description:
      "Advanced AI techniques to create high-quality, diverse musical compositions",
    cost: "",
    category: "music",
    inputAsset: [
      {
        type: "audio",
        key: "reference_audio_url",
      },
    ],
  },
  {
    endpointId: "fal-ai/mmaudio-v2",
    label: "MMAudio V2",
    description:
      "MMAudio generates synchronized audio given video and/or text inputs. It can be combined with video models to get videos with audio.",
    cost: "",
    inputAsset: ["video"],
    category: "video",
  },
  {
    endpointId: "fal-ai/sync-lipsync",
    label: "sync.so -- lipsync 1.8.0",
    description:
      "Generate realistic lipsync animations from audio using advanced algorithms for high-quality synchronization.",
    cost: "",
    inputAsset: ["video", "audio"],
    category: "video",
  },
  {
    endpointId: "fal-ai/stable-audio",
    label: "Stable Audio",
    description: "Stable Diffusion music creation with high-quality tracks",
    cost: "",
    category: "music",
  },
  {
    endpointId: "fal-ai/playht/tts/v3",
    label: "PlayHT TTS v3",
    description: "Fluent and faithful speech with flow matching",
    cost: "",
    category: "voiceover",
    initialInput: {
      voice: "Dexter (English (US)/American)",
    },
  },
  {
    endpointId: "fal-ai/playai/tts/dialog",
    label: "PlayAI Text-to-Speech Dialog",
    description:
      "Generate natural-sounding multi-speaker dialogues. Perfect for expressive outputs, storytelling, games, animations, and interactive media.",
    cost: "",
    category: "voiceover",
    inputMap: {
      prompt: "input",
    },
    initialInput: {
      voices: [
        {
          voice: "Jennifer (English (US)/American)",
          turn_prefix: "Speaker 1: ",
        },
        {
          voice: "Furio (English (IT)/Italian)",
          turn_prefix: "Speaker 2: ",
        },
      ],
    },
  },
  {
    endpointId: "fal-ai/f5-tts",
    label: "F5 TTS",
    description: "Fluent and faithful speech with flow matching",
    cost: "",
    category: "voiceover",
    initialInput: {
      ref_audio_url:
        "https://github.com/SWivid/F5-TTS/raw/21900ba97d5020a5a70bcc9a0575dc7dec5021cb/tests/ref_audio/test_en_1_ref_short.wav",
      ref_text: "Some call me nature, others call me mother nature.",
      model_type: "F5-TTS",
      remove_silence: true,
    },
  },
  {
    endpointId: "fal-ai/veo2",
    label: "Veo 2",
    description:
      "Veo creates videos with realistic motion and high quality output, up to 4K.",
    cost: "",
    category: "video",
  },
  {
    endpointId: "fal-ai/ltx-video-v095/multiconditioning",
    label: "LTX Video v0.95 Multiconditioning",
    description: "Generate videos from prompts, images using LTX Video-0.9.5",
    cost: "",
    imageForFrame: true,
    category: "video",
  },
];

================
File: src/lib/ffmpeg.ts
================
import type { MediaItem } from "@/data/schema";
import { fal } from "./fal";
import { resolveMediaUrl } from "./utils";

export async function getMediaMetadata(media: MediaItem) {
  try {
    const { data: mediaMetadata } = await fal.subscribe(
      "fal-ai/ffmpeg-api/metadata",
      {
        input: {
          media_url: resolveMediaUrl(media),
          extract_frames: true,
        },
        mode: "streaming",
      },
    );

    return mediaMetadata;
  } catch (error) {
    console.error(error);
    return {};
  }
}

================
File: src/lib/project.ts
================
import { VideoProject } from "@/data/schema";
import { fal } from "./fal";
import { extractJson } from "./utils";

const SYSTEM_PROMPT = `
You're a video editor assistant. You will receive a request to create a new short video project.
You need to provide a short title (2-5 words) and a brief description (2-3 sentences) for the project.
This description will help the creator to understand the context and general direction of the video.

## Output example:

\`\`\`json
{
  "title": "Summer Memories",
  "description": "A of clips of a summer vacation, featuring beach, sunsets, beautiful blue ocean waters."
}
\`\`\`

## Important guidelines:

1. The description should be creative and engaging, come up with cool ideas that could fit a 10-30s video.
2. Think of different situations, like product advertisement, casual videos, travel vlog, movie teaser, etc.
3. Last but not least, **always** return the result in JSON format with the keys "title" and "description".
**Do not add any extra content and/or explanation, return plain JSON**.

`;

type ProjectSuggestion = {
  title: string;
  description: string;
};

export async function createProjectSuggestion() {
  const { data } = await fal.subscribe("fal-ai/any-llm", {
    input: {
      system_prompt: SYSTEM_PROMPT,
      prompt: "Create a short video project with a title and description.",
      model: "meta-llama/llama-3.2-1b-instruct",
    },
  });

  return extractJson<ProjectSuggestion>(data.output);
}

================
File: src/lib/prompt.ts
================
import type { VideoProject } from "@/data/schema";
import { fal } from "./fal";

// Define the allowed LLM model types based on the Fal.ai API schema
export type LlmModelType =
  | "anthropic/claude-3.5-sonnet"
  | "anthropic/claude-3-5-haiku"
  | "anthropic/claude-3-haiku"
  | "google/gemini-pro-1.5"
  | "google/gemini-flash-1.5"
  | "google/gemini-flash-1.5-8b"
  | "meta-llama/llama-3.2-1b-instruct"
  | "meta-llama/llama-3.2-3b-instruct"
  | "meta-llama/llama-3.1-8b-instruct"
  | "meta-llama/llama-3.1-70b-instruct"
  | "openai/gpt-4o-mini"
  | "openai/gpt-4o";

type EnhancePromptOptions = {
  type: "image" | "video" | "music" | "voiceover";
  project?: VideoProject;
  model?: LlmModelType; // Use the defined type
};

const SYSTEM_PROMPT = `
You're a video editor assistant. You will receive instruction to enhance the description of
images, audio-clips and video-clips in a video project. You will be given the name of project
and a brief description. Use that contextual information to come up with created and well-formed
description for the media assets. The description should be creative and engaging.

Important guidelines:

1. The description should be creative and engaging.
2. It should be concise, don't exceed 2-3 sentences.
3. The description should be relevant to the project.
4. The description should be well-formed and grammatically correct.
5. Last but not least, **always** return just the enhanced prompt, don't add
any extra content and/or explanation. **DO NOT ADD markdown** or quotes, return the
**PLAIN STRING**.
`;

export async function enhancePrompt(
  prompt: string,
  options: EnhancePromptOptions = { type: "video" },
) {
  console.log("🔍 Starting enhancePrompt with:", { prompt, options });

  const { type, project, model = "meta-llama/llama-3.2-1b-instruct" } = options;
  const projectInfo = !project
    ? ""
    : `
    ## Project Info

    Title: ${project.title}
    Description: ${project.description}
  `.trim();
  const promptInfo = !prompt.trim() ? "" : `User prompt: ${prompt}`;

  // Check if API key exists before making the request
  const apiKey =
    typeof window !== "undefined"
      ? window.localStorage?.getItem("falai_key") || ""
      : "";
  console.log(
    "🔍 Using API key from localStorage:",
    apiKey
      ? "Found (starts with " + apiKey.substring(0, 5) + "...)"
      : "NOT FOUND",
  );

  try {
    console.log("🔍 Making request to Fal.ai using fal.subscribe...");

    const { data } = await fal.subscribe("fal-ai/any-llm", {
      input: {
        system_prompt: SYSTEM_PROMPT,
        prompt: `
          Create a prompt for generating a ${type} via AI inference. Here's the context:
          ${projectInfo}
          ${promptInfo}
        `.trim(),
        model,
      },
    });

    console.log("🔍 Received response from Fal.ai:", data);
    return data.output.replace(/^"|"$/g, "");
  } catch (error) {
    console.error("❌ enhancePrompt error:", error);
    throw error;
  }
}

================
File: src/lib/share.ts
================
import { kv } from "@vercel/kv";
import { customAlphabet } from "nanoid";

export const IS_SHARE_ENABLED = !!process.env.KV_REST_API_TOKEN;

export interface ShareVideoParams {
  title: string;
  description?: string;
  videoUrl: string;
  thumbnailUrl: string;
  createdAt: number;
  width: number;
  height: number;
}

const nanoid = customAlphabet("1234567890abcdefghijklmnopqrstuvwxyz", 10);

// Six months in seconds
const SIX_MONTHS = 60 * 60 * 24 * 30 * 6;

export async function shareVideo(params: ShareVideoParams) {
  const id = nanoid();
  await kv.set(
    `share:${id}`,
    {
      id,
      ...params,
      createdAt: Date.now(),
    },
    {
      ex: SIX_MONTHS,
    },
  );
  return id;
}

export async function fetchSharedVideo(id: string) {
  return kv.get<ShareVideoParams>(`share:${id}`);
}

================
File: src/lib/uploadthing.ts
================
import {
  generateReactHelpers,
  generateUploadDropzone,
} from "@uploadthing/react";

import type { OurFileRouter } from "@/app/api/uploadthing/core";

export const UploadDropzone = generateUploadDropzone<OurFileRouter>();
export const { useUploadThing, uploadFiles } =
  generateReactHelpers<OurFileRouter>();

================
File: src/lib/utils.ts
================
import type { MediaItem, VideoTrack } from "@/data/schema";
import { GenerateData, LAST_PROJECT_ID_KEY } from "@/data/store";
import { type ClassValue, clsx } from "clsx";
import { ImageIcon, MicIcon, MusicIcon, VideoIcon } from "lucide-react";
import type { FunctionComponent } from "react";
import { twMerge } from "tailwind-merge";
import type { InputAsset } from "./fal";

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs));
}

export function extractJson<T>(raw: string): T {
  const json = raw.replace("```json", "").replace("```", "");
  return JSON.parse(json);
}

export function rememberLastProjectId(projectId: string) {
  if (typeof window !== "undefined" && typeof document !== "undefined") {
    document.cookie = `${LAST_PROJECT_ID_KEY}=${projectId}; max-age=31536000; path=/`;
  }
}

export function mapInputKey(
  input: Record<string, unknown>,
  inputMap: Record<string, string>,
): Record<string, unknown> {
  if (typeof input !== "object" || input === null) return input;
  const newInput: Record<string, unknown> = {};

  for (const [key, value] of Object.entries(input)) {
    const newKey = inputMap[key] || key;
    if (!(newKey in newInput)) {
      newInput[newKey] = value;
    }
  }

  return newInput;
}

export const trackIcons: Record<
  VideoTrack["type"] | "image",
  FunctionComponent
> = {
  video: VideoIcon,
  music: MusicIcon,
  voiceover: MicIcon,
  image: ImageIcon,
};

export function resolveDuration(item: MediaItem): number | null {
  if (!item) return null;

  const metadata = item.metadata;
  if (
    metadata &&
    "duration" in metadata &&
    typeof metadata.duration === "number"
  ) {
    return metadata.duration * 1000;
  }

  const data = item.output;
  if (!data) return null;
  if ("seconds_total" in data) {
    return data.seconds_total * 1000;
  }
  if ("audio" in data && "duration" in data.audio) {
    return data.audio.duration * 1000;
  }
  return null;
}

/**
 * Depending on the output type of the job, the URL of the audio/image/video
 * might be represented by different properties. This utility function resolves
 * the URL of the media based on the output data.
 */
export function resolveMediaUrl(item: MediaItem | undefined): string | null {
  if (!item) return null;

  if (item.kind === "uploaded") {
    return item.url;
  }
  const data = item.output;
  if (!data) return null;
  if (
    "images" in data &&
    Array.isArray(data.images) &&
    data.images.length > 0
  ) {
    return data.images[0].url;
  }
  const fileProperties = {
    image: 1,
    video: 1,
    audio: 1,
    audio_file: 1,
    audio_url: 1,
  };
  const property = Object.keys(data).find(
    (key) => key in fileProperties && "url" in data[key],
  );
  if (property) {
    return data[property].url;
  }
  return null;
}

export function getAssetType(asset: InputAsset): "image" | "video" | "audio" {
  return typeof asset === "string" ? asset : asset.type;
}

export const assetKeyMap: Record<"image" | "video" | "audio", string> = {
  image: "image",
  video: "video_url",
  audio: "audio_url",
};

export function getAssetKey(asset: InputAsset): string {
  return typeof asset === "string"
    ? assetKeyMap[asset]
    : asset.key || assetKeyMap[asset.type];
}

================
File: src/styles/main-app-theme.css
================
/* Main App Theme Integration */

:root {
  /* Import main app variables - softer light tone matching homepage */
  --bg-light: #f4f4f6;
  --text-light: #2d3748;
  --glass-bg-light: rgba(255, 255, 255, 0.7);
  --glass-border-light: rgba(255, 255, 255, 0.2);

  /* Dark Mode Colors - matches homepage */
  --bg-dark: #0a1128;
  --text-dark: #e0e0e0;
  --glass-bg-dark: rgba(26, 26, 26, 0.5); /* Slightly more transparent for better readability */
  --glass-border-dark: rgba(255, 255, 255, 0.1);

  /* Common Variables */
  --font-family: "Poppins", sans-serif;
  --transition-speed: 0.3s;
  --radius-sm: 5px;
  --radius-md: 10px;
  --radius-lg: 15px;
}

/* Override Tailwind variables in light mode */
:root {
  --background: 220 20% 97%;
  --foreground: 220 20% 10%;
  --card: 0 0% 98%;
  --card-foreground: 220 20% 10%;
  --popover: 0 0% 98%;
  --popover-foreground: 220 20% 10%;
  --primary: 220 85% 57%;
  --primary-foreground: 0 0% 98%;
  --secondary: 220 20% 96.1%;
  --secondary-foreground: 220 20% 9%;
  --muted: 220 20% 96.1%;
  --muted-foreground: 220 20% 45%;
  --accent: 220 20% 96.1%;
  --accent-foreground: 220 20% 9%;
  --border: 220 20% 89.8%;

  /* Background gradient - softer light tone matching homepage */
  --tw-gradient-from: #f4f4f6;
  --tw-gradient-to: #e8e8ec;
}

/* Override Tailwind variables in dark mode */
.dark {
  --background: 222 47% 11%;
  --foreground: 0 0% 88%;
  --card: 222 47% 11%;
  --card-foreground: 0 0% 88%;
  --popover: 222 47% 11%;
  --popover-foreground: 0 0% 88%;
  --primary: 217.2 91.2% 59.8%;
  --primary-foreground: 222.2 47.4% 11.2%;
  --secondary: 217.2 32.6% 17.5%;
  --secondary-foreground: 210 40% 98%;
  --muted: 217.2 32.6% 17.5%;
  --muted-foreground: 215 20.2% 65.1%;
  --accent: 217.2 32.6% 17.5%;
  --accent-foreground: 210 40% 98%;
  --border: 217.2 32.6% 17.5%;

  /* Background gradient for dark mode - matches homepage */
  --tw-gradient-from: #0a1128;
  --tw-gradient-via: #1b3a6b;
  --tw-gradient-to: #16213e;
}

/* Body styles */
body {
  font-family: var(--font-family);
  background: linear-gradient(
    to bottom right,
    var(--tw-gradient-from),
    var(--tw-gradient-via),
    var(--tw-gradient-to)
  );
  min-height: 100vh;
  transition: background-color var(--transition-speed), color
    var(--transition-speed);
}

/* Glassmorphism effects - match main app - ONLY for left panel */
.glassmorphism {
  background: rgba(255, 255, 255, 0.15);
  backdrop-filter: blur(12px);
  border-radius: var(--radius-lg);
  border: 1px solid rgba(255, 255, 255, 0.2);
  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
  transition: all 0.3s ease-in-out;
}

.dark .glassmorphism {
  background: linear-gradient(
    to bottom right,
    rgba(10, 17, 40, 0.7),
    rgba(27, 58, 107, 0.7),
    rgba(22, 33, 62, 0.7)
  );
  border-color: rgba(255, 255, 255, 0.05);
  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
  backdrop-filter: blur(10px);
}

/* Button hover states */
button:hover,
.btn:hover {
  background: linear-gradient(145deg, #2563eb, #1d4ed8);
  color: white;
}

/* Scrollbar styles */
::-webkit-scrollbar {
  height: 0.25rem;
  width: 0.25rem;
}

::-webkit-scrollbar-track {
  background-color: rgba(38, 38, 38, 0.2);
}

::-webkit-scrollbar-thumb {
  background-color: rgba(64, 64, 64, 0.5);
  border-radius: 0.25rem;
}

::-webkit-scrollbar-thumb:hover {
  background-color: rgba(82, 82, 82, 0.8);
}

/* Input fields */
input,
textarea,
select {
  background-color: var(--glass-bg-light);
  border: 1px solid var(--glass-border-light);
  border-radius: var(--radius-sm);
  color: var(--text-light);
  transition: all var(--transition-speed) ease;
}

.dark input,
.dark textarea,
.dark select {
  background-color: var(--glass-bg-dark);
  border-color: var(--glass-border-dark);
  color: var(--text-dark);
}

input:focus,
textarea:focus,
select:focus {
  border-color: hsl(var(--primary));
  outline: none;
}

/* Card styles */
.card {
  background: var(--glass-bg-light);
  border-radius: var(--radius-md);
  border: 1px solid var(--glass-border-light);
  transition: all var(--transition-speed) ease;
}

.dark .card {
  background: linear-gradient(
    to bottom right,
    rgba(10, 17, 40, 0.7),
    rgba(27, 58, 107, 0.7),
    rgba(22, 33, 62, 0.7)
  );
  border-color: var(--glass-border-dark);
}

================
File: src/styles/right-panel.css
================
/* Right Panel Layout Styles */

/* Set width for all screen sizes */
.right-panel-container {
  width: 560px !important;
  min-width: 560px !important;
}

/* Apply main app theme to right panel */
.right-panel-container {
  background: #f4f4f6 !important; /* Opaque light background */
  border-color: var(--glass-border-light) !important;
  transition: transform 0.3s ease-in-out, opacity 0.3s ease-in-out;
}

.dark .right-panel-container {
  background: #0a1128 !important; /* Opaque dark background matching homepage */
  border-color: var(--glass-border-dark) !important;
}

/* Two-column layout container */
.two-column-container {
  display: flex;
  gap: 1rem;
  margin-bottom: 1rem;
  min-height: 300px;
}

/* Left column for model selectors */
.model-selectors-column {
  flex: 0 0 48%;
}

/* Right column for helper */
.helper-column {
  flex: 0 0 48%;
  display: flex;
  flex-direction: column;
  height: 100%;
  overflow-y: auto;
  max-height: 400px;
}

/* Floating Media Button */
.floating-media-button {
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
  animation: fadeIn 0.3s ease-in-out;
}

.dark .floating-media-button {
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
}

@keyframes fadeIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

================
File: .editorconfig
================
# Editor configuration, see http://editorconfig.org
root = true

[*]
charset = utf-8
indent_style = space
indent_size = 2
insert_final_newline = true
trim_trailing_whitespace = true

[*.md]
max_line_length = off
trim_trailing_whitespace = false

================
File: .env.example copy
================
# https://fal.ai/dashboard/keys
FAL_KEY="5eda4036-c86a-4637-873b-5f79ea72d588:b58feab09b3bd2bf165e5ba2ff7be918"

# https://uploadthing.com required if you want to use file upload
UPLOADTHING_TOKEN=""

# https://upstash.com required if you want to the share button
KV_URL=""
KV_REST_API_READ_ONLY_TOKEN=""
KV_REST_API_TOKEN=""
KV_REST_API_URL=""

================
File: .gitignore
================
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.js
.yarn/install-state.gz

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# local env files
.env*.local
.env

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

================
File: .nvmrc
================
18.17.0

================
File: biome.json
================
{
  "$schema": "https://biomejs.dev/schemas/1.9.4/schema.json",
  "vcs": {
    "enabled": false,
    "clientKind": "git",
    "useIgnoreFile": false
  },
  "files": {
    "ignoreUnknown": false,
    "ignore": ["node_modules", ".next"]
  },
  "formatter": {
    "enabled": true,
    "indentStyle": "space",
    "indentWidth": 2
  },
  "organizeImports": {
    "enabled": true
  },
  "linter": {
    "enabled": true,
    "rules": {
      "recommended": true
    }
  },
  "javascript": {
    "formatter": {
      "quoteStyle": "double"
    }
  }
}

================
File: CODE_OF_CONDUCT.md
================
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, caste, color, religion, or sexual
identity and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

- Demonstrating empathy and kindness toward other people
- Being respectful of differing opinions, viewpoints, and experiences
- Giving and gracefully accepting constructive feedback
- Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
- Focusing on what is best not just for us as individuals, but for the overall
  community

Examples of unacceptable behavior include:

- The use of sexualized language or imagery, and sexual attention or advances of
  any kind
- Trolling, insulting or derogatory comments, and personal or political attacks
- Public or private harassment
- Publishing others' private information, such as a physical or email address,
  without their explicit permission
- Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
[INSERT CONTACT METHOD].
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.1, available at
[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].

Community Impact Guidelines were inspired by
[Mozilla's code of conduct enforcement ladder][Mozilla CoC].

For answers to common questions about this code of conduct, see the FAQ at
[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at
[https://www.contributor-covenant.org/translations][translations].

[homepage]: https://www.contributor-covenant.org
[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html
[Mozilla CoC]: https://github.com/mozilla/diversity
[FAQ]: https://www.contributor-covenant.org/faq
[translations]: https://www.contributor-covenant.org/translations

================
File: components.json
================
{
  "$schema": "https://ui.shadcn.com/schema.json",
  "style": "new-york",
  "rsc": true,
  "tsx": true,
  "tailwind": {
    "config": "tailwind.config.ts",
    "css": "src/app/globals.css",
    "baseColor": "neutral",
    "cssVariables": true,
    "prefix": ""
  },
  "aliases": {
    "components": "@/components",
    "utils": "@/lib/utils",
    "ui": "@/components/ui",
    "lib": "@/lib",
    "hooks": "@/hooks"
  },
  "iconLibrary": "lucide"
}

================
File: CONTRIBUTING.md
================
# Contributing

Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are **greatly appreciated**.

1. Make sure you read our [Code of Conduct](https://github.com/fal-ai-community/video-starter-kit/blob/main/CODE_OF_CONDUCT.md)
2. Fork the project and clone your fork
3. Setup the local environment with `npm install`
4. Create a feature branch (`git checkout -b feature/add-cool-thing`) or a bugfix branch (`git checkout -b fix/smash-that-bug`)
5. Commit the changes (`git commit -m 'feat(client): added a cool thing'`) - use [conventional commits](https://conventionalcommits.org)
6. Push to the branch (`git push --set-upstream origin feature/add-cool-thing`)
7. Open a Pull Request

Check the [good first issue queue](https://github.com/fal-ai-community/video-starter-kit/labels/good+first+issue), your contribution will be welcome!

## License

Distributed under the MIT License. See [LICENSE](https://github.com/fal-ai-community/video-starter-kit/blob/main/LICENSE) for more information.

================
File: LICENSE
================
Copyright 2024 https://fal.ai

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

================
File: next.config.mjs
================
/** @type {import('next').NextConfig} */
const nextConfig = {
  async headers() {
    return [
      {
        // Apply these headers to all routes in the SDK application.
        source: "/(.*)",
        headers: [
          {
            key: "Content-Security-Policy",
            // Allow embedding from self, localhost, staging (mecai.app), and production (novelvisionai.art)
            value:
              "frame-ancestors 'self' http://localhost:8000 https://mecai.app https://novelvisionai.art;",
          },
        ],
      },
    ];
  },
};

export default nextConfig;

================
File: nvmrc
================
18.17.0

================
File: package.json
================
{
  "name": "fal-video-studio",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint",
    "format": "biome format --write",
    "prepare": "husky"
  },
  "dependencies": {
    "@fal-ai/client": "^1.2.1",
    "@fal-ai/server-proxy": "^1.1.1",
    "@hookform/resolvers": "^3.9.1",
    "@radix-ui/react-accordion": "^1.2.1",
    "@radix-ui/react-collapsible": "^1.1.1",
    "@radix-ui/react-dialog": "^1.1.4",
    "@radix-ui/react-dropdown-menu": "^2.1.2",
    "@radix-ui/react-label": "^2.1.1",
    "@radix-ui/react-popover": "^1.1.3",
    "@radix-ui/react-select": "^2.1.3",
    "@radix-ui/react-separator": "^1.1.0",
    "@radix-ui/react-slider": "^1.2.3",
    "@radix-ui/react-slot": "^1.1.0",
    "@radix-ui/react-tabs": "^1.1.1",
    "@radix-ui/react-toast": "^1.2.4",
    "@radix-ui/react-toggle": "^1.1.1",
    "@radix-ui/react-toggle-group": "^1.1.1",
    "@radix-ui/react-tooltip": "^1.1.5",
    "@remotion/media-utils": "4.0.249",
    "@remotion/player": "4.0.249",
    "@remotion/preload": "4.0.249",
    "@tanstack/react-query": "^5.62.7",
    "@uploadthing/react": "^7.1.5",
    "@vercel/analytics": "^1.4.1",
    "@vercel/kv": "^3.0.0",
    "class-variance-authority": "^0.7.1",
    "clsx": "^2.1.1",
    "cmdk": "^1.0.0",
    "date-fns": "^4.1.0",
    "framer-motion": "^12.5.0",
    "geist": "^1.3.1",
    "idb": "^8.0.0",
    "lucide-react": "^0.468.0",
    "nanoid": "^5.0.9",
    "next": "^14.2.23",
    "next-themes": "^0.4.4",
    "react": "^18",
    "react-dom": "^18",
    "react-hook-form": "^7.54.0",
    "react-hotkeys-hook": "^4.6.1",
    "remotion": "4.0.249",
    "tailwind-merge": "^2.5.5",
    "tailwindcss-animate": "^1.0.7",
    "throttle-debounce": "^5.0.2",
    "uploadthing": "^7.4.4",
    "waveform-path": "^0.0.2",
    "zod": "^3.24.1",
    "zustand": "^5.0.2"
  },
  "devDependencies": {
    "@biomejs/biome": "1.9.4",
    "@remotion/cli": "^4.0.240",
    "@types/node": "^20",
    "@types/react": "^18",
    "@types/react-dom": "^18",
    "@types/throttle-debounce": "^5.0.2",
    "husky": "^9.1.7",
    "postcss": "^8",
    "tailwindcss": "^3.4.1",
    "typescript": "^5"
  }
}

================
File: postcss.config.mjs
================
/** @type {import('postcss-load-config').Config} */
const config = {
  plugins: {
    tailwindcss: {},
  },
};

export default config;

================
File: README.md
================
# AI Video Starter Kit

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![fal.ai](https://img.shields.io/badge/fal.ai-latest-purple)](https://fal.ai)
[![Next.js](https://img.shields.io/badge/Next.js-14-black)](https://nextjs.org)
[![Remotion](https://img.shields.io/badge/Remotion-latest-blue)](https://remotion.dev)

A powerful starter kit for building AI-powered video applications. Built with Next.js, Remotion, and fal.ai, this toolkit simplifies the complexities of working with AI video models in the browser.

![AI Video Starter Kit](https://github.com/fal-ai-community/video-starter-kit/blob/main/src/app/opengraph-image.png?raw=true)

## Features

- 🎬 **Browser-Native Video Processing**: Seamless video handling and composition in the browser
- 🤖 **AI Model Integration**: Direct access to state-of-the-art video models through fal.ai
  - Minimax for video generation
  - Hunyuan for visual synthesis
  - LTX for video manipulation
- 🎵 **Advanced Media Capabilities**:
  - Multi-clip video composition
  - Audio track integration
  - Voiceover support
  - Extended video duration handling
- 🛠️ **Developer Utilities**:
  - Metadata encoding
  - Video processing pipeline
  - Ready-to-use UI components
  - TypeScript support

## Tech Stack

- [fal.ai](https://fal.ai) - AI model infrastructure
- [Next.js](https://nextjs.org) - React framework
- [Remotion](https://remotion.dev) - Video processing
- [IndexedDB](https://developer.mozilla.org/docs/Web/API/IndexedDB_API) - Browser-based storage (no cloud database required)
- [Vercel](https://vercel.com) - Deployment platform
- [UploadThing](https://uploadthing.com) - File upload

## Quick Start

1. Clone the repository:

```bash
git clone https://github.com/fal-ai-community/video-starter-kit
cd video-starter-kit
```

2. Install dependencies:

```bash
npm install
# or
yarn install
# or
pnpm install
```

3. Set up your environment variables:

```bash
cp .env.example .env.local
```

4. Start the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
```

Open [http://localhost:3000](http://localhost:3000) to see the application.

## Contributing

We welcome contributions! See our [Contributing Guide](CONTRIBUTING.md) for more information.

## Community

- [Discord](https://discord.gg/fal-ai) - Join our community
- [GitHub Discussions](https://github.com/fal-ai-community/video-starter-kit/discussions) - For questions and discussions
- [Twitter](https://twitter.com/fal) - Follow us for updates

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Deployment

The easiest way to deploy your application is through [Vercel](https://vercel.com/new?utm_source=fal-ai&utm_medium=default-template&utm_campaign=video-starter-kit).

## Fal.ai Integration Guide

### Overview
This application integrates with Fal.ai API services for AI-powered video, image, and audio generation. The integration follows the official Fal.ai proxy pattern to securely handle API keys.

### How It Works

#### 1. Client-Side Configuration (`src/lib/fal.ts`)

The client is configured to:
- Retrieve the API key from localStorage (`falai_key`)
- Send API requests through our custom proxy endpoint
- Include the target URL in the `x-fal-target-url` header

```typescript
export const fal = createFalClient({
  proxyUrl: "/api/fal",
  credentials: () => {
    // Get API key from localStorage
    return typeof window !== "undefined" ? localStorage?.getItem("falai_key") || "" : "";
  },
  requestMiddleware: async (request) => {
    // Add the target URL header for the proxy
    const apiKey = typeof window !== "undefined" ? localStorage?.getItem("falai_key") || "" : "";
    if (apiKey) {
      request.headers = {
        ...request.headers,
        "Authorization": `Key ${apiKey}`,
        "x-fal-target-url": request.url
      };
    }
    return request;
  }
});
```

#### 2. Server-Side Proxy (`src/app/api/fal/route.ts`)

The proxy:
- Extracts the target URL from the `x-fal-target-url` header
- Forwards the request to Fal.ai with proper authentication
- Returns the response from Fal.ai to the client

```typescript
async function forwardToFal(req: NextRequest) {
  // Get the target URL from headers
  const targetUrl = req.headers.get("x-fal-target-url");
  
  // Forward the request with the API key
  const forwardedResponse = await fetch(targetUrl, {
    method: req.method,
    headers: {
      // Include the Authorization header
      "Authorization": authHeader,
      // Other headers...
    },
    body: requestBody
  });
  
  // Return the response to the client
  return new NextResponse(responseData, {
    status: forwardedResponse.status,
    headers: responseHeaders
  });
}
```

### Common Issues and Solutions

1. **400 Bad Request Error**:
   - Make sure the client is setting the `x-fal-target-url` header
   - Verify the URL format being passed to Fal.ai

2. **401 Unauthorized Error**:
   - Check that the API key is correctly stored in localStorage
   - Ensure the Authorization header is properly formatted as `Key YOUR_API_KEY`

3. **DNS Resolution Errors**:
   - Ensure your network can reach the Fal.ai servers
   - Check for any firewall or network restrictions

### Resources
- [Official Fal.ai Documentation](https://docs.fal.ai/clients/javascript/)
- [Fal.ai Next.js Integration Guide](https://docs.fal.ai/integrations/nextjs/)

================
File: tailwind.config.ts
================
import type { Config } from "tailwindcss";
import { withUt } from "uploadthing/tw";

const config: Config = withUt({
  darkMode: ["class"],
  content: [
    "./src/pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/components/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      colors: {
        background: {
          DEFAULT: "hsl(var(--background))",
          dark: "hsl(var(--background-dark))",
          light: "hsl(var(--background-light))",
        },
        foreground: "hsl(var(--foreground))",
        card: {
          DEFAULT: "hsl(var(--card))",
          foreground: "hsl(var(--card-foreground))",
        },
        popover: {
          DEFAULT: "hsl(var(--popover))",
          foreground: "hsl(var(--popover-foreground))",
        },
        primary: {
          DEFAULT: "hsl(var(--primary))",
          foreground: "hsl(var(--primary-foreground))",
        },
        secondary: {
          DEFAULT: "hsl(var(--secondary))",
          foreground: "hsl(var(--secondary-foreground))",
        },
        muted: {
          DEFAULT: "hsl(var(--muted))",
          foreground: "hsl(var(--muted-foreground))",
        },
        accent: {
          DEFAULT: "hsl(var(--accent))",
          foreground: "hsl(var(--accent-foreground))",
        },
        destructive: {
          DEFAULT: "hsl(var(--destructive))",
          foreground: "hsl(var(--destructive-foreground))",
        },
        border: "hsl(var(--border))",
        input: "hsl(var(--input))",
        ring: "hsl(var(--ring))",
        chart: {
          "1": "hsl(var(--chart-1))",
          "2": "hsl(var(--chart-2))",
          "3": "hsl(var(--chart-3))",
          "4": "hsl(var(--chart-4))",
          "5": "hsl(var(--chart-5))",
        },
      },
      borderRadius: {
        lg: "var(--radius)",
        md: "calc(var(--radius) - 2px)",
        sm: "calc(var(--radius) - 4px)",
      },
      keyframes: {
        "accordion-down": {
          from: {
            height: "0",
          },
          to: {
            height: "var(--radix-accordion-content-height)",
          },
        },
        "accordion-up": {
          from: {
            height: "var(--radix-accordion-content-height)",
          },
          to: {
            height: "0",
          },
        },
      },
      animation: {
        "accordion-down": "accordion-down 0.2s ease-out",
        "accordion-up": "accordion-up 0.2s ease-out",
      },
    },
  },
  plugins: [require("tailwindcss-animate")],
});
export default config;

================
File: test-fal.js
================
/**
 * test-fal.js - Diagnostic script for testing Fal.ai API connectivity
 *
 * This script tests different approaches to making requests to the Fal.ai API
 * to diagnose production fetch failure issues.
 *
 * Usage: node test-fal.js
 */

// Fal.ai API key - using the known key prefix
const FAL_KEY =
  "5eda4036-c86a-4637-873b-5f79ea72d588:b58feab09b3bd2bf165e5ba2ff7be918";
const TARGET_URL = "https://queue.fal.run/fal-ai/any-llm";
const TEST_BODY = { prompt: "hello" };

// Import required modules
const axios = require("axios").default;
const { setTimeout } = require("timers/promises");

// Utility function to log request and response details
function logRequestDetails(testName, url, headers, body) {
  console.log(`\n========== TEST CASE: ${testName} ==========`);
  console.log(`Request URL: ${url}`);
  console.log("Headers sent:");
  console.log(JSON.stringify(headers, null, 2));
  console.log("Body sent:");
  console.log(JSON.stringify(body, null, 2));
}

// Utility function to log response details
function logResponseDetails(status, body) {
  console.log(`Response status: ${status}`);
  console.log("Response body:");
  console.log(typeof body === "string" ? body : JSON.stringify(body, null, 2));
  console.log("===========================================\n");
}

// Utility function to log errors
function logError(error) {
  console.error("Error occurred:");
  console.error(error);
  if (error.response) {
    console.error("Response status:", error.response.status);
    console.error("Response data:", error.response.data);
  }
  console.log("===========================================\n");
}

/**
 * TEST CASE A1: Exact replication of curl working request
 * - Method: POST
 * - Headers explicitly set
 * - Body JSON stringified
 */
async function testA1() {
  const headers = {
    Authorization: `Key ${FAL_KEY}`,
    "Content-Type": "application/json",
    "x-fal-target-url": TARGET_URL,
  };

  logRequestDetails(
    "A1 - Exact curl replication",
    TARGET_URL,
    headers,
    TEST_BODY,
  );

  try {
    const response = await fetch(TARGET_URL, {
      method: "POST",
      headers: headers,
      body: JSON.stringify(TEST_BODY),
    });

    const responseBody = await response.text();
    logResponseDetails(response.status, responseBody);
    return { success: response.ok, status: response.status };
  } catch (error) {
    logError(error);
    return { success: false, error };
  }
}

/**
 * TEST CASE A2: Use Node Headers() object
 * - Tests if Headers object construction affects request handling
 */
async function testA2() {
  const headers = new Headers();
  headers.append("Authorization", `Key ${FAL_KEY}`);
  headers.append("Content-Type", "application/json");
  headers.append("x-fal-target-url", TARGET_URL);

  logRequestDetails(
    "A2 - Using Headers() object",
    TARGET_URL,
    Object.fromEntries(headers.entries()),
    TEST_BODY,
  );

  try {
    const response = await fetch(TARGET_URL, {
      method: "POST",
      headers: headers,
      body: JSON.stringify(TEST_BODY),
    });

    const responseBody = await response.text();
    logResponseDetails(response.status, responseBody);
    return { success: response.ok, status: response.status };
  } catch (error) {
    logError(error);
    return { success: false, error };
  }
}

/**
 * TEST CASE A3: All headers lowercase
 * - Tests for case-sensitivity issues in header handling
 */
async function testA3() {
  const headers = {
    authorization: `Key ${FAL_KEY}`,
    "content-type": "application/json",
    "x-fal-target-url": TARGET_URL,
  };

  logRequestDetails("A3 - Lowercase headers", TARGET_URL, headers, TEST_BODY);

  try {
    const response = await fetch(TARGET_URL, {
      method: "POST",
      headers: headers,
      body: JSON.stringify(TEST_BODY),
    });

    const responseBody = await response.text();
    logResponseDetails(response.status, responseBody);
    return { success: response.ok, status: response.status };
  } catch (error) {
    logError(error);
    return { success: false, error };
  }
}

/**
 * TEST CASE A4: Use axios instead of fetch
 * - Tests if the issue is specific to fetch implementation
 */
async function testA4() {
  const headers = {
    Authorization: `Key ${FAL_KEY}`,
    "Content-Type": "application/json",
    "x-fal-target-url": TARGET_URL,
  };

  logRequestDetails("A4 - Using axios", TARGET_URL, headers, TEST_BODY);

  try {
    const response = await axios.post(TARGET_URL, TEST_BODY, { headers });
    logResponseDetails(response.status, response.data);
    return {
      success: response.status >= 200 && response.status < 300,
      status: response.status,
    };
  } catch (error) {
    logError(error);
    return { success: false, error };
  }
}

/**
 * TEST CASE A5: Raw fetch with simplified object construction
 * - No cloning or mutation of objects
 */
async function testA5() {
  logRequestDetails(
    "A5 - Simplified fetch",
    TARGET_URL,
    {
      Authorization: `Key ${FAL_KEY}`,
      "Content-Type": "application/json",
      "x-fal-target-url": TARGET_URL,
    },
    TEST_BODY,
  );

  try {
    // Direct fetch with minimal object construction
    const response = await fetch(TARGET_URL, {
      method: "POST",
      headers: {
        Authorization: `Key ${FAL_KEY}`,
        "Content-Type": "application/json",
        "x-fal-target-url": TARGET_URL,
      },
      body: JSON.stringify(TEST_BODY),
    });

    const responseBody = await response.text();
    logResponseDetails(response.status, responseBody);
    return { success: response.ok, status: response.status };
  } catch (error) {
    logError(error);
    return { success: false, error };
  }
}

// Run all tests sequentially with a delay between them
async function runAllTests() {
  console.log("Starting Fal.ai API connectivity tests...");

  // Run each test with a delay between them to avoid rate limiting
  await testA1();
  await setTimeout(1000);

  await testA2();
  await setTimeout(1000);

  await testA3();
  await setTimeout(1000);

  await testA4();
  await setTimeout(1000);

  await testA5();

  console.log(
    "\nAll tests completed. Check the logs to see which approach succeeded.",
  );
  console.log(
    "After Fabio confirms which test passes, we will port the exact successful structure back into the SDK proxy.",
  );
}

// Run the tests
runAllTests().catch((error) => {
  console.error("Error running tests:", error);
});

================
File: tsconfig.json
================
{
  "compilerOptions": {
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}



================================================================
End of Codebase
================================================================
