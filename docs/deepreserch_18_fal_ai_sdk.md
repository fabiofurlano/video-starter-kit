Understood! I will now conduct a deep research to gather model-specific prompting techniques, best practices, advanced strategies, and fine-tuning tips from official Fal AI documentation and other verified sources. I will also include pricing details only from the Fal AI API and create a structured table with all key insights.

I'll update you once the research is complete.

# Prompting Techniques & Best Practices for Fal AI Models

## Image Generation Models

### FLUX.1 [dev] – Text-to-Image
**About & Key Features:** FLUX.1 [dev] is a 12-billion-parameter “flow” transformer model for image generation ([Generating Images from Text | fal.ai Docs](https://docs.fal.ai/guides/generating-images-from-text/#:~:text=%2A%20fal,5%20Large%20is)). It produces high-quality images from text and supports both personal and commercial use ([FLUX.1 [dev] | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/flux/dev#:~:text=fal)). As a next-gen model, it emphasizes detailed, coherent outputs but isn’t as speed-optimized as newer variants.

**Prompting Best Practices:** Craft descriptive prompts that specify the subject, style, and context. FLUX [dev] responds well to **long-form prompts** with rich detail (e.g. *“Extreme close-up of a tiger’s eye with detailed iris and the word ‘FLUX’ painted over it in white brush strokes”* ([FLUX.1 [dev] | Text to Image | API Documentation | fal.ai](https://fal.ai/models/fal-ai/flux/dev/api#:~:text=const%20result%20%3D%20await%20fal.subscribe%28%22fal,%7D%2C%20logs%3A%20true))). You can use **weighted prompt segments** separated by pipes (`|`) and double-colon weight indicators (`::`) to emphasize certain aspects ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Image%3A%20portrait%20,s%201000)) ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=portrait%20,s%201000)). For example, you might structure a prompt into segments for subject, style, background, etc., with numeric weights to guide the model’s focus. Although FLUX models do not use a traditional diffusion “negative prompt,” you can still mitigate unwanted elements by explicitly phrasing what you want (e.g. “no text on background” or “blank background”). FLUX [dev] allows **high resolution outputs** (e.g. 1024×1024) and may require ~20–40 inference steps for best quality ([FLUX.1 [dev] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/dev/examples#:~:text=num_inference_steps%3A%2028)) ([FLUX.1 [dev] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/dev/examples#:~:text=num_inference_steps%3A%2040)). Enabling more steps (the default is often 28) tends to improve detail. 

**Advanced Tips:** Leverage FLUX’s related tools and fine-tuning options. For example, Fal provides **Flux Realism LoRA** add-ons for style fine-tuning ([FLUX.1 [dev] | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/flux/dev#:~:text=improves%20image%20quality%2C%20typography%2C%20prompt,res%20realism%20%2013)), and a **Portrait Trainer** for faces (which fine-tunes FLUX on portrait data) ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=The%20FLUX%20model%20series%20has,art%20inpainting%2Foutpainting%20%28%2011%20FLUX.1)). You can chain FLUX [dev] with other models: e.g. generate a base image with FLUX [dev], then refine it via **flux redux (image-to-image)** or use it as input to a video model. This model is flexible and handles complex, lengthy prompts better than most – take advantage of that improved prompt understanding by providing clear scene descriptions and even camera or lighting directions.

**Cost & Performance:** FLUX [dev] is billed per image (by megapixels). The Fal API pricing is approximately **$0.025 per 1 megapixel image** (about 2.5¢ for a 1024×1024 image) ([ZimmWriter Image API Integration | www.rankingtactics.com](https://www.rankingtactics.com/zimmwriter-image-api-integration/#:~:text=www,The%20main)). It’s a balanced choice for quality vs. speed: not as fast as flux’s optimized versions, but still reasonably quick on GPU. There are no strict usage limits beyond typical Fal rate limits, but each request queues and runs on Fal’s inference engine. For faster results, ensure your prompt isn’t requesting an extremely large resolution (1080p is supported, but higher resolutions will incur proportionally higher cost and latency). 

### FLUX.1 [schnell] – Fast Text-to-Image 
**About & Key Features:** FLUX.1 [schnell] is an optimized variant of FLUX (also 12B parameters) designed for speed ([Fastest FLUX Endpoint | fal.ai Docs](https://docs.fal.ai/fast-flux/#:~:text=fal,flux)). “Schnell” (German for “fast”) can generate images in as few as **1 to 4 diffusion steps** while preserving quality ([README.md · frankjoshua/FLUX.1-schnell at aa3b18d89cc7c592cc8291df10c9b5045cf3a5db](https://huggingface.co/frankjoshua/FLUX.1-schnell/blame/aa3b18d89cc7c592cc8291df10c9b5045cf3a5db/README.md#:~:text=1.%20Cutting,model%20can%20be%20used%20for)). It delivers similar output fidelity to FLUX [dev] but much faster – ideal for rapid iteration or high-throughput needs. 

**Prompting Best Practices:** FLUX [schnell] supports the **same prompting syntax** as FLUX [dev]. You can use detailed or short prompts, but because it excels with fewer steps, concise prompts often yield good results quickly. For complex scenes, however, don’t shy away from detail – the model can handle it. Use **weighted cues and style tags** as with FLUX [dev] (the examples in Fal’s UI for [schnell] show prompts split into segments like “portrait | style | background | parameters” with weights like `::8` ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Image%3A%20portrait%20,s%201000))). This helps structure your prompt and instruct the model which aspects to prioritize. **Keep prompts focused:** since [schnell] converges in very few steps, extremely long prompts might not always utilize every detail; it’s often effective to emphasize the key elements with higher weights. 

**Advanced Tips:** FLUX [schnell] is great for **prompt chaining** – you can generate a quick image, then feed that image (using the `image_url` input of the FLUX *redux* endpoint or an image-to-image model) along with a refined prompt to improve it further. This two-step approach (fast draft then refined pass) leverages [schnell]’s speed and [dev] or [pro]’s quality. Also consider using **upscaling or enhancement** on [schnell] outputs if you need larger images – Fal provides upscaler endpoints (like RealESRGAN-based upscalers). In practice, [schnell] is often used to **test different prompt ideas quickly**, so you can iterate over styles or compositions in seconds. Take advantage of that by trying multiple small variations of your prompt (changing a weight or adjective) to see differences in output.

**Cost & Performance:** FLUX [schnell] is extremely cost-efficient – roughly **$0.003 per image** (for ~1MP images) ([ZimmWriter Image API Integration | www.rankingtactics.com](https://www.rankingtactics.com/zimmwriter-image-api-integration/#:~:text=www,The%20main)), an order of magnitude cheaper than [dev] due to its low step count. This low cost makes it feasible to generate many images quickly. Despite its speed, it maintains “cutting-edge output quality” comparable to larger diffusion models ([README.md · frankjoshua/FLUX.1-schnell at aa3b18d89cc7c592cc8291df10c9b5045cf3a5db](https://huggingface.co/frankjoshua/FLUX.1-schnell/blame/aa3b18d89cc7c592cc8291df10c9b5045cf3a5db/README.md#:~:text=%23%20Key%20Features%201.%20Cutting,personal%2C%20scientific%2C%20and%20commercial%20purposes)). It’s the **fastest FLUX endpoint** on Fal; as Fal’s docs claim, it’s likely the fastest in the world for this model ([Fastest FLUX Endpoint | fal.ai Docs](https://docs.fal.ai/fast-flux/#:~:text=We%20believe%20fal%20has%20the,beat%20it%20within%20one%20week)). There are no special usage limits, but note that by default it produces up to 4 inference steps – if you increase the steps (you can request up to, say, 4 or slightly above), generation time and cost increase slightly. The default image size is often 768×768 or 512×512 for speed; larger sizes will still be faster than other models, but will proportionally cost more (since pricing is per megapixel). 

### FLUX.1 [pro] v1.1 “Ultra” – High-Resolution Pro Model 
**About & Key Features:** FLUX.1 [pro] v1.1 Ultra is the latest professional-grade FLUX model ([FLUX.1 [dev] | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/flux/dev#:~:text=improves%20image%20quality%2C%20typography%2C%20prompt,res%20realism%20%2013)). It maintains the image quality of the earlier pro model but pushes resolution up to **2K output** with improved photorealism ([FLUX.1 [dev] | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/flux/dev#:~:text=improves%20image%20quality%2C%20typography%2C%20prompt,res%20realism%20%2013)). In other words, it’s tuned for high-resolution, highly realistic images. It’s suitable when you need the best quality Fal’s FLUX lineup can offer (e.g. marketing images, prints, etc.). 

**Prompting Best Practices:** Use **specific, vivid prompts** to take advantage of FLUX [pro]’s capabilities. Because it’s geared toward photorealism, prompting with photography terms can help (camera model, lens, lighting conditions, etc.). For example, adding details like *“Canon EOS R5, 50mm f/1.4, golden-hour lighting”* – as seen in Fal’s example prompts – can yield extremely realistic results ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Image%3A%20Close,and%20altruism%20through%20scene%20details)) ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Background%20hints%20at%20charitable%20setting,and%20altruism%20through%20scene%20details)). [Pro] also benefits from **structured prompts**: clearly delineate the subject, setting, mood, and any text you want embedded. It handles text in images better than most (though results may vary), so you can experiment with including short words/logos in the prompt. Keep in mind it’s a large model; a too-brief prompt might result in a generic image. Provide enough context for it to work with, or even a reference image (via the image-to-image “redux” variant) if you want to guide composition or style. Negative prompting is not explicitly documented (similar to other FLUX models), but you can still specify undesired elements in the prompt (e.g. “**, no crowds, no text**”). 

**Advanced Tips:** **Leverage high resolution:** you can request outputs up to 2048px with [pro] Ultra. However, very large outputs might benefit from providing an initiating image or using **dual-pass generation** (generate a smaller image, then use Fal’s upscaler or image-to-image on [pro] to recreate it at larger size). This model also likely allows **fine-tuning via Fal’s serverless model system** if you had a specialized need (Fal supports custom model deployment, though that’s beyond the scope of this answer). In terms of prompt strategy, treat FLUX [pro] similar to how you’d treat Stable Diffusion XL: be precise and consider using **artist or style references** if you want a particular look (the model should understand many artist names or style descriptors). You can also utilize **Fal’s LoRA tools** with [pro] – e.g. apply a LoRA style by referencing it in the Fal API if such are available for FLUX [pro]. This can imbue a specific aesthetic or character that the base model might not natively have.

**Cost & Performance:** FLUX [pro] v1.1 Ultra is the most expensive of the FLUX family – approximately **$0.05 per 1MP image** (roughly 5¢ for 1024×1024) ([Flux 1.1 Pro Ultra Mode Is Here | undefined - Flux Labs AI](https://www.fluxlabs.ai/blog/flux-11-pro-ultra-mode-is-here#:~:text=Flux%201%20Pro%3A%20fixed%20price,or%20Freepik%20if%20you)) ([ZimmWriter Image API Integration | www.rankingtactics.com](https://www.rankingtactics.com/zimmwriter-image-api-integration/#:~:text=www,The%20main)). The higher cost reflects the larger model size and higher resolution processing. It’s still billed per megapixel output, so generating a full 2K image will cost more (e.g. a 2048×2048 output is ~4 MP, roughly $0.20). In terms of speed, it’s slower than [dev] or [schnell] due to the increased computation for fidelity. Expect a slightly longer queue and generation time for Ultra outputs. Fal’s infrastructure uses powerful GPUs (often H100s) to handle these – as a developer, just be aware that *bigger images = more time/credits*. There may be rate limits to prevent abuse (large batch requests etc.), but under normal use you can generate images freely. For large-scale usage, Fal’s enterprise plans allow scaling out the inference if needed.

### Stable Diffusion 3.5 Large – Text-to-Image (MMDiT model)
**About & Key Features:** Stable Diffusion 3.5 Large is Stability AI’s latest diffusion model (a **Multimodal Diffusion Transformer**) ([Generating Images from Text | fal.ai Docs](https://docs.fal.ai/guides/generating-images-from-text/#:~:text=%2A%20fal,efficiency)). It introduces improved image quality, better understanding of complex prompts, and even decent handling of text (typography) in images ([Generating Images from Text | fal.ai Docs](https://docs.fal.ai/guides/generating-images-from-text/#:~:text=%2A%20fal,efficiency)). Notably, it’s more resource-efficient than its predecessors, meaning it can generate higher quality without drastically higher compute. This model is state-of-the-art for general-purpose image generation, benefitting from the vast training data Stability AI provides. It supports modalities like text+image input (ControlNet, etc.), making it “multimodal.”

**Prompting Best Practices:** **Rich, descriptive prompts** are recommended to fully utilize SD 3.5’s capabilities. Because it can understand long, complex prompts well, you can describe a scene in depth – for example Fal’s demo prompt: *“A dreamlike Japanese garden in perpetual twilight, bathed in bioluminescent cherry blossoms… The scene is captured through a cinematic lens… 8K resolution, masterful photography, hyperdetailed, magical realism.”* ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Prompt)) ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=creating%20an%20otherworldly%20atmosphere,masterful%20photography%2C%20hyperdetailed%2C%20magical%20realism)). This model will parse a paragraph-long prompt and attempt to satisfy as many details as possible. It’s helpful to **organize your prompt**: start with the main subject and setting, then add style and technical details (lens, lighting, art style). **Negative prompting is supported** – in Fal’s interface “Additional Settings” you can provide a negative prompt to steer the model away from certain content (e.g. “blurry, out of frame, text”). Use this to avoid common issues. Also, Stable Diffusion 3.5 responds well to **artist/style cues** (“in the style of Studio Ghibli” or “rendered in Unreal Engine”), and to **camera/photography terminology** for realism. If generating text in the image (like a sign or logo), it’s still challenging, but 3.5 is better than 1.x at legible text – keep the text short and include descriptors like *“clean lettering”*. 

**Advanced Tips:** Take advantage of the **multimodal features**: Fal’s UI for SD 3.5 exposes **ControlNet** inputs (you can provide a control image and choose a control type like depth, canny edges, etc.) and allows adding **LoRA models** ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Loras)) ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Image%20Encoder%20Path)). Advanced users can provide a sketch or pose via ControlNet to guide the composition while letting the model fill in details. Similarly, you can attach LoRA weights (for example, a LoRA that imparts a specific art style or a trained character) – Fal allows specifying these under “Loras” in the input. Another tip is to utilize the **“IP Adapter”** fields Fal shows ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Add%20item)) – this suggests image prompt adapters, which can align generated images to a reference image’s style. By uploading a reference under “Image Url” and toggling IP Adapter, you could influence the output toward that reference’s aesthetic. **Prompt chaining** is also useful: you might generate an initial image with SD3.5, then feed it back in with a refined prompt (image-to-image) to enhance certain features. For example, generate a base, then re-run with prompt “enhance details of [previous description]” at a higher guidance scale to sharpen. SD3.5 Large is quite powerful, so also consider using fewer inference steps than older models needed – it might reach good quality in 20–30 steps instead of 50. Adjust **CFG scale** (guidance scale) depending on output: if images are getting distorted or off-track, try a slightly lower scale (e.g. 7 instead of the default 8 or 9).

**Cost & Performance:** Stable Diffusion 3.5 is priced per image output on Fal. It is roughly on par with other diffusion models in cost; *for instance, Fal’s UI indicates about 15 images per $1 at default settings*, which works out to roughly **$0.067 per image** (for a ~1MP image). This is a bit higher cost than smaller Stable Diffusion models, reflecting the larger model size. However, its **“resource-efficiency” improvements** mean it doesn’t scale up cost drastically ([Generating Images from Text | fal.ai Docs](https://docs.fal.ai/guides/generating-images-from-text/#:~:text=%2A%20fal,efficiency)) – you’re getting better quality for a similar cost to older models. Performance-wise, it is heavier than SD1.5 or 2.1: generation may take a bit longer and uses more VRAM. Fal likely runs it on an A100 or better; you might see 5–10 seconds for a 512×512 image with default steps. The model supports up to 1024×1024 or higher, but larger sizes will use more credits (since cost is by pixel count). Fal’s API allows real-time streaming of Stable Diffusion results if you use their WebSocket (so you can see progressive results for long generations). There aren’t specific rate limits noted beyond standard Fal API limits. In summary, expect to pay a few cents per image and get top-tier results with moderate latency. For heavy use, consider Fal’s enterprise if you need to scale this model.

## Video Generation Models

### Minimax Video-01-Live – Text-to-Video (Hailuo AI)
**About & Key Features:** MiniMax’s Video-01-Live is a frontier text-to-video model, specialized in turning **static inputs into fluid animations** ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=static%20images%20to%20life%20and,of%20fal%E2%80%99s%20inference%20engine%2C%20this)). It excels in scenarios with high motion – e.g. lively scenes, camera movements – while maintaining **exceptional temporal consistency**, especially for faces ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=static%20images%20to%20life%20and,of%20fal%E2%80%99s%20inference%20engine%2C%20this)). It produces clips just a few seconds long (typically ~3–5 seconds) at moderate resolution. This model shines in both **stylized and realistic content**; it strikes a balance allowing cartoon-like results or real-life footage style. It’s considered a **“Live2D” model – ideal for bringing a single image to life** with motion.

**Prompting Best Practices:** You can use MiniMax Video in two ways on Fal: pure **text-to-video** (describe the scene and motion) or **image-to-video** (provide an initial image plus a prompt). For best results, use the **image-to-video mode with a reference image**. For example, supply an image of a character or scene, and prompt what action or camera motion should happen. The prompt can be relatively short: focus on the action and setting rather than excessive detail (since some details come from the image). If using text-only, describe the subject and the desired motion. MiniMax responds well to **action verbs and camera directions** – e.g. *“A painting of a ship at sea *starts to move*: waves crash, the ship rocks. Cinematic camera pan from left to right.”* Keep prompts in present tense as if describing a film scene. Because it handles faces well, you can use it for **talking avatar animations** or **dynamic portraits** – in such cases, prompt subtle motions (blink, head turn) to avoid unnatural movement. Note that **consistency of the subject** frame-to-frame is a strength, so you don’t need to repeatedly mention the subject in the prompt; set the scene once. Also, consider the **length** – Fal likely allows you to specify duration or it may be fixed ~4s. If duration is fixed, the model divides your described action into that time span. If it’s user-adjustable, be aware longer durations might reduce frame quality. 

**Advanced Tips:** Combine MiniMax with other tools for enhanced output. For instance, you can generate a still image with an image model (Flux or SD) and feed it to MiniMax to animate it – this **prompt chaining** ensures you get the exact character or art style you want, then adds motion. Use **camera instructions** liberally: MiniMax’s training emphasizes smooth camera moves ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=animations%2C%20excelling%20particularly%20in%20high,of%20fal%E2%80%99s%20inference%20engine%2C%20this)), so include terms like “camera zooms in,” “pan around,” “steadycam follows the subject,” etc., to exploit that cinematography understanding. If you have a specific motion in mind, you might also experiment with giving a **reference video** (if supported by the multi-conditioning “director” version of the model – Fal’s interface shows a “video-01-director” in their changelogs, which suggests an ability to use a reference video’s motion). When dealing with fast motion, ensure your prompt sets the scene clearly in the first frame (the model doesn’t get multiple prompts over time, just one overall description). For example, *“A middle-aged man stands in a desert outpost (start). He suddenly begins running as the wind kicks up sand (action). Camera shakes to emphasize speed (camera).”* The model will interpret this sequence of events within the short video. 

**Cost & Performance:** MiniMax Video-01-Live is **billed per output video** on Fal. Official Fal pricing indicates similar models (like Hunyuan) cost about $0.4 per video ([Pricing | fal.ai](https://fal.ai/pricing#:~:text=Pricing%20,megapixel%2C%20Billed%20by%20the)), so expect on the order of a few dimes per generated clip. It’s a **partner model** (Hailuo AI), implying it may not be open-source and thus carries a cost per run. Performance is quite good – it generates ~4-second 480p videos in roughly a minute or two. Fal’s infrastructure note mentions “under a minute” generation for top models ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=and%20realistic%20content,the%20fact%20that%20AI%20video)), and MiniMax is optimized, so your request might complete in ~30–60 seconds. There may be a queue for video models since they use a lot of GPU time (Fal will automatically queue and stream status). Also, **output frame rate** is decent (~24 FPS), giving smooth motion. However, video outputs are capped in length to keep inference feasible (likely 5 seconds max for this model by default). If you need longer videos, you’d have to generate in segments. Fal’s usage limits: typically, video models might have a concurrency limit (e.g. only a certain number of videos generating at once per user) – check Fal’s documentation if scaling up. Each generated video has a fixed cost; since MiniMax can also generate *with audio* (if combined with MMAudio), note that audio generation might be separate (MiniMax itself generates visuals only). 

### Hunyuan Video – Text-to-Video (Tencent Hunyuan)
**About & Key Features:** Hunyuan Video is a **large-scale (13B parameter) video foundation model** by Tencent ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=animations%2C%20excelling%20particularly%20in%20high,of%20fal%E2%80%99s%20inference%20engine%2C%20this)). It focuses on **cinematic video quality** with physically accurate motion and coherent scene transitions. In plain terms, Hunyuan produces short videos that look more like real movie footage: it handles camera movements, object interactions, and “real-world physics” (e.g. objects obey gravity, continuity of motion) exceptionally well ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=animations%2C%20excelling%20particularly%20in%20high,of%20fal%E2%80%99s%20inference%20engine%2C%20this)). It tends to generate a sequence with a clear beginning-to-end action (continuous action sequences) rather than disjointed frames. On Fal, it’s a flagship model for high-fidelity video and often requires powerful GPUs – the upside is Fal’s optimizations allow it to generate in **under a minute** for a clip ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=and%20realistic%20content,the%20fact%20that%20AI%20video)), which is quite fast for such complexity.

**Prompting Best Practices:** **Detailed scene descriptions** are important. Treat your prompt like a movie snippet script. Include the setting, characters, and the action, as well as cinematic cues. For example: *“A lone motorcycle speeds down a neon-lit city street at night, rain reflecting blue and pink lights on the asphalt. The camera follows from behind at a low angle, then swings to the side as the bike skids to a stop. Sparks fly as the tires scrape. (Cinematic, slow-motion as needed, dramatic lighting).”* In that prompt we described the environment, the action, and even the camera behavior. Hunyuan will attempt to honor those, producing a cohesive few-second scene. **Keep prompts logically consistent** – Hunyuan is good at continuity, so make sure your prompt doesn’t describe disjointed events that would be hard to resolve in a short clip. It’s better at realistic styles; you can prompt cartoon or fantasy, but its strength is making things look like real footage, so leveraging that (with terms like *“cinematic, 4K detail, film-like”*) will play to its strengths. **Specify motion clearly:** if you want a certain subject movement or camera cut, say so. Otherwise, the model might choose a default interpretation (like a simple pan). Because Hunyuan is trained for high quality, even short prompts can yield decent results, but providing context helps (who, where, when). Also, consider **temporal words** – use phrases like “slowly walks”, “suddenly jumps” to indicate timing of actions. If your scene has multiple actors, be mindful that the model might not perfectly maintain identity for more than one main subject (though it’s better than many models at it). It might be safer to focus on one primary subject in the prompt for clarity.

**Advanced Tips:** **Multimodal conditioning** could be very useful if supported: Hunyuan has an *image-to-video (I2V)* variant (Tencent released HunyuanVideo-I2V open-source for image input ([tencent/HunyuanVideo-I2V - Hugging Face](https://huggingface.co/tencent/HunyuanVideo-I2V#:~:text=tencent%2FHunyuanVideo,Video%20Model))). On Fal, if available, try providing a starting image via the Hunyuan Video LoRA endpoint or a special parameter. This can lock in the subject or background you want, and then Hunyuan will animate it with the described action. Another tip is to break complex sequences into smaller prompts and generate separate clips, then stitch them – because Hunyuan’s outputs are short, if you want a “story” longer than ~5 seconds, you might do multiple requests (e.g. scene1: man enters room, scene2: man picks up object). Use editing to concatenate them. **Leverage camera and lighting**: since this model excels at cinematic looks, you can specify camera lenses (e.g. “fish-eye view” vs “telephoto close-up”) and lighting conditions (“dusk with long shadows”, “strobe lighting in a club”). It will try to incorporate those and it makes the output feel professionally shot. **Motion physics** are a highlight – for example, if you describe “a ball bouncing three times then rolling to a stop,” Hunyuan is likely to get those beats correct. Don’t hesitate to describe nuanced motion (“limping run” vs “running” or “fluttering leaves in wind”). Finally, if available, use **Fal’s “Hunyuan Video LoRA”** endpoint ([Hunyuan Video | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/hunyuan-video#:~:text=fal,1)) or similar – this might allow fine-tuning or style adjustments on Hunyuan’s base model, giving you even more control (such as a specialized style or improved prompt adherence for certain content). 

**Cost & Performance:** Hunyuan Video is one of the pricier endpoints: **$0.40 per video output** (flat rate) on Fal ([Pricing | fal.ai](https://fal.ai/pricing#:~:text=Pricing%20,megapixel%2C%20Billed%20by%20the)). This means each clip (regardless of length up to the model’s limit) costs 40 cents. It’s billed per video rather than per second, likely assuming a standard clip length (e.g. ~4 seconds). In terms of performance, Fal’s integration is highly optimized – generation time is often ~30 seconds for a 4-second HD clip, which is remarkably fast given the model’s size. Fal’s team even boasted record-breaking generation times under a minute for cinematic content ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=and%20realistic%20content,the%20fact%20that%20AI%20video)). If you request the maximum resolution or length, times could edge up, but it’s still real-time relative to older methods. There might be **GPU time limits** per request (for example, Fal might limit to 8 seconds of 480p video to fit in memory/time). Keep outputs within recommended parameters to avoid errors. The **quality** is very high: expect 720p or higher frames with good coherence. For usage, Fal likely restricts usage to non-free tier given it’s partner model – you’ll need sufficient credits. Because each run is costly, test with lower settings or shorter durations if possible (to verify prompt effectiveness) before spending on full quality runs. 

### Kling 1.5 (Pro) – Image-to-Video / Text-to-Video 
**About & Key Features:** Kling 1.5 Pro is an advanced AI video generator known for its **1080p HD output** and enhanced physics simulation ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=model%20achieves%20record,also%20exponentially%20growing%20each%20month)). It’s a successor in the Kling series, which are models geared towards professional-quality video. Kling 1.5 can produce longer and more complex videos than many others, and particularly focuses on **realistic physical interactions** (for example, water flow, collisions, gravity) in the generated video. It’s often used for scenarios that demand a combination of high resolution and believable motion. The model was cutting-edge for 2024 and has since been updated (Kling 1.6), but 1.5 Pro remains relevant on Fal for slightly lower cost and as some users may prefer its style.

**Prompting Best Practices:** **Start with an image when possible.** Kling Pro models typically accept an initial image to guide the video (Fal’s interface for Kling 1.5 Pro is under “image-to-video” by default ([Kling 1.6 | Image to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/kling-video/v1.6/pro/image-to-video#:~:text=Kling%201)) ([Kling 1.6 | Image to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/kling-video/v1.6/pro/image-to-video#:~:text=Input))). Providing a keyframe or character image ensures the output video looks consistent with that. For example, supply a photo of a person and prompt them to perform an action. If you don’t have an image, you can still do text-to-video, but results may vary more in appearance. When writing the prompt, describe both the **scene and the movement**. Kling handles diverse situations, but it helps to emphasize what should happen physically. For instance: *“A red sports car drifts around a corner on a race track, kicking up smoke. The camera is stationary at the corner as the car enters frame, slides, then exits frame. Debris and smoke follow the tire motion.”* That prompt gives a clear physical scenario (drifting car with smoke, debris) which Kling can simulate (smoke and debris following physics). **Mention the resolution or detail** if needed – since Kling can do 1080p, you might say “in detailed HD” or similar to push it toward using full capacity. For style, if you want a particular look (cinematic vs. CCTV vs. animated), mention it. Kling is quite general; it doesn’t inherently lean cartoonish or realistic – your prompt can nudge it. Using **present tense and descriptive language** (like “walks,” “explodes,” “slowly pans”) will help it map timing. 

**Advanced Tips:** **Effects and multi-condition inputs:** Fal’s listing shows a “/effects” endpoint for Kling 1.5 Pro ([Luma Dream Machine | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/luma-dream-machine#:~:text=fal,5%20%28pro%29%20new)), suggesting you can input not just an image but possibly a **video or segmentation mask as additional conditioning**. If you have a reference video for motion or a mask to constrain the region of generation, use those advanced inputs via the API. For instance, you could input a rough animation (even stick figures) and have Kling render it in HD with your described scene – that’s speculative but indicated by “effects” features. Another advanced strategy is to exploit **the physics engine aspect**: test prompts that involve interactions (e.g. “a glass bowl shatters on the floor”) – Kling’s training includes physics, so it might handle these better than other models. Also, **lighting and camera**: since it’s pro, it likely supports complex lighting (shadows, reflections) and camera moves. You can do multi-sentence prompts where the second sentence is about camera work, like “Camera follows the glass pieces on the floor in slow motion.” Given Kling’s resolution, you can also extract high-quality frames (even use the output as images). If you need to extend a Kling video’s length, you can try a trick: generate sequential clips where the last frame of one is the first frame of the next (Fal’s API might allow you to feed the last frame as an initial image for the next segment). This can approximate a longer continuous video by pieces. It’s labor-intensive, but an advanced user could semi-automate it. Additionally, consider **post-processing**: using Fal’s **video upscaler** on Kling outputs or their **frame interpolator** if any, to possibly smooth or lengthen the video. Kling 1.5 outputs are already HD, but if you generate at a slightly lower res for speed (say 720p) you could then upscale after. 

**Cost & Performance:** Kling 1.5 Pro is **billed per second of video** on Fal’s platform. The pricing is around **$0.10 per second of generated video** ([Pricing | fal.ai](https://fal.ai/pricing#:~:text=Pricing%20,megapixel%2C%20Billed%20by%20the)). So a 5-second clip costs roughly $0.50. This usage-based pricing makes sense since users can often choose the duration. It’s a bit cheaper than Kling 1.6 (which is the same rate; 1.6 is $0.10/s as well) – essentially, Kling models cost ~$6 per minute of video on Fal’s API. Performance: it’s a heavyweight model, so generation might take on the order of **seconds per frame**. A 5-second (120-frame) 1080p video could take a few minutes to generate. Fal’s documentation suggests using a **high-end GPU backend** for these (and indeed Fal offers H100 instances). Expect, say, 2–3 minutes wait time for a clip on the regular queue. Because it’s high-res, the file sizes can be a few MBs for a short clip. Fal returns a URL to download the video when ready. On usage constraints: Fal might restrict maximum length (for example, possibly 10 seconds max to prevent excessive GPU use in one request). Also note that Kling models might be subject to content moderation (they are powerful – Fal likely blocks NSFW or harmful content prompts). All told, Kling 1.5 Pro gives excellent quality at a cost – use it when 1080p or complex physics are required. For simpler needs, you might use a cheaper model or lower resolution.

### Kling 1.0 (Standard) – Text-to-Video (basic version)
**About & Key Features:** Kling 1.0 Standard is an earlier generation model in the Kling video series. It produces somewhat lower resolution outputs (often 720p or less) and less detailed physics compared to the Pro version. However, it’s still capable of taking a text prompt (or an image + prompt) and generating a coherent video clip (a few seconds long). As a **standard tier**, it was designed to be more accessible (faster and lighter) at the cost of some fidelity. It’s suitable for quick prototyping or content where 1080p isn’t needed. This model is now a bit older, but Fal still provides it, likely because it runs faster or on less powerful hardware – offering a **cost-effective option** for video generation.

**Prompting Best Practices:** Use **simpler, concise prompts** for Kling 1.0. It doesn’t capture fine-grained details as well as the newer models, so focus on the primary action or scene. For example: *“A small puppy jumps into a pile of leaves.”* That might yield a cute, simple animation. If you heap too many details (lighting, exact camera moves, etc.), the model might struggle or produce muddier results. It’s best at **single focused actions** or scenes. If using image-to-video, provide a clear, front-and-center subject in the image (the model will then animate that subject doing something you describe). Kling 1.0 may not maintain identity perfectly through complex motion, so prompts with big changes (e.g. subject turning into something else) might not be stable – stick to one subject. **Leverage styles**: you can explicitly request a cartoon or an illustration style if you want – standard models sometimes adhere to style cues more easily (since they have less emphasis on realism). For instance, add *“in the style of a pencil sketch”* if you want an animated sketch look. Conversely, if you want realistic, mention *“realistic”* or camera terms to anchor it, though expect slightly less realism than Kling 1.5 or Hunyuan. Also, keep the prompt **short in chronology** – basically describe one scene, not multiple cuts. Kling 1.0 likely can’t do a complex multi-part sequence in ~3 seconds. 

**Advanced Tips:** One useful approach with Kling 1.0 is using it for **storyboarding**. Because it’s cheaper/faster, you can try out various scenes quickly, then upscale or refine with Kling Pro or another model. You can also experiment with its **“effects” mode or multi-conditioning** if available (Fal shows Kling 1.0 Standard under image-to-video and possibly “effects” similar to 1.5). This might let you input, say, a segmentation map or pose to guide the video. For example, you could draw a simple stick figure animation of the motion you want, feed that as a conditioning video, and have Kling render it with your prompt’s appearance. This is advanced and requires knowledge of Fal’s API (and whether they exposed that feature publicly). Another tip: because the output isn’t the highest resolution, you can more safely use Fal’s **video upscaler** on it to enhance resolution. The RealESRGAN upscaler for video can sharpen and upsize a Kling 1.0 clip fairly well since it doesn’t have to deal with super fine details (you’ll basically get a cleaned-up 1080p version of a 720p output). Just keep an eye on any artifacts – upscaling will exaggerate any weirdness from the original. If the model sometimes produces **jitter or flicker**, a trick is to generate a slightly longer video and then trim the ends (often the start or end frame might be less consistent). Or generate multiple and pick the one with the least flicker. Finally, consider **mixing audio** after generation: Kling 1.0 doesn’t generate sound, but pairing it with an audio model like MMAudio or adding sound effects manually can significantly improve the perceptual quality of the final video.

**Cost & Performance:** Kling 1.0 Standard is more economical. It’s typically **charged per second of video** like the Pro, but at a lower rate (Fal’s pricing might not list it explicitly, but one can infer it’s lower – possibly around **$0.05 per second**). In Fal’s UI, it might even show how many runs per $1; e.g., it might say something like “for $1 you can run ~20 times,” indicating a cheaper cost per clip. In any case, it’s roughly half the price of Kling Pro. Performance is faster – running on slightly less compute, maybe it generates frames a bit quicker. Users have reported around *4-5 seconds per video generation* with LTX or similar models on Fal ([Playing with the new LTX Video model, pretty insane results ... - Reddit](https://www.reddit.com/r/StableDiffusion/comments/1h1bb0f/playing_with_the_new_ltx_video_model_pretty/#:~:text=Playing%20with%20the%20new%20LTX,5%20seconds%20per%20video%20generation)), and Kling 1.0 standard would be in that ballpark when it launched. So expect perhaps ~10–30 seconds for a short clip to generate, depending on length and complexity. The output resolution might be around 576p or 720p by default, which speeds things up. There may be limits like max 3 seconds length (some early Kling versions had shorter max durations). But you can run multiple in parallel more easily since it’s lighter. Overall, it’s a good **budget option**: faster and cheaper, at the cost of some quality. Many developers use it during development, then switch to Kling Pro or Veo2 for final high-res results.

### Luma “Dream Machine” v1.5 – Text-to-Video 
**About & Key Features:** Luma Dream Machine 1.5 is a video generation model by Luma Labs that is notable for **immersive scene generation and consistent character interactions** ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=Luma%20Dream%20Machine%20is%20a,more%20ideas%20and%20dream%20bigger)). It’s built to handle both text and image inputs, creating **realistic videos from text or images** ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=Luma%20Dream%20Machine%20is%20a,more%20ideas%20and%20dream%20bigger)). Dream Machine places emphasis on maintaining accurate physics and interactions (similar to Hunyuan/Kling) and also showcases **cinematic camera movements** ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=Dream%20Machine%E2%80%99s%20advanced%20capabilities%20ensure,some%20limitations%2C%20the%20team%20is)). In practice, it’s used for creative storytelling – the model attempts to translate imaginative prompts into vivid video clips. It’s often praised for efficient generation and has been offered on Luma’s own platform as well; Fal’s integration makes it accessible via API. This model is a blend of artistic and realistic capabilities, hence the name “Dream Machine” – it’s meant to realize your imagined scenarios in video form.

**Prompting Best Practices:** **Imagination is key** – don’t be afraid to feed it fantastical or complex prompts. Luma DM is designed to accommodate “dreamlike” sequences. However, include concrete visual elements so the model has something to render. For example: *“A castle made of clouds floats in a sunset sky. Golden light beams through the towers as flying whales circle around it. Camera orbits slowly to reveal a distant rainbow horizon.”* This prompt is fanciful but provides clear visual cues (castle, clouds, sky, whales) and an action (camera orbits, whales circle). The model can take that and animate a short clip capturing the essence. **Include motion or changes** – even if subtle (clouds drifting, etc.), because video models need something to animate. Pure static scenes may result in only minimal motion. Luma DM, as the name suggests, handles **multiple subjects interacting** better than some; you can have, say, a person and an object and it should keep track of both. But try to keep it to one primary focal point to avoid confusion. Also, leverage its cinematic flair: asking for specific camera moves (orbit, zoom, POV, tilt) and atmosphere (lighting, mood) will enrich the output. For style, you can push it to either realistic or stylized: e.g., add *“in Pixar style”* for a cartoon vibe, or *“photorealistic 4K detail”* for realism. It’s quite versatile. **Keep prompt length moderate** – a brief paragraph is fine, but a super long prompt might not improve output further and could introduce incoherence. Finally, if using image-to-video mode, provide an image that aligns with your prompt (e.g., an image of a castle if your prompt is about a castle in the sky), so the model has a solid starting point.

**Advanced Tips:** **Multiconditioning and “Ray 2”**: The search results hinted at something called “Ray2 flash model” and Dream Machine updates ([Luma Dream Machine Just Got BETTER – Video to Audio & NEW ...](https://www.youtube.com/watch?v=Ms-UVcA64Uw#:~:text=Luma%20Dream%20Machine%20Just%20Got,them%20both%20to%20the)). This suggests Luma has multiple models or modes (maybe one for faster generation called Ray). On Fal, Dream Machine might have an update or a parameter for “Ray” (real-time mode) vs quality mode. If available, you could choose faster generation at slightly lower quality for drafts, then switch to high quality. Also, being a partner model, it might allow **longer durations or higher resolution** if you have the credits. Luma DM possibly can go beyond 5 seconds (some demos on Luma’s site show ~8 second clips). If Fal permits, try requesting a bit longer video if your scene needs it – the model is optimized for quality even at extended frames. Another tip: **control the beats of the video** by structuring your prompt with a sequence of events. For instance, use sentences that imply first and second halves of the video: “A wizard stands in a field of stars. He raises his staff (beginning). Suddenly, the stars swirl around him forming a galaxy (climax).” The model might interpret that as an evolving scene. Additionally, you can combine Dream Machine with Fal’s **video-to-video or interpolation tools**. For example, generate two different Dream Machine clips and then morph or interpolate between them using an interpolation model if one exists on Fal – that could create a longer continuous video from two segments. As for consistency, if you have a main character across clips, use the **same prompt wording** for that character in both clips to help the model keep it similar. Dream Machine is also known for **fast iteration** – the Luma team highlights speed ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=creation.%20It%20generates%20high,more%20ideas%20and%20dream%20bigger)), so you can quickly try different ideas. Embrace that by trying multiple variants of your prompt (different camera angle, different time of day) to see which you like best. 

**Cost & Performance:** Luma Dream Machine is priced around **$0.50 per video** generated ([Luma Dream Machine | Text to Video | AI Playground - Fal.ai](https://fal.ai/models/fal-ai/luma-dream-machine#:~:text=Generate%20video%20clips%20from%20your,can%20run%20this%20model)). Fal’s interface notes “Your request will cost $0.5 per video” for this model. So, each clip (likely ~4 seconds) is half a dollar. The performance is quite good: generation is usually on the order of seconds per second of video. Dream Machine has been optimized for speed (the Luma folks tout rapid iteration), so you might get results in ~20–30 seconds for a short clip. It may not be as heavy as something like Veo2, which is why the cost is a bit lower than Veo2’s per-second pricing. *For $1 you can run ~2 videos*, Fal indicates, which aligns with $0.5 each. If you want longer videos, you would pay proportionally (e.g., $1 for an 8-second video if allowed). Check Fal’s model card for any length restrictions – if not stated, assume ~5 seconds default. On usage, since it’s a partner model, you need adequate credits; there likely isn’t a free tier run for this. But it being available on Luma’s own platform means it’s battle-tested. Fal’s hosting likely uses powerful GPUs and possibly multi-GPU inference to accelerate it. **Reliability:** fewer “glitches” – users report that Dream Machine outputs are relatively stable with fewer artifacts ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=Dream%20Machine%E2%80%99s%20advanced%20capabilities%20ensure,impressive%20capabilities%20in%20the%20future)) (like less flicker or weird distortions). So you can generally trust the output quality. If anything, any inconsistencies might come from trying to do too much in a short clip. If you stay within a straightforward scene, it performs near state-of-the-art for its class.

### MMAudio V2 – Video-to-Audio (Sound Generation)
**About & Key Features:** MMAudio V2 is an audio generation model that takes **video (and optionally text) as input and produces a synchronized audio track** ([MMAudio V2 | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/mmaudio-v2#:~:text=fal)). In essence, it’s designed to add suitable audio to a silent video. This could be background music, sound effects, or ambient audio that matches the content of the video. MMAudio stands for *Multi-Modal Audio*, indicating it can use multiple modalities: it “watches” the video frames and “reads” the text prompt to decide what audio to generate. A key feature is that it outputs an **audio-augmented video file** (it returns the video with sound overlayed) ([MMAudio V2 | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/mmaudio-v2#:~:text=OpenDownload)), making integration easy. It’s fast and optimized for synchronization, meaning events in the video should align with audio cues (for example, if it sees an explosion in the video, it will generate an explosion sound at that moment). It’s particularly useful for automatically sound-designing AI-generated videos from other models.

**Prompting Best Practices:** Always provide **both inputs**: a video and a brief text describing the desired audio style. The video input is mandatory (the model needs visuals to sync with), and the text prompt guides what kind of audio. For instance, if you have a clip of a busy city street, you might prompt “city street ambiance with honking cars and footsteps” – MMAudio will then generate traffic noise, car horns, and footstep sounds in time with any visible cars or people in the video. Keep the text prompt concise and **focused on audio aspects** (e.g. genre, instruments, types of sounds). If you want music, specify genre or mood: *“upbeat electronic music”* or *“tense orchestral score”*. If you want real-world sounds, specify them: *“ocean waves and seagulls”*. The model will mix elements as appropriate. **Match the prompt to the video content** – if the video is of a dog barking but you prompt “quiet library ambiance,” that mismatch may confuse the result or just yield silence even though the dog’s mouth moves. So describe what *should* be heard given what is seen. Also, mention **timing cues** if needed: for example “start with silence then crescendo as the object appears” if that’s important, though the model primarily aligns automatically. Since MMAudio is v2, it’s likely improved at coherence and quality, but it’s still good to avoid extremely complex audio descriptions in one go. One or two sentences are enough. 

**Advanced Tips:** Use MMAudio in a **workflow chain**: generate video with another model (Flux, Kling, etc.), then pass that video to MMAudio with an appropriate prompt to add sound. This effectively creates a complete audiovisual clip. You can also feed **real videos** (not AI-generated) to MMAudio to test its Foley abilities – for example, give it a clip of someone juggling and prompt “gentle background music and soft thuds when balls caught.” This model can generate both music and sound effects simultaneously. If you want **music only or SFX only**, clarify that in the prompt (“no sound effects, only background music” or vice versa). Conversely, if you want a full mix, mention the elements (“rain sounds and soft piano music”). The synchronization is a standout feature, but if you find slight misalignments, you might experiment with prompt phrasing (e.g., explicitly saying “when X happens, [sound]”). Also, note Fal lists an alternate usage as `text-to-audio` for MMAudio ([Model Gallery - Fal.ai](https://fal.ai/models?=keywords=audio#:~:text=Model%20Gallery%20,audio.%20MMAudio)) – which suggests you can use it to generate an audio file from text alone (like a music generator akin to MusicGen). If you call the `.../text-to-audio` endpoint with just a prompt, it might produce sound unrelated to any video. That’s useful if you need a standalone sound. Another tip: combine MMAudio with **voice models** for a complete result. For example, if your video is a person talking, you’d use a voice TTS to generate speech audio, then use MMAudio to add background ambience and perhaps auto-duck (lower volume) the background when speech is happening. Currently, MMAudio doesn’t know about the speech content (it would only see the mouth flaps, not the actual words). So coordinate it by generating speech audio first and then instruct MMAudio accordingly (e.g. “add city ambient noise under dialogue”). If precise control is needed, you might generate separate layers (music vs SFX) by running MMAudio multiple times with different prompts and mixing audio tracks externally. But in most cases, a single run with a well-crafted prompt yields a complete synced audio track.

**Cost & Performance:** MMAudio V2’s pricing isn’t explicitly listed per second, but since it’s an **audio-generation** model, it’s likely billed by audio duration. Many TTS/audio models on Fal cost on the order of a few cents per minute. It’s reasonable to expect something like **$0.02–0.03 per second of audio** (which is roughly $1.80 per minute). This is in line with Fal’s TTS (PlayHT v3 is $0.03/minute) but likely higher since music/sound generation is heavier. If the video is, say, 5 seconds, the cost might be ~$0.10. Fal’s interface might state it like “$X per minute of audio.” In any case, adding audio is generally cheaper than generating the video itself. Performance: MMAudio is **fast – near real-time**. Generating a few seconds of audio should only take a couple of seconds of processing. It’s designed to not waste time; Fal even notes that adding the output audio is done in a background thread so it **doesn’t charge GPU time for non-inference tasks** ([Fastest FLUX Endpoint | fal.ai Docs](https://docs.fal.ai/fast-flux/#:~:text=Note)). You can expect low latency. The model can probably generate common sample rates (likely 22kHz or 48kHz audio). There might be a limit to audio length (likely tied to video length, which might max out ~10s unless you have a special plan). If you input a longer video, Fal might either cut it or require a separate processing approach. Quality-wise, MMAudio v2 improved on v1’s coherence, but occasionally the audio might sound a bit generic or “AI-ish.” For critical use, minor post-editing of the audio (equalizing levels, etc.) could help. However, for most projects, it’s a huge time-saver to get synchronized sound automatically. No known specific usage limits except the general Fal rate limiting; you can call this for each video you generate to quickly sound-enable them.

### Sync.So Lipsync 1.8.0 – Video-to-Video (Audio-driven Animation)
**About & Key Features:** The Sync.So Lipsync model (v1.8.0, now a 1.9.0-beta on Fal ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=sync.so%20,to%20Video))) is a specialized model that **animates a video (typically of a face) to lip-sync with a given audio**. Essentially, you give it a static face video (or even just an image treated as a video) and an audio clip of speech, and it outputs a video where the face’s mouth (and possibly expressions) move in sync with the speech ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=Video%20Url)) ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=Audio%20Url)). It uses advanced algorithms to ensure high-quality synchronization – the goal is a realistic talking head. This model does not generate the voice – you provide the voice/audio – but it generates the facial movements. It’s great for dubbing, creating virtual presenters, or giving a voice to still images. Sync.So is known in the space for good quality lipsync, and this model being on Fal indicates a collaboration or usage license for developers.

**Prompting Best Practices:** For this model, “prompting” is more about **providing the right inputs** rather than a text prompt. The inputs are: a **video (or image) of the person** and an **audio clip** of the speech ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=Video%20Url)) ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=Audio%20Url)). Ensure the video/image has the person’s face clearly visible, ideally facing the camera or only slightly turned – this gives the model a good template to animate. The audio should be clean speech (or singing). There is typically no text prompt needed, since the audio itself guides the lip movements. However, if Fal’s interface had an “Additional Settings” or textual input, it could allow some control like choosing **intensity of expression** or enabling head movements. In absence of that, just focus on input quality: e.g. a **neutral expression image** works best, so the model can animate mouth and slight expressions on top of it. If the person’s lips are closed in the image, that’s fine – the model will open them as needed. Avoid images where the mouth is extremely open or in odd positions initially. For the audio, note the language – the model likely supports multiple languages (depending on training), but English speech is the safest bet for best sync. The length of the output video will match the audio length. So if you have a 15-second audio, ensure your input face video or image is okay to be used for that duration (a single image can be used for 15 seconds; the background will just be static unless the model does slight head moves). One more tip: trim any silence at the start of the audio, because the model will still produce video for it (maybe with closed mouth). If you want a natural lead-in, you can leave a bit of silence – the person would appear listening then start talking. 

**Advanced Tips:** **Facial enhancements:** sometimes just moving lips isn’t enough for realism. The Sync.So model might also animate jaw movement, subtle head nods, eye blinks, etc. If you find the output too static except the lips, consider using an additional tool to add *micro-expressions*. For example, you could first use Sync.So to get the perfectly synced lips, then run the video through an **animation model (like an expression transfer or a subtle head-movement generator)** to add a bit of natural motion. Alternatively, when preparing your source image/video, you could provide a short video of the person blinking or nodding neutrally for a second, and use that as the input video (the model will use those frames to keep the person “alive” during speech, possibly). Another trick: if the voice audio has emotion (excited, sad), the model might not know to change expression; you could manually pick a source image with a matching expression (smiling for upbeat audio, etc.). That way the overall tone matches. Also, **backgrounds** – if your input is just an image with a background, that background will stay static. To avoid any perception of stutter, sometimes people blur the background or use a solid color. If needed, remove the background (Fal has a background removal model ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=fal,with%20evolved%20consistency%21%20animation%20stylized))) before lipsync, then you can composite a video background later. This avoids any weirdness like a moving jaw causing background pixels to flicker (if the model tries to move the whole head). The Sync.So model is likely robust, but these are considerations for polish. If you want to get very advanced, multiple speakers in one video could be done by switching inputs: e.g., for dialogue, run model on PersonA’s face with PersonA’s audio, then PersonB’s face with PersonB’s audio, then edit the two resulting videos together. The model itself doesn’t take multiple audio streams at once (no multi-speaker in one run). Finally, since this is a beta version, check for any artifacts around the mouth. If you see some, a light post-process in a video editor (color correction or slight blur on the mouth region) can help hide minor glitches.

**Cost & Performance:** Lipsync models are relatively heavy but not as bad as full video generation. You’re basically doing face motion, which involves a neural renderer. The pricing on Fal is around **$0.05 per minute of output** audio/video ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=)). That corresponds to 5¢ per minute (or about $0.00083 per second). This is actually quite affordable – indeed Fal’s UI said *“$0.05 per audio minute”* for the dialog TTS which is similar in cost. It means a 10-second clip costs less than a cent. The reason is that the model likely runs on efficient hardware (maybe it uses a GPU for a short time). Performance wise, **it’s fast**. Expect that a 10-second clip will generate in a few seconds. Even a minute of video might only take a handful of seconds to process on a modern GPU, since the model might leverage a precomputed facial geometry and then just render frames quickly. Possibly it could even work in real-time (some lipsync systems run at >20fps). On Fal, after submitting, you’ll likely get the result within the length of the audio or faster. There aren’t typically strict limits on duration from a technical standpoint – you could do a full minute monologue. However, extremely long inputs (e.g. 5 minutes) might be split or not allowed due to memory. Also, ensure the audio sampling rate is what the model expects (Fal likely handles this – just feed common formats like MP3/WAV). In terms of usage, since it’s a partner model (by SyncSo), Fal allows commercial use but presumably expects you to abide by ethical guidelines (e.g. don’t deepfake someone without consent, etc.). The output quality is usually high: as long as your input face is high-res, the output will be correspondingly high-res video with synched lips. Overall, cost is minor and speed is high, making it a very handy tool to use in your pipeline without much worry about expense or delays.

### Veo 2 – Text-to-Video (Google’s Model)
**About & Key Features:** Veo 2 is Google’s cutting-edge video generation model, recently integrated into Fal’s API ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=We%27re%20thrilled%20to%20announce%20that,most%20advanced%20AI%20video%20technologies)). It represents a significant leap in video synthesis, offering **unprecedented quality and control** in AI-generated videos ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=Setting%20New%20Standards%20in%20Video,Generation)). Key features include a superior understanding of **physics and human motion**, leading to natural animations, as well as **cinematographic excellence** – the model is aware of camera techniques and can produce complex shots (e.g. tracking, zoom, depth-of-field effects) ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=,Significantly%20fewer%20unwanted%20artifacts%20or)). Another major feature is its ability to output at **high resolution (up to 4K)** and handle extended durations (minutes-long videos) ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=understands%20the%20language%20of%20filmmaking,Significantly%20fewer%20unwanted%20artifacts%20or)), which is far beyond typical models. Veo2 also greatly reduces unwanted artifacts and flicker, making it *production-ready*. Essentially, it’s the state-of-the-art as of 2025 for text-to-video, encapsulating Google’s research advancements in this area. Fal’s integration allows developers to use this powerhouse via simple API calls, though at a premium cost.

**Prompting Best Practices:** **Think like a director.** When prompting Veo2, you have a lot of control, so writing a prompt akin to a movie scene description is effective. Include details on **setting, actors, actions, camera, and even lens/film style** if desired. For example: *“Cinematic shot of a female doctor in a dark yellow hazmat suit, illuminated by harsh fluorescent lab lighting. The camera slowly zooms in on her face, then pans gently to reveal the anxious expression in her eyes ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=Cinematic%20shot%20of%20a%20female,anxiety%20etched%20across%20her%20brow)). She stands before a laboratory table with beakers, slight motion blur on her hands as they tremble.”* This kind of prompt leverages Veo2’s strengths: it mentions lighting, camera movement, emotion, etc. The model will attempt to execute those directions: a slow zoom, a pan, and a focus on the facial expression. **Be specific but not overstuffed.** Veo2 can parse complex prompts, but ensure clarity – break the prompt into sentences for different parts of the scene (as in the example). You can also use commands like *“Shot with a 35mm lens on Kodak Portra 400 film”* to imbue a certain photographic quality (that was in Fal’s example prompt ([Veo 2 | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/veo2#:~:text=The%20camera%20floats%20gently%20through,film%2C%20the%20golden%20light%20creates))). If you want a certain duration or pacing, you might have to imply it (e.g. “slowly over 8 seconds, the scene unfolds…” though the actual duration depends on what you request via API). **Leverage physics**: if your scene has physical interactions (explosions, water, running, flying objects), describe them – Veo2 will handle the causality and realism better than other models. Also, utilize the fact that it can do longer videos: you can actually describe a beginning, middle, end in the prompt. For instance, “First, the camera does X… Then Y happens… Finally Z.” Veo2 might follow that sequence since it can generate more frames (though there’s no guarantee it splits exactly, it will try to incorporate the whole story). **Style**: if you want consistency with a particular visual style (say Pixar-like animation or monochrome noir film), state it – Veo2 is versatile and can emulate styles given its training on diverse footage. One more tip: Because Veo2 aims to reduce hallucinations, you likely *don’t* need to heavily negative-prompt. Still, avoid ambiguous references in your prompt that could confuse context. Keep the scene self-contained (the model doesn’t know earlier prompts or images unless provided).

**Advanced Tips:** **Use extended features if available:** Fal’s mention of “extensive camera controls” ([Veo 2 | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/veo2#:~:text=fal)) and Veo2’s abilities suggests there may be special parameters you can pass (possibly via the API rather than plain prompt) to control aspects like focal length, aperture (for depth of field), or even a **storyboard of keyframes**. If Fal’s API or OpenAPI schema for Veo2 allows inputting something like a sequence of text prompts with timestamps or keyframe images, you could truly control minute details of a multi-second video. This would be an advanced usage where you break down a 20-second video into 4 segments and prompt each for the segment’s content – Veo2 could then blend them if such interface exists (this is speculative; if not directly available, the model still might interpret a multi-sentence prompt sequentially). Also, explore **multi-modal inputs**: given its roots in research, Veo2 might accept a **reference image or video** (e.g. to anchor style or initial frame). If so, you could feed it a starting keyframe from which to build the video, ensuring consistency of a character or environment. Another advanced technique is using **Veo2 for longer outputs by chaining**: because Fal charges per second and the model can do minutes, you might still not want to do a full minute in one go (cost!). Instead, generate, say, a 15-second segment, then use its last frame as the first frame (image) for the next segment, with a prompt continuing the action. This can let you create a multi-part sequence that’s longer. You will have to manually stitch, but the continuity should be good if you carry over the last frame. Additionally, **adjust output resolution deliberately** – if you want a 4K output but are prototyping, start with 720p or 1080p to test prompt, then scale up the resolution parameter for final runs. Veo2 can do 4K ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=understands%20the%20language%20of%20filmmaking,Significantly%20fewer%20unwanted%20artifacts%20or)), but generating at that resolution is heavy; only do it when you’re satisfied with the scene at lower res. For the technically inclined, note that Veo2 likely uses a huge model (maybe >30B parameters and diffusion steps), so it’s slow – any trick to reduce required frames (like asking for 24 FPS instead of 30, or 5-second instead of 8-second) can save time and credits if it doesn’t sacrifice your vision. Because hallucinations are reduced, you can also trust it a bit more with **literal prompt** – e.g., it might actually render text on signs correctly (though that remains challenging). If it does handle text, you could even try including short dialogue or subtitles in the video by prompting a character to speak with visible speech (though more reliably, you’d overlay text in post). 

**Cost & Performance:** Veo2 is the most expensive model listed. Fal’s pricing for Veo2 is **$2.50 for the first 5 seconds, and $0.50 per additional second** ([Veo 2 | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/veo2#:~:text=For%205s%20video%20your%20request,50)). That means a 5-second video is $2.50, 6 seconds is $3.00, 10 seconds is $5.00, etc. Essentially **$0.50/second** after the 5-second minimum. This aligns with the model’s complexity and the fact it can produce high-res, lengthy clips. So, a 30-second 1080p video would cost $2.50 + 25*$0.50 = $15.00, which is steep but for what it is (perhaps replacing a whole video crew for a short B-roll) it could be worth it. Performance: It is heavy. Expect that 5 seconds of video may take on the order of a couple of minutes to generate (the exact speed depends on Fal’s hardware – they might allocate multiple GPUs or TPUs given the partnership with Google). Fal likely uses top-tier accelerators (maybe Google TPUs, since Google model) to serve this. Even so, generating each frame in 4K with a huge model could be slow. If time is critical, you might generate at lower resolution or fewer FPS. The model can do up to 4K@??fps, but you could request 720p@24fps to save time. The output will be delivered as a file URL; because it’s potentially large (4K frames * X frames), Fal might have a short retention on those outputs – make sure to download promptly. There are probably strict usage limits for Veo2: Fal might limit each user to certain seconds per day, or require an application for extended use, given the cost and load. They even mention giving out coupons on social media for free tries ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=Image%3A%20Veo2%3A%20State,Now%20on%20fal)). So treat this as a premium resource. In terms of quality, it’s state-of-art: you’ll see far fewer glitches, and the video will likely impress with how “film-like” it is. The physics and camera control mean it’s less trial-and-error – if your prompt is good, first output might be spot on. That helps offset the cost (you might not need many reruns). All in all, Veo2 offers top quality at high cost, suitable for when you need the **absolute best video generation Fal can provide** and are willing to pay for it.

## Music & Audio Generation Models

### MiniMax Music (Hailuo AI) – Text-to-Music
**About & Key Features:** MiniMax Music is a generative music model that creates **short musical compositions from text prompts**. It leverages advanced AI techniques (likely a transformer or diffusion in audio domain) to produce high-quality, diverse music ([MiniMax (Hailuo AI) Music | Text to Audio | AI Playground - Fal.ai](https://fal.ai/models/fal-ai/minimax-music#:~:text=Generate%20music%20from%20text%20prompts,quality%2C%20diverse%20musical%20compositions)). It can include not just instrumentals but potentially vocals or humming if lyrics are provided (some descriptions hint it can handle lyrics and vocals in style of a reference track ([image-01 replicate.com api & minimax image-01 github AI Model](https://www.toolify.ai/ai-model/minimax-image-01#:~:text=image,2K))). The model is geared to output up to around 1 minute of music per prompt (possibly around 20–30 seconds is typical). It’s a partner model (Hailuo AI, same family as MiniMax video), suggesting it might excel in certain styles or have unique training focusing on contemporary music. Key features: *multi-genre support* (classical, EDM, rock, etc.), *coherent musical structure* (with a start, development, and end), and *possibly the ability to follow provided lyrics/melody hints*.

**Prompting Best Practices:** Write prompts that describe the **genre, mood, tempo, and instrumentation** of the desired music. For example: *“Upbeat electronic dance track, 128 BPM, with a strong bass drop and melodic synth lead. Happy and energetic vibe.”* This gives the model clear guidance: style (EDM), tempo (128 BPM), mood (happy energetic), key elements (bass drop, synth lead). The model will attempt to generate music matching that description. If you want a certain **instrument or sound**, mention it (e.g. electric guitar riff, piano solo, orchestral strings). Keep prompts relatively short; one or two sentences suffice. Overly detailed prompts (like specifying exact chord progressions) might be beyond its understanding, as it’s not explicitly programming notes but rather style. However, you *can* specify structure in a loose way, e.g. “starts with a soft intro, then builds into a full orchestra climax” – the model might capture that progression. If the model supports **lyrics**, you could include a lyric snippet. For instance, “Pop song with female vocals singing: ‘In the night we shine, our hearts align’ – style similar to Taylor Swift.” The output might then include a vocal-like melody (though generating intelligible lyrics is still extremely hard; likely it would be more of a vocalise). If you don’t want vocals, explicitly say *“instrumental only”*. Conversely, for choral or vocal humming, say *“choir humming an ambient tune”*. Also, indicate **era or reference artist** if you want a specific flavor: *“80s synthwave style”*, *“jazzy piano like Bill Evans”*. The model knows “high-quality” from its prompt training, so you don’t usually need to say “high quality” – instead focus on musical descriptors. Since it’s multi-modal (MiniMax uses text prompt; no melody input in this case), the prompt is your main tool. One more tip: **tempo (BPM)** can often be understood (as seen above), and **emotion words** (sad, joyful, epic) are important since music is all about mood.

**Advanced Tips:** If you have access to **reference audio (melody or chord progression)** conditioning, that can be a game-changer. Some music models allow you to upload a reference track or humming and then continue or style-transfer it. MiniMax Music might not have that in Fal’s interface (the prompt-based usage is primary), but if it ever offers a “reference audio URL” input, you could feed a short melody you like and prompt “continue this in a symphonic style” – currently though, likely only text prompt is supported. Another advanced approach is to use **prompt interpolation**: generate two different music pieces with different prompts, then use an audio interpolation tool (if available, or manually cross-fade) to merge them. This is outside the model usage but can create a longer piece with multiple sections (since each generation might be short). If you need the music to fit a specific length or timing (say for a video), you might have to generate slightly longer and then cut or loop sections. The model’s output will have a natural end (maybe a fade-out or a final chord). If you want a loopable piece, mention *“loopable”* or *“seamless loop”* in the prompt – it might try to structure it so that it can repeat smoothly. For including **lyrics**: if you truly need the model to attempt singing, provide a very short lyric line. Keep in mind, current tech usually ends up with gibberish vocals (the melody might be there but the “words” won’t be clear). It can still be musically pleasing though, like ethereal vocals. Also, consider chaining this with other models: e.g., generate an instrumental track with MiniMax Music, and if you need spoken or sung words on top, use a TTS model to overlay spoken word or rap (some creative uses have the AI rap with a monotone over a beat). On the technical side, music models like this sometimes offer *different sampling or creativity settings* (like temperature or variation). Fal might not expose those to the user, but if they do, a lower temperature would make the output more conservative/repetitive (maybe safer for background music), while a higher might introduce novel riffs but also could get chaotic. If you find outputs too repetitive, you might not have control directly – try adjusting prompt by adding an instruction like “with variation and a bridge section” to coax it into complexity.

**Cost & Performance:** MiniMax Music is **billed per output file**. According to some references, it might be around **$0.035 per generated music piece** (this was an estimate from an external source for similar model) – Fal likely translates that to per second or per track cost. It could be something like *$0.03 per 20s clip* or *$0.10 for up to 1 min* etc. Without official numbers, assume it’s on the order of a few cents for a typical clip (cheaper than video models, more akin to TTS pricing but a bit higher since music is complex). Performance: generating a 30-second audio might take perhaps 5–10 seconds on Fal’s backend, depending on model size. It’s much faster than video generation. Fal’s infrastructure likely streams the audio result as soon as it’s ready. The model might output at CD quality (44.1 kHz). The resulting file could be in WAV or a high-bitrate MP3. If the model struggles, sometimes you might get a shorter clip than requested or a somewhat abrupt end – to mitigate, ensure your prompt implies an ending (or explicitly ask for a “fade out ending”). There’s no mention of usage limits beyond normal rate limiting; you can probably generate many music clips for different needs. Quality wise, expect *“high-quality, diverse compositions”* as the model’s description states ([MiniMax (Hailuo AI) Music | Text to Audio | AI Playground - Fal.ai](https://fal.ai/models/fal-ai/minimax-music#:~:text=Generate%20music%20from%20text%20prompts,quality%2C%20diverse%20musical%20compositions)) – meaning it usually adheres to genre conventions and produces listenable results. It may not produce a radio-ready masterpiece every time (some outputs might feel stock or repetitive), but it’s quite impressive for quick background scores or idea generation. In summary, it’s a low-cost, fast tool for on-demand music with just a carefully written prompt.

### Stable Audio (Open) – Text-to-Audio (Music & Sound Generation)
**About & Key Features:** Stable Audio (Open) is Stability AI’s open-source text-to-audio model. It generates audio (music, sound effects, or ambience) from text descriptions, and is “open” in that it’s available for commercial and research use without many restrictions ([Stable Audio Open | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-audio#:~:text=fal)). It’s a fast timing-conditioned latent diffusion model for audio, meaning it is particularly good at aligning sounds to a specified timing or beat (one can even specify BPM as part of the prompt). Key features include: ability to handle **beats/tempo in prompt**, decent audio quality for music snippets, and a focus on being **resource-efficient** (runs relatively quick). It may not produce full-length songs like closed models but can output shorter clips (the official Stable Audio API allowed up to ~45 seconds for the free model and 90 seconds for the advanced). As an open model on Fal, it’s likely the version that can do around 20–30 seconds of audio per request. It’s well-suited for generating loops, instrument samples, or background ambiance.

**Prompting Best Practices:** Use **concise, descriptive prompts focusing on sound**. For music, similar approach as with MiniMax: specify genre, instruments, mood, and optionally structure. E.g., *“Lo-fi hip hop beat, 80 BPM, soft piano chords and vinyl crackle, relaxing vibe.”* Stable Audio was known to pay attention to tempo (BPM) and instruments – include those clearly. If generating sound effects or ambience, describe the environment and sounds: *“Rainforest ambiance with light rain and distant animal calls”*. You can also combine concepts: *“sci-fi atmosphere with humming spacecraft engine and beeping computers”*. The model will mix them. Because it’s diffusion-based, it can also handle more abstract prompts like *“a single prolonged mystical synth pad swelling and fading”*. But clarity helps. Including **duration cues** can be useful if supported (some versions let you specify target duration or say “10-second loop”). If not explicitly, the prompt could say “short” or “loopable” or “with a clear ending chord” depending on your need. **Musical prompts** might benefit from specifying *“instrumental”* if you don’t want any vocalization. Conversely, you might try *“choral humming”* if you want human voice sounds. In testing, stable audio responds well to genre labels (rock, classical, EDM, etc.) and mood adjectives. It’s also good to mention **audio qualities** like “high quality recording” or “studio mix” if you want clean output, or “low quality, vintage record” if you want a filtered effect (though it might add hiss/pops to simulate that). Essentially, treat the prompt like telling a sound designer what to create.

**Advanced Tips:** Stable Audio’s model has a concept of **timing conditioning** – if you have an exact length or rhythm, you might try to impose it via the prompt. For instance, “a 4-bar blues riff at 120 BPM” – the model might actually follow the bar structure and give you roughly 4 bars of music. If Fal’s interface allows a *duration parameter*, use it (the official Stability API does allow specifying duration in seconds and tempo). If not, you can often infer by model: stable audio open might default to around 10 seconds unless told otherwise. So if you want longer, see if repeating the prompt might coax it (not guaranteed). Another trick: because it’s open, you could generate multiple layers (e.g., get a drum loop and a melody separately by prompting for each, then mix them yourself). This gives more control than prompting the model to do everything at once. For example, prompt1: “funky drum beat, 100 BPM, no melody”, prompt2: “funky bassline at 100 BPM”, then combine externally. That is akin to multitrack recording with AI instruments. It requires some manual audio editing, but yields better customization. If you want **consistency for looping**, sometimes generating a slightly longer clip and then trimming can help, or instruct “loopable end-to-beginning”. Keep an ear out: occasionally, generative audio might have a slight hiss or artifact; a simple post-process filter or noise reduction can clean that. Also, note that stable audio open might not be as “creative” as closed models (it could lean towards safe, somewhat generic outputs). To push creativity, you might add unusual descriptors (“haunting”, “ethereal”, “glitchy”) to get more experimental results. Finally, if you find the output too short or abrupt, you can always run another generation and butt them together, maybe cross-fading if needed, to extend the track. Because it’s free-license, you can also edit it freely without issue.

**Cost & Performance:** Stable Audio Open is **very low-cost or possibly free-tier on Fal**. As an open model, Fal might simply charge by compute time if anything. It might effectively cost **fractions of a cent per second** of audio. Synexa’s comparison showed stable diffusion XL at 0.004 per image ([Synexa Pricing - Affordable AI Computing Solutions](https://synexa.ai/pricing#:~:text=Billed%20per%20image,002%20per%20image)); by analogy stable audio might be similarly low per output. Likely, Fal just charges by GPU-seconds for this one, which might be on the order of $0.0001 per second of audio. In other words, the cost is negligible (a 10-second clip might be <$0.001). Performance: it’s **fast** – generating 10 seconds of audio might only take 1–2 seconds on GPU. The model’s efficient. You’ll get the result quickly, and because it’s not returning huge data (audio file of a few hundred KB for 10s MP3), download is instant. Fal positions it as open, meaning you don’t have heavy usage restrictions. You could probably generate a lot of audio, but remember audio generation can produce **unexpected content** sometimes (like maybe some copyright melody or something, though stable audio is trained to avoid known songs). But since it’s open and presumably licensed data, those concerns are minimal. Quality is decent: not as polished as a human composition, but for backing tracks, ambient sounds, game jams, etc., it’s quite useful. Also stable audio is known to actually adhere to given BPM fairly well (if you say 120 BPM, it uses an underlying timing mechanism to try to match that tempo). This is an advanced feature that you can rely on for rhythm-critical tasks. 

## Voice & Speech Models

### PlayHT Text-to-Speech v3 – Voice Synthesis (Single-Speaker)
**About & Key Features:** PlayHT’s Text-to-Speech v3 is a **state-of-the-art TTS model** known for its natural and expressive voices. It offers **multilingual support** and improved emotional tone rendering ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=Blazing,volume%20processing%20and%20efficient%20workflows)). This model can generate voice audio from text with different preset voices (Fal’s UI shows a dropdown of voices, e.g. “Jennifer (US)” as default ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=Input))). It’s optimized for speed (labeled “blazing-fast”) and is ideal for high-volume use cases where you need lots of narration or dialogue generated quickly. In v3, PlayHT has likely introduced more neural voices and possibly some controllable attributes like pitch or speaking rate, although the primary interface is selecting a voice persona. The key features: *very natural prosody*, *emotional intonations* (the voices can laugh, cry, whisper in tone if the text implies emotion), and support for many languages/accents (English US, UK, Spanish, etc., per voice availability).

**Prompting Best Practices:** The “prompt” here is actually the text you want spoken (not a descriptive prompt, but the actual script). So to use it effectively: feed it **well-punctuated, clear text** as input. The model will read exactly what’s written, so ensure you include commas, periods, question marks etc., to guide the speech rhythm. For example, writing “Let’s eat, grandma!” vs “Lets eat grandma” will drastically change meaning and intonation – just like any TTS. To get an **emotion or tone**, you can add **emotive cues or punctuation**: e.g. “I can’t believe it…” (with ellipsis) might be spoken with a tone of disbelief. Or “No! Absolutely not!” will come out forceful due to exclamation. Use exclamation points, question marks, or even typed out sounds (like “*[sigh]*”) to convey pauses or emotions. Some TTS systems even support **SSML (Speech Synthesis Markup Language)** – I’m not sure if Fal exposes that, but if so you could fine-tune prosody with `<say-as>`, `<break time="500ms"/>`, etc. In absence of SSML, a trick is to break input into multiple sentences or use commas to induce pauses. For example, to make the voice speak slower or with a thoughtful pause: “I… I’m not sure, that’s a good idea.” The ellipsis and comma will naturally slow it down. Also, choose the appropriate **voice persona** for your use case – Fal likely offers multiple. Jennifer (US) might be a generic female narrator. If you need a male voice, pick one of the male names. If you need a British accent or Spanish, select those if available. The content of text should match the language of the voice (don’t feed Spanish text to an English voice, though multilingual support means some voices might speak multiple languages, but best to use a native voice for each language). **Keep the text length reasonable** – very long paragraphs might be better broken into smaller chunks for generation to avoid any timeout and to allow you to adjust between sentences if needed. However, PlayHT v3 is designed for even long-form (like audiobooks) streaming, so it can handle long input, but Fal might have request limits. 

**Advanced Tips:** Use the **additional settings** if Fal’s API provides any (like speaking rate, pitch). If not directly in UI, sometimes you can insert SSML tags in the text (e.g., `<prosody rate="slow">Hello</prosody>`). Check Fal docs if they mention SSML for PlayHT; since it’s a partner model, they might allow raw SSML. With SSML, you can fine-tune pronunciation of certain words via the `<phoneme>` or `<sub alias=""/>` tags (for example, if a name is pronounced incorrectly, you can spell it out phonetically or use IPA). Also, this model being advanced likely handles **numbers and abbreviations** smartly (e.g. “Dr.” as “Doctor”), but if not, you may need to write them out (write “Doctor” instead of “Dr.” in the input to ensure correct expansion). For **emphasis**, you can use ALL CAPS for a word which often makes TTS stress it (e.g. “That was AMAZING!” will emphasize AMAZING). But don’t overuse all caps, it might sound unnatural if everything is emphasized. Another advanced trick: if you want a certain **style that’s not directly emotional** (like a flat robotic tone for effect), you may deliberately remove punctuation or write in a monotonic style – but given these voices are tuned for natural speech, getting a flat tone might be hard unless the voice itself is robotic. Instead, you could use a different model for that style (like “ElevenLabs monotone” etc., but that’s outside this list). For multilingual usage, ensure you provide text in the correct language if the voice supports it. PlayHT v3 does multilingual well, so you can even mix languages in one input and if the voice can handle both (some can code-switch), it might speak both appropriately. Test short sentences first when mixing languages to verify it doesn’t butcher one of them. If you have **large volume** to generate (like thousands of sentences), consider using Fal’s batch or streaming API to avoid re-loading the model each time; set up a pipeline feeding text and receiving audio. The model is fast, but there’s overhead per request. If allowed, maintain a single connection for multiple TTS conversions. Finally, if output doesn’t sound right (like mispronunciations or weird emphasis), experiment by slightly rephrasing your text. Sometimes adding a comma or changing a word spelling can trigger a better reading. It’s a bit of an art to coerce TTS to exactly the tone you want, but PlayHT v3 is one of the more expressive ones, so you’ll likely get a pleasing result by just writing naturally and including punctuation.

**Cost & Performance:** PlayHT TTS v3 is priced at **$0.03 per generated audio minute** ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=)). That’s 3 cents per minute of speech, or about **$0.0005 per second**. In other terms, you can get ~33 minutes of audio for $1 ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=)). This is very cost-effective, suitable for long narration (an hour of audio would be about $1.80). The performance is realtime or better – *blazing fast* as they say. Typically, such models can generate at many times faster than real time. So a 1 minute clip might generate in a few seconds. Fal might even stream the audio so playback can start near immediately if integrated that way. There’s likely no significant queue unless your usage is huge, since TTS is relatively lightweight. Usage limits: TTS might have a max text length per call (maybe a few thousand characters), but you can split text easily. Also, be mindful of not spamming too fast if you do massive volume – Fal might throttle to protect backend. But practically, generating even hundreds of pages of audiobook is feasible with this cost and speed. Since it’s a **Partner model**, it’s production ready – voices are thoroughly quality-checked for minimal errors. You might occasionally hear a mispronounced name or acronym, but overall clarity is excellent. If you need even higher fidelity or different voices, Fal also lists ElevenLabs (Turbo v2.5) as another option ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=fal,12)), but PlayHT v3 is already top-tier and more cost-effective (ElevenLabs typically is pricier per character). For multilingual, the cost stays the same per minute regardless of language. 

### PlayHT Text-to-Speech *Dialog* (PlayAI Dialog) – Multi-Speaker Dialogue TTS 
**About & Key Features:** PlayHT’s Text-to-Speech Dialog model (branded as PlayAI Dialog on Fal) is a system for generating **multi-speaker conversational audio** from a dialogue script ([PlayAI Text-to-Speech Dialog - Fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=PlayAI%20Text,games%2C%20animations%2C%20and%20interactive%20media)). Instead of having to synthesize each speaker line with separate calls, this model takes a formatted dialogue as input (with speaker labels and their lines) and produces a continuous audio of the conversation. It’s tailored for expressive, **natural-sounding multi-speaker interactions** ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=Generate%20natural,games%2C%20animations%2C%20and%20interactive%20media)) – ideal for storytelling, games, or animated dialogue. It likely uses distinct voice models for each speaker label and handles the timing to sound like a real back-and-forth conversation. It supports expressive output, meaning each speaker’s line will carry emotion and intonation appropriately. It simplifies the process of dialogue generation by letting you input the conversation in text and automatically picking suitable voices (maybe default male for Speaker 1, female for Speaker 2, etc., which you may map or it might randomize unless specified).

**Prompting Best Practices:** The input format is typically **“Speaker name: dialogue”** on each line ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=Input)). As shown in Fal’s example, you list each line prefixed by a speaker ID (could be generic like Speaker 1, Speaker 2 or character names) and a colon, then the spoken text. For example:  
```
Speaker 1: Hey, did you catch the game last night?  
Speaker 2: Of course! What a match—it had me on the edge of my seat.  
Speaker 1: Same here! That last-minute goal was unreal...  
Speaker 2: Gotta be the goalie. Those saves were unbelievable!  
```  
 ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=Input)). This format allows the model to know when one voice should stop and the other start. **Use distinct labels** for each speaker consistently. Likely, the first occurrence of a speaker label cues the model to assign a voice to it (possibly alternating genders or using a preset pairing like Speaker 1 male, Speaker 2 female by default). If you want specific voices, see if Fal documentation allows specifying a voice per speaker (maybe via some meta-instruction or by using actual names of known voices as speaker labels). If not, you get default distinct voices. **Write natural conversational text** – include interjections, pauses (you can use punctuation like ellipses or dashes to indicate pauses or cut-offs, which the model will render in speech), laughter (e.g. “haha” will be spoken laughingly if the model is good), and questions/exclamations with proper punctuation so the voice rises or falls. Keep each speaker’s line relatively short (a sentence or two) as in real conversation; long monologues can be done but note the model might still breathe/pauses naturally. Possibly insert **[sounds or actions]** in brackets if you want a pause or effect (though it might just skip reading them; it might not add sound effects, since it’s TTS not an SFX model). For example, “Speaker 1: *[laughs]* That was great.” might cause a brief pause or even a synthesized laugh if the model knows to interpret “[laughs]”. This might not happen unless specifically programmed, so you might instead just write “(laughs)” or literally “hahaha” for laughter. **Maintain conversational tone** – even in writing, use contractions (“I’m, you’re, let’s”) and brief responses (“Yeah.”, “Uh, maybe.”) to get the most natural cadence. The model tries to output as if two people talking, which means it might automatically add subtle *turn-taking cues* (like slight pauses between turns). 

**Advanced Tips:** If you want to influence how a particular speaker sounds, you might try giving an explicit cue in their first line or as a separate instruction. Possibly something like:  
```
Speaker 1 (calm, deep voice): I think we should head out now.  
Speaker 2 (cheerful, high-pitched): Sure, I'm ready!  
```  
Check if the model reads out the parenthetical or actually uses it as an instruction. Ideally, it would use it as instruction and not voice it. The safer approach if that’s not supported is to imply tone through word choice and punctuation (e.g. lots of exclamation and upbeat words for cheerful, short monotone replies for calm). Another advanced use: **narration in dialogues** – maybe you want a narrator voice too. You could include a third speaker “Narrator:” for descriptive lines. The model may then assign a third voice. Or you could incorporate narration in a speaker line like:
```
Narrator: [The two friends walk down the road.]
Speaker 1: It’s been a long day, hasn’t it?
``` 
If the model is well-designed, it might skip or read narration in a different style (or maybe you want it read). This is something to experiment with. Regarding voices, if the default choices aren’t what you want, see if Fal allows specifying speakers like “John:” and “Emily:” where John and Emily correspond to particular voice presets known to the system. If not documented, you might try using actual names of PlayHT voices. For example, PlayHT had voices named like “Will (US)” etc. Perhaps if you use “Will:” as a speaker, it might choose that voice. This is speculative, but some multi-speaker TTS allow naming the voice by the speaker label. The Fal example just used generic Speaker 1/2, which suggests it auto-assigns voices. If you need a specific pairing (male vs female, etc.), you might need to generate lines separately with single-speaker TTS and mix yourself. But the whole point of this model is convenience, so likely it picks two contrasting voices by default. 

One more advanced thing: because it’s conversation, pay attention to **overlap/interruptions**. Real dialogues sometimes have interjections before the other finishes. The model probably doesn’t do overlapping speech (it will produce sequential audio). To simulate an interruption, you can use a dash or ellipsis at the end of one’s line and immediately start the next line, like:
```
Speaker 1: I just wanted to say that I—
Speaker 2: –I know what you’re going to say.
``` 
This should make Speaker 1 cut off and Speaker 2 jump in. The model should respect the punctuation and make Speaker 1’s voice cut off in tone. 

**Cost & Performance:** The dialog TTS is priced at **$0.05 per minute of audio** ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=)), slightly higher than single-speaker (because it’s doing more complex processing). That’s 5 cents per minute, so still just **$0.000833 per second** of generated audio – very affordable. For $1 you get ~20 minutes of dialogue ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=)). The generation time is roughly similar to single-speaker TTS, maybe a tad more to stitch voices, but essentially real-time. For example, a 1-minute dialogue might generate in a second or two. The model handles splitting and voices behind the scenes, so you don’t have to make multiple calls. As input, you can likely feed quite a long dialogue (maybe a multi-minute conversation script). But if extremely long, consider chunking by scene to keep context clear and to avoid any potential buffer limits. The output audio will have the voices alternating with appropriate timing (the model adds slight pause between lines). It’s ready to use – you won’t need to manually splice anything. Quality: each speaker voice is nearly as good as the single-speaker PlayHT voices, with the added benefit of the model possibly capturing conversational flow, like slight differences in how one would speak in a dialogue vs narration (maybe quicker back-and-forth). It’s also context-aware: if one speaker asks a question, the next answer might have the right intonation of responding, which separate TTS calls might not achieve easily. As for usage, the same general rules as TTS apply: avoid extremely jargon-y or rare words (or provide phonetic hints), watch out for voice mismatches (if two speakers speak different languages in the same input, not sure if model can handle that – likely not well, better to stick to one language per conversation model call). With 2+ voices, ensure no speaker’s segment is super long monologue because the model might have memory constraints on that speaker’s prosody. But moderate lengths are fine. In summary, the cost is trivial for what you get – an entire dialogue voiced by AI. This opens up quick creation of radio-play style content or game NPC dialogues without hiring two voice actors, which is exactly its best use case.

### F5 TTS – Voice Cloning (Zero-Shot TTS)
**About & Key Features:** F5 TTS is a recent open-source text-to-speech model notable for its **zero-shot voice cloning** capability ([Play 3.0 mini – A lightweight, reliable, cost-efficient Multilingual TTS ...](https://news.ycombinator.com/item?id=41840872#:~:text=,under%2010G%20vram%20nvidia%20gpu)) ([F5 TTS | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/f5-tts#:~:text=Text%20to%20be%20converted%20to,speech)). It can take a short sample of a person’s voice (reference audio + the transcript of that audio) and then generate new speech in that voice saying any text. It’s a diffusion-transformer hybrid model (Flow Matching + Diffusion Transformer) that achieves highly fluent and natural speech. Key features: supports cloning with very little data (just a few seconds of reference audio), fast non-autoregressive generation, and it’s multilingual to an extent (though primarily English-focused). It’s state-of-the-art in voice cloning as of its release (335M parameters but very capable ([New State-of-the-Art TTS Model Released: F5-TTS : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/1g2giso/new_stateoftheart_tts_model_released_f5tts/#:~:text=New%20State,is%20designed%20for%20English))). On Fal, you provide a **Reference Audio URL and corresponding reference text** for that audio, plus the text you want generated ([F5 TTS | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/f5-tts#:~:text=Text%20to%20be%20converted%20to,speech)). The model then speaks the new text in the reference voice. This is ideal for creating custom voices – e.g. clone your own voice or a character’s voice, to produce any narration.

**Prompting Best Practices:** The main “prompt” is still the text to speak (like any TTS). But the crucial part is providing a **good reference audio + accurate reference transcript**. For best results: use a clear recording of the target voice, ideally a clean sample with no background noise and representative speaking style. You don’t need a long sample; something like 5–15 seconds of speech is usually enough (the paper mentions ~3 seconds can work, but a bit more helps quality). Make sure the **reference text exactly matches** what’s said in the reference audio, as the model uses that to understand the voice’s characteristics (tone, accent) aligned with phonetics. If they mismatch, the cloning might falter. On Fal’s interface, you’ll likely paste a URL to an audio file (maybe .wav) and then type the transcript in a provided field ([F5 TTS | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/f5-tts#:~:text=Text%20to%20be%20converted%20to,speech)). Do that carefully. Then for the new text input (the content you want spoken in that voice), treat it like normal TTS text – include punctuation, etc., to shape prosody. The model will try to keep the same speaking style as in the sample. If your reference speaker speaks very formally in the sample, the output will likely also sound formal. If you want a different tone from the same voice (say reference is calm but you want angry), the model might not perfectly extrapolate emotions not present in the sample. You may need to provide a reference sample that matches the style you want. Or try to coerce via text (like adding “!” etc., but voice timbre might remain calm if sample was calm). So choose the sample that has a similar energy/intonation to what you’ll generate. Also, note the **Model Type selection** – Fal shows “F5-TTS” as a dropdown (maybe if future versions come, you might choose them) ([F5 TTS | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/f5-tts#:~:text=Model%20Type)). Just leave it to F5-TTS unless you know of a variant. 

**Advanced Tips:** **Voice adaptation** – since it’s zero-shot, it can clone on the fly without fine-tuning, but sometimes providing a slightly longer sample (like multiple sentences) yields more robust cloning (the model can average out the voice traits). If possible, give ~30 seconds of audio rather than 5s. Also, ensure the reference audio has the **same audio quality** you want; if the sample is telephone-quality, the output might carry that muffled quality too. Use high-quality reference audio for high-quality output. If the output voice has minor artifacts or doesn’t perfectly match, an advanced trick: you can run the output through a voice conversion model or enhancer, but likely unnecessary. F5 is quite good natively. It might struggle with **singing** – if you input singing audio and then give it text, it’s not going to sing (it’s a TTS, not trained for singing intonation). It will speak in that person’s voice normally. For multilingual: if you give it an English sample, it captures an English accent of that voice. If you then feed non-English text, it may or may not do well (the model was primarily trained on English but possibly some multilingual ability via training data). It might speak other languages with an English accent of that voice. For cross-gender or totally different content, it usually retains voice identity but might not nail emotion that wasn’t in sample. Another tip: if generating a long piece in the cloned voice, consider breaking the text into paragraphs or sentences and generate each separately, possibly with slight variation in punctuation to keep it expressive. Because cloned voices can sometimes become a tad monotone if you generate a lot in one go (not always, but it’s a general TTS thing). By chunking, you allow resetting the model per sentence which can actually keep it crisp. If continuity of intonation is needed across sentences, then do it all at once. Also, be mindful of **embedding content** – if the reference voice is someone famous or proprietary, ensure you have rights if it’s for public/commercial use. F5 is open, but outputs could still raise issues if misused (Fal likely expects ethical use; they might even block obvious public figure cloning via some policy). Technically, to maximize similarity, include tricky phonemes in the sample if your target text will have them. For instance, if output text has a lot of “th” sounds but sample had none, the model might be a bit less tuned on how that voice says “th”. Usually not a big problem though – the model presumably builds a voice profile across all phonemes from the transcript. 

**Cost & Performance:** As an advanced model, F5-TTS is surprisingly efficient. The pricing on Fal isn’t explicitly given in the snippet, but likely it’s charged per second of output (like TTS) plus maybe overhead for using the reference. Given other TTS rates, it could be around **$0.05–$0.06 per minute** as well. Possibly Fal hasn’t separately priced it yet, but presumably similar ballpark because it’s open-source (the expensive part is that it uses diffusion internally, but it’s optimized for quick inference). It might be a bit slower than PlayHT’s model because cloning and diffusion add overhead, but still likely real-time or just above. You might see perhaps 2-3 seconds to generate 5 seconds of audio on moderate GPU. Still, very usable. From user reports, F5 can generate ~1000 words per minute of audio on decent hardware ([New State-of-the-Art TTS Model Released: F5-TTS : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/1g2giso/new_stateoftheart_tts_model_released_f5tts/#:~:text=r%2FStableDiffusion%20www,58%20minutes%20total)), which is basically real-time. As for performance: expect a small delay for the model to process the reference audio the first time (but it’s negligible with a short sample). Possibly Fal might cache embeddings if you use the same reference repeatedly in one session. There’s the step of uploading the file – Fal’s interface likely handles that (you either host the file yourself or upload via Fal UI). Once set, generation is one step. The voice similarity can be quite high – many find it quite close to ground truth voice in tests. There might be very slight differences or a tiny robotic undertone if compared side by side, but for most applications it’s convincingly the same speaker. That said, extremely unique voice qualities (like a raspy voice or heavy accents) might be somewhat averaged out. It’s better at cloning typical voices. Regarding usage limits: since it’s open, Fal probably just gates it behind the usual API key and costs. They likely allow use as long as it’s not disallowed content (e.g. using someone’s voice without consent might violate policies). Also note, **reference text must be accurate** – it’s required in Fal’s UI as a starred field ([F5 TTS | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/f5-tts#:~:text=Text%20to%20be%20converted%20to,speech)). If you don’t have an exact transcript, transcribe it first (even use an ASR model to get it, but correct any mistakes). The model’s success depends on that alignment. All in all, F5 TTS on Fal lets you do what was science fiction a few years ago – clone a voice in seconds – at minimal cost and fairly fast speeds, making it extremely powerful for custom voiceover needs.

---

**Comparison Table of Key Insights:**

| **Model** | **Category** | **Key Features** | **Best Use Cases** | **Cost & Performance (Fal API)** | **Prompting Techniques & Tips** |
|-----------|-------------|------------------|-------------------|----------------------------------|--------------------------------|
| **FLUX.1 [dev]** (fal-ai/flux/dev) | Image (Text→Image) | 12B flow-transformer model; high-quality outputs; general-purpose styles ([FLUX.1 [dev] | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/flux/dev#:~:text=fal)). | Detailed artistic images; versatile content generation for concept art, design. | ~$0.025 per 1MP image ([ZimmWriter Image API Integration | www.rankingtactics.com](https://www.rankingtactics.com/zimmwriter-image-api-integration/#:~:text=www,The%20main)). Moderate speed (needs ~20–40 steps for best quality). | Use **long, descriptive prompts** with subject, style, context (handles complex prompts well). Can include **weighted segments** (`::`) to emphasize parts ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Image%3A%20portrait%20,s%201000)). No explicit negative prompt field, so state undesired elements in prompt (“no text,” etc.). Leverage **Flux LoRAs** or image-to-image for fine control. |
| **FLUX.1 [schnell]** (fal-ai/flux/schnell) | Image (Text→Image) | Optimized “fast” FLUX; 1–4 diffusion steps for image ([Fastest FLUX Endpoint | fal.ai Docs](https://docs.fal.ai/fast-flux/#:~:text=fal,flux)). Very fast inference with quality near FLUX dev ([README.md · frankjoshua/FLUX.1-schnell at aa3b18d89cc7c592cc8291df10c9b5045cf3a5db](https://huggingface.co/frankjoshua/FLUX.1-schnell/blame/aa3b18d89cc7c592cc8291df10c9b5045cf3a5db/README.md#:~:text=1.%20Cutting,model%20can%20be%20used%20for)). | High-throughput image gen; quick previews or batch generation where speed matters. | ~$0.003 per image ([ZimmWriter Image API Integration | www.rankingtactics.com](https://www.rankingtactics.com/zimmwriter-image-api-integration/#:~:text=www,The%20main)) (very low cost). Blazing fast (few seconds per image). | **Concise or detailed prompts** both work. Supports **same syntax** as FLUX dev (use `|` and `::` weights) ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Image%3A%20portrait%20,s%201000)). Great for **rapid iteration** – try multiple prompt tweaks quickly. If quality needs a boost, increase steps slightly (up to 4). Use outputs as drafts to later upscale or refine with slower models. |
| **FLUX.1 [pro] v1.1 Ultra** (fal-ai/flux-pro/v1.1-ultra) | Image (Text→Image) | Latest pro-grade FLUX; up to 2K resolution, improved realism ([FLUX.1 [dev] | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/flux/dev#:~:text=improves%20image%20quality%2C%20typography%2C%20prompt,res%20realism%20%2013)). Photorealistic tendencies, high detail. | Photorealistic images, high-res needs (marketing material, prints). When quality trumps speed. | ~$0.05 per 1MP image ([Flux 1.1 Pro Ultra Mode Is Here | undefined - Flux Labs AI](https://www.fluxlabs.ai/blog/flux-11-pro-ultra-mode-is-here#:~:text=Flux%201%20Pro%3A%20fixed%20price,or%20Freepik%20if%20you)). Slower inference due to high res and detail (larger model). | **Photographic prompts** shine – include camera models, lighting, and realistic detail for lifelike results ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Image%3A%20Close,and%20altruism%20through%20scene%20details)) ([FLUX.1 [schnell] | Text to Image |  | fal.ai](https://fal.ai/models/fal-ai/flux/schnell/examples#:~:text=Background%20hints%20at%20charitable%20setting,and%20altruism%20through%20scene%20details)). Use for **final renders** (after prototyping with faster models). Add exact styles or artist names for specific looks. Ensure punctuation and grammar in prompt for clarity (model is sensitive to prompt clarity). |
| **Stable Diffusion 3.5 Large** (fal-ai/stable-diffusion-v35-large) | Image (Text→Image) | Stability AI’s MMDiT model; excels at complex prompts, typography, and coherent detail ([Generating Images from Text | fal.ai Docs](https://docs.fal.ai/guides/generating-images-from-text/#:~:text=%2A%20fal,efficiency)). Multimodal inputs (ControlNet, etc.). | General-purpose image gen; complex scenes, on-brand visuals (when using ControlNet/LoRAs). Great for when Stable Diffusion-style output or community familiarity is needed. | ~$0.06–$0.07 per image (approx. 15 images/$1) – moderate ([Stable Diffusion 3.5 Large | Text to Image | AI Playground - Fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Stable%20Diffusion%203,Related)). Good speed (slower than SD1.5 but optimized for faster inference than prior XL models). | **Paragraph-length prompts** are workable – describe scenes thoroughly ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Prompt)) ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=creating%20an%20otherworldly%20atmosphere,masterful%20photography%2C%20hyperdetailed%2C%20magical%20realism)). Use **ControlNet** for pose/structure control (Fal UI supports depth, canny, etc.). **Negative prompts** supported – use them to rule out unwanted traits. Can attach **LoRA** models for style/character consistency ([Stable Diffusion 3.5 Large | Text to Image | AI Playground | fal.ai](https://fal.ai/models/fal-ai/stable-diffusion-v35-large#:~:text=Loras)). Very flexible – fine-tune prompt for best results (it can handle a lot of detail). |
| **Minimax Video-01-Live** (fal-ai/minimax/video-01-live) | Video (Text→Video and Image→Video) | High-motion animation from text or single image; smooth camera moves; consistent subject rendering ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=static%20images%20to%20life%20and,of%20fal%E2%80%99s%20inference%20engine%2C%20this)). Short clips (~3–5s). | Animating static images (live portraits, artwork to video), action scenes with one main subject. Great for adding motion to illustrations or creating quick video content from an image. | ~$0.30–$0.40 per video (estimated similar to Hunyuan) ([Pricing | fal.ai](https://fal.ai/pricing#:~:text=Pricing%20,megapixel%2C%20Billed%20by%20the)). ~4s clip generates in ~30–60s (fast for video). | **Use image input for best results** (provide an image to animate along with prompt). **Describe motion and camera** clearly – e.g. “camera pans around X” for dynamic shots. Keep prompts focused on one scene/event. Good with **verbs** describing actions. Leverage it to **bring stills to life** – generate an image with Flux/SD, then animate with Minimax. |
| **Hunyuan Video** (fal-ai/hunyuan-video) | Video (Text→Video) | Tencent’s 13B model; cinematic quality, real-world physics, smooth continuous actions ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=animations%2C%20excelling%20particularly%20in%20high,HD%20output%20with%20enhanced%20physics)). ~4s clips with complex motion. | High-fidelity short videos (like mini movie scenes, product visuals). Ideal when realism and fluid motion are crucial – e.g. showcasing a concept with a sweeping camera shot. | **$0.40 per video** (flat) ([Pricing | fal.ai](https://fal.ai/pricing#:~:text=Pricing%20,megapixel%2C%20Billed%20by%20the)). Generates ~4s in <1 minute (optimized on Fal) ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=and%20realistic%20content,the%20fact%20that%20AI%20video)). | **Write prompts as mini movie scenes.** Include environment, actions, and camera cues (e.g. “tracking shot,” “slow motion”). Use **cinematic language** – the model excels with that. Ensure prompt elements can logically happen in ~4s. Let the model handle physics – just describe outcome (e.g. “glass falls and shatters” will be handled realistically). |
| **Kling 1.5 Pro** (fal-ai/kling-video/v1.5/pro) | Video (Image→Video, Text→Video) | Professional video gen at 1080p; enhanced physics and detail ([What’s new in fal: New video & image models, portrait trainer, creator mode, and so much more](https://buttondown.com/fal/archive/whats-new-in-fal-new-video-image-models-portrait/#:~:text=model%20achieves%20record,also%20exponentially%20growing%20each%20month)). Slightly older than v1.6, but high quality and available for image or text input. | High-res video needs – ads, prototypes requiring HD. Good for **physically complex scenes** (explosions, water, etc.) where simulation of effects is needed. | **~$0.10 per second** of video ([Pricing | fal.ai](https://fal.ai/pricing#:~:text=Pricing%20,megapixel%2C%20Billed%20by%20the)) (≈$0.5 for 5s clip). Slower: expect a couple minutes for a clip (HD output). | **Prefer image-to-video** for consistency: supply a base image of scene/character. **Detail the scenario** including physical events (model will simulate accurately). Use **short lines** if doing dialogues – but Kling is more visual; focus on visual events. Prompt for **1080p** detail if needed (though output is inherently HD). |
| **Kling 1.0 Standard** (fal-ai/kling-video/v1/standard/text-to-video) | Video (Image→Video, Text→Video) | Standard-quality Kling model; lower resolution, faster, simpler physics. Good for basic animations. | Quick, lower-cost video tasks – concept testing, simple cartoon-style animations, or where 1080p isn’t required. | ~$0.05 per second (estimated). Very fast generation (~half the time of Kling Pro). | **Keep prompts simple and singular.** Best for one action or scene – doesn’t handle multiple complicated events as well as newer models. Use for **storyboards**: e.g. generate rough animation of a scene to visualize it, then refine elsewhere. For styles, explicitly say “cartoon” or “sketch” if desired – it can adapt style somewhat. Add **pauses/ellipses** in text to simulate timing if multiple things happen (though overlap isn’t explicit). |
| **Luma Dream Machine 1.5** (fal-ai/luma-dream-machine) | Video (Text→Video, Image→Video) | High-quality imaginative video gen; consistent multi-subject interactions, realistic physics + cinematic camera ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=Luma%20Dream%20Machine%20is%20a,more%20ideas%20and%20dream%20bigger)) ([Luma Dream Machine - Fountn](https://fountn.design/resource/luma-dream-machine/#:~:text=Dream%20Machine%E2%80%99s%20advanced%20capabilities%20ensure,some%20limitations%2C%20the%20team%20is)). Balanced for creative scenes. | Creative storytelling videos, fantasy or sci-fi scenes with rich detail. Also good for multi-character interactions in one scene (which some models struggle with). | **$0.50 per video** (flat) ([Luma Dream Machine | Text to Video | AI Playground - Fal.ai](https://fal.ai/models/fal-ai/luma-dream-machine#:~:text=Generate%20video%20clips%20from%20your,can%20run%20this%20model)). ~20–30s generation for ~4–5s clip (efficient engine). | **Write “dreamlike” or creative prompts** freely – model is built for imagination. Still, include concrete elements so it has something to render (structures, characters). Supports both **text-only and image+text**; use image input if you have a specific look to maintain. Great with **ambient and cinematic prompts** (mention camera movement, environment mood). Can handle a bit longer clips (some users get ~8s); for longer, you can chain multiple outputs. |
| **MMAudio V2** (fal-ai/mmaudio-v2) | Audio (Video→Audio, Text→Audio) | Audio generation for video: adds synchronized sound (music/SFX) to silent videos ([MMAudio V2 | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/mmaudio-v2#:~:text=fal)). Takes video + optional text description of desired audio, outputs video with audio track. | Automatically sound-designing AI-generated videos (e.g. add atmosphere to Minimax or Hunyuan outputs). Quickly getting background music or effects for a clip without manual editing. | Pricing per second of audio (likely on par with TTS, e.g. a few cents/minute). Real-time or faster performance (generates audio quickly relative to video length). | **Always provide a text prompt describing the audio** along with the video. E.g. “crowd noise and traffic sounds” for a city scene, or “tense orchestral music” for a dramatic scene. Be specific about what kind of audio and mood. It will align sounds to visible actions (e.g. add footstep sounds when it sees walking). If you only want music, mention no SFX (and vice versa). After generation, you can adjust the volume levels externally if needed (the mix is generally good, but you might lower music under dialogue, etc.). |
| **Sync.So Lipsync 1.8** (fal-ai/sync-lipsync) | Video (Audio→Video) | Lip-sync animation model: given a face video/image and an audio clip, produces a video of the face speaking the audio ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=Video%20Url)) ([sync.so -- lipsync 1.9.0-beta | Video to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/sync-lipsync#:~:text=Audio%20Url)). High-quality synchronization of mouth movements to speech. | Creating talking avatar videos (dubbing characters, animating portraits). Re-dubbing videos in different languages (just swap audio and lipsync). Virtual presenters for text-to-video solutions (combined with TTS to provide the voice). | ~$0.05 per minute of output (speech) ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=)). Very fast (runs faster than real-time; 10s clip generates in a couple seconds). | **Input = video (or still image) + audio**. Use a clear front-facing image/video of the person. Provide an audio with clean speech. Ensure the video/image and audio durations align (the output length matches audio length). The model auto-syncs; you don’t need to prompt anything if inputs are set. For best results, the face in the image should have a neutral mouth (closed or slight smile) initially so it can animate fully. Also, match the audio language to the person – lipsync on mismatched language might look a bit off if the mouth shapes don’t align perfectly (though it generally just follows phonetics). |
| **Veo 2** (fal-ai/veo2) | Video (Text→Video) | Google’s latest SOTA video model; unparalleled physics & cinematography ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=,Significantly%20fewer%20unwanted%20artifacts%20or)), supports up to 4K resolution and extended durations ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=understands%20the%20language%20of%20filmmaking,Significantly%20fewer%20unwanted%20artifacts%20or)). Minimal artifacts (near production-ready output). | High-end video generation for professional content. Anywhere you need a fully realized video scene with complex interactions – e.g. film pre-visualization, ad creatives, VFX ideation. It can handle multi-second continuous shots with fine detail. | **$2.50 for 5s, +$0.50 each additional second** ([Veo 2 | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/veo2#:~:text=For%205s%20video%20your%20request,50)) (~$0.50/sec). Slowest model: heavy compute – ~a few minutes for a 5s HD clip (Fal uses multi-GPU/TPU to accelerate but it’s still resource-intensive). | **Write film scene-style prompts**. Include camera directions, detailed actions, and environment specifics ([Veo 2 | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/veo2#:~:text=The%20camera%20floats%20gently%20through,film%2C%20the%20golden%20light%20creates)) ([Veo2: State-of-the-Art Video Generation Now on fal](https://blog.fal.ai/veo2-state-of-the-art-video-generation-now-on-fal/#:~:text=,Significantly%20fewer%20unwanted%20artifacts%20or)). Veo2 can follow sequential descriptions, so you can outline a short story in one prompt (it will try to realize it in the given time). Use **specific lens/film terms** for style (e.g. “35mm film, shallow depth of field” for a cinematic look ([Veo 2 | Text to Video | AI Playground | fal.ai](https://fal.ai/models/fal-ai/veo2#:~:text=petals%20glowing%20in%20the%20warm,and%20weathered%20wood%20of%20the))). Because it can do long videos, you might explicitly state “~10 second scene” in prompt if you have that length (and of course adjust the Fal duration parameter accordingly). The cost is high, so **test at lower resolution or shorter length first** to tweak prompt, then scale up to 4K or full length for final. |
| **MiniMax Music** (fal-ai/minimax-music) | Audio (Text→Music) | AI music generator (by Hailuo); creates short musical compositions from prompts ([MiniMax (Hailuo AI) Music | Text to Audio | AI Playground - Fal.ai](https://fal.ai/models/fal-ai/minimax-music#:~:text=Generate%20music%20from%20text%20prompts,quality%2C%20diverse%20musical%20compositions)). Can produce instrumental or vocal-like sounds, multi-genre, with coherent structure (intro, beat, etc.). | Background music for videos or games, inspiration for composers, quick music samples without licensing issues. Great for when you need a quick soundtrack of a specific style/mood. | ~$0.03–$0.05 per generated track (estimated; very low cost). Generates a 20s–30s clip in a few seconds (fast). | **Prompt = description of music** (not natural language scene, but musical properties). State **genre, instruments, mood, tempo (BPM)**. E.g. “Calm acoustic guitar melody, 70 BPM, relaxing folk style.” The model respects BPM and instrument cues. Use **mood adjectives** (“uplifting, somber, energetic”) to shape tone. If you want certain sections (intro, buildup), you can mention them, but remember outputs are short (the model might pack a mini-structure in that time). For any vocals, you can try adding “with male vocals humming” – it may add vocal sounds (though lyrics are not precise). Keep the prompt concise – think of it like describing a music piece to a composer. |
| **Stable Audio (Open)** (fal-ai/stable-audio) | Audio (Text→Audio) | Stability’s open-source text-to-audio model. Can generate music or sound effects from text. Has timing conditioning (understands tempo and length cues) ([DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music ...](https://arxiv.org/html/2405.20289v1#:~:text=DITTO,conditioned%20latent%20audio)). Good at loops and shorter audio (e.g. 10–30s). | Royalty-free sound effect generation (environments, noises) and music loops for projects. Ideal for quick audio backgrounds – e.g. ambient sounds for a game level, or a drum loop for a song. | *Very* low cost (on par with compute pricing; likely <$0.01 for 10s clip). Near real-time generation. | **Prompt similar to MiniMax’s approach**: describe the audio content. For music: include **genre/instruments and BPM** (it was designed to honor tempo – e.g. “128 BPM techno beat with synth chords”). For ambiance: describe environment and elements (“busy café ambiance: chatter, clinking cups”). You can also request loops (“seamless loop of ocean waves”). Keep instructions clear and succinct – stable audio isn’t as good with long narrative prompts, it wants key audio descriptors. If target duration is important, specify (some stable audio interfaces allow “duration: X seconds” in prompt or separate parameter – check Fal docs). Otherwise it defaults to around 10 seconds for open model. |
| **PlayHT TTS v3** (fal-ai/playht/tts/v3) | Voice (Text→Speech) | Single-speaker neural TTS with **emotive, multilingual voices** ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=Blazing,volume%20processing%20and%20efficient%20workflows)). Very natural prosody; many voice options (male/female, various accents). Optimized for speed and quality. | Voiceover for articles, video narration, IVR systems, or audiobook creation. Any use case needing a human-like voice reading provided text. | **$0.03 per audio minute** ([PlayAI Text-to-Speech v3 | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/v3#:~:text=)) (~33 minutes per $1). Real-time or faster generation. | **Input actual script text** (not a description). Use punctuation to guide tone: “...” for pauses, “!” for excitement, “?” to prompt raising tone. **Select voice persona** if possible (Fal UI voice dropdown) to match the context (e.g. Joanna for friendly narration, a different voice for a serious tone). If needed, add minor cues in text like *“(whispering)”* or use an exclamation to convey shouting – the model will reflect it to some extent. For languages, just input the text in that language and pick a voice that supports it. Use SSML tags if Fal supports (to fine-tune speed, emphasis or pronunciation for acronyms, etc.). Keep sentences reasonably sized – the model handles long texts, but you might break on paragraph boundaries for control. |
| **PlayHT TTS Dialog** (fal-ai/playai/tts/dialog) | Voice (Text→Speech, multi-speaker) | Multi-speaker dialogue synthesis – reads a formatted script with multiple characters and generates a continuous conversation audio ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=Input)) ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=)). Assigns distinct voices to each speaker label, with natural turn-taking and interaction. | Voicing game dialogues, audiobooks with multiple characters, radio drama or conversational agents. Saves manually splicing individual TTS outputs. | **$0.05 per audio minute** ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=)). Real-time generation (very fast for dialogues). | **Format input as a script** with each line prefixed by a speaker name and colon ([PlayAI Text-to-Speech Dialog | Text to Speech | AI Playground | fal.ai](https://fal.ai/models/fal-ai/playai/tts/dialog#:~:text=Input)). E.g. “Alice: … Bob: …”. The model auto-selects voices for each (likely one male, one female if not specified). Write dialogue naturally – include interjections (“uh,” “hmm”), interruptions (use ‘—’ or ‘…’ at line breaks to simulate cutting off). The model will include appropriate pause lengths between speakers. If you want a specific voice for a speaker, try using a known voice name as the speaker label (if supported), or generate those lines separately with single-speaker TTS as a workaround. But usually, letting it assign voices is easiest. Use punctuation and spelling to indicate tone (e.g. “nooo!” versus “no.”). The model handles up to 5+ speakers, but ensure each has a unique name and lines. |
| **F5 TTS** (fal-ai/f5-tts) | Voice (Voice Cloning TTS) | Diffusion-based zero-shot **voice cloning** – clones voice from a short sample and generates new speech in that voice ([Play 3.0 mini – A lightweight, reliable, cost-efficient Multilingual TTS ...](https://news.ycombinator.com/item?id=41840872#:~:text=,under%2010G%20vram%20nvidia%20gpu)). Supports high-quality, near-human mimicry of voice timbre. | Custom voice generation: cloning a particular person’s voice (with permission) for TTS – e.g. have a virtual narrator in a specific voice. Continuation of an actor’s voice for production, voice banking for someone who lost speech, or creating a new character voice from a sample. | *Likely ~$0.05–$0.06 per minute* (similar to advanced TTS). Slightly slower than standard TTS but still around real-time. | **Provide reference audio + its transcript**, and input text to speak. For best results, use ~10–30s of clear reference audio with an accurate transcript ([F5 TTS | Text to Audio | AI Playground | fal.ai](https://fal.ai/models/fal-ai/f5-tts#:~:text=Text%20to%20be%20converted%20to,speech)) – this trains the voice clone on that speaker’s style. Ensure reference audio reflects the tone you want (e.g. energetic sample if you need energetic speech). Then input the desired speech text. Use normal TTS prompting for that text (punctuate for emotion, etc.). The output voice will mimic the reference. If output comes out with minor artifacts or off-emphasis, experiment by changing reference (maybe a different clip of the person’s voice) or tweaking the wording of your input text (the model might carry over some rhythms from reference transcript). Note: If the reference speaker has a strong accent and you write text in another language, the model may attempt it but with the speaker’s accent. Match language if possible. **Ethical tip:** only clone voices you have rights/consent for – e.g. your own voice or authorized voice talent – as the model will produce very realistic clones. |

